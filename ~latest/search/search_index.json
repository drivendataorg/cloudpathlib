{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Our goal is to be the meringue of file management libraries: the subtle sweetness of <code>pathlib</code> working in harmony with the ethereal lightness of the cloud.</p> <p>A Python library with classes that mimic <code>pathlib.Path</code>'s interface for URIs from different cloud storage services.</p> <pre><code>with CloudPath(\"s3://bucket/filename.txt\").open(\"w+\") as f:\n    f.write(\"Send my changes to the cloud!\")\n</code></pre>"},{"location":"#why-use-cloudpathlib","title":"Why use cloudpathlib?","text":"<ul> <li>Familiar: If you know how to interact with <code>Path</code>, you know how to interact with <code>CloudPath</code>. All of the cloud-relevant <code>Path</code> methods are implemented.</li> <li>Supported clouds: AWS S3, Google Cloud Storage, and Azure Blob Storage are implemented. FTP is on the way.</li> <li>Extensible: The base classes do most of the work generically, so implementing two small classes <code>MyPath</code> and <code>MyClient</code> is all you need to add support for a new cloud storage service.</li> <li>Read/write support: Reading just works. Using the <code>write_text</code>, <code>write_bytes</code> or <code>.open('w')</code> methods will all upload your changes to cloud storage without any additional file management as a developer.</li> <li>Seamless caching: Files are downloaded locally only when necessary. You can also easily pass a persistent cache folder so that across processes and sessions you only re-download what is necessary.</li> <li>Tested: Comprehensive test suite and code coverage.</li> <li>Testability: Local filesystem implementations that can be used to easily mock cloud storage in your unit tests.</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p><code>cloudpathlib</code> depends on the cloud services' SDKs (e.g., <code>boto3</code>, <code>google-cloud-storage</code>, <code>azure-storage-blob</code>) to communicate with their respective storage service. If you try to use cloud paths for a cloud service for which you don't have dependencies installed, <code>cloudpathlib</code> will error and let you know what you need to install.</p> <p>To install a cloud service's SDK dependency when installing <code>cloudpathlib</code>, you need to specify it using pip's \"extras\" specification. For example:</p> <pre><code>pip install cloudpathlib[s3,gs,azure]\n</code></pre> <p>With some shells, you may need to use quotes:</p> <pre><code>pip install \"cloudpathlib[s3,gs,azure]\"\n</code></pre> <p>Currently supported cloud storage services are: <code>azure</code>, <code>gs</code>, <code>s3</code>. You can also use <code>all</code> to install all available services' dependencies.</p> <p>If you do not specify any extras or separately install any cloud SDKs, you will only be able to develop with the base classes for rolling your own cloud path class.</p>"},{"location":"#conda","title":"conda","text":"<p><code>cloudpathlib</code> is also available using <code>conda</code> from conda-forge. Note that to install the necessary cloud service SDK dependency, you should include the appropriate suffix in the package name. For example:</p> <pre><code>conda install cloudpathlib-s3 -c conda-forge\n</code></pre> <p>If no suffix is used, only the base classes will be usable. See the conda-forge/cloudpathlib-feedstock for all installation options.</p>"},{"location":"#development-version","title":"Development version","text":"<p>You can get latest development version from GitHub:</p> <pre><code>pip install https://github.com/drivendataorg/cloudpathlib.git#egg=cloudpathlib[all]\n</code></pre> <p>Note that you similarly need to specify cloud service dependencies, such as <code>all</code> in the above example command.</p>"},{"location":"#quick-usage","title":"Quick usage","text":"<p>Here's an example to get the gist of using the package. By default, <code>cloudpathlib</code> authenticates with the environment variables supported by each respective cloud service SDK. For more details and advanced authentication options, see the \"Authentication\" documentation.</p> <pre><code>from cloudpathlib import CloudPath\n\n# dispatches to S3Path based on prefix\nroot_dir = CloudPath(\"s3://drivendata-public-assets/\")\nroot_dir\n#&gt; S3Path('s3://drivendata-public-assets/')\n\n# there's only one file, but globbing works in nested folder\nfor f in root_dir.glob('**/*.txt'):\n    text_data = f.read_text()\n    print(f)\n    print(text_data)\n#&gt; s3://drivendata-public-assets/odsc-west-2019/DATA_DICTIONARY.txt\n#&gt; Eviction Lab Data Dictionary\n#&gt;\n#&gt; Additional information in our FAQ evictionlab.org/help-faq/\n#&gt; Full methodology evictionlab.org/methods/\n#&gt;\n#&gt; ... (additional text output truncated)\n\n# use / to join paths (and, in this case, create a new file)\nnew_file_copy = root_dir / \"nested_dir/copy_file.txt\"\nnew_file_copy\n#&gt; S3Path('s3://drivendata-public-assets/nested_dir/copy_file.txt')\n\n# show things work and the file does not exist yet\nnew_file_copy.exists()\n#&gt; False\n\n# writing text data to the new file in the cloud\nnew_file_copy.write_text(text_data)\n#&gt; 6933\n\n# file now listed\nlist(root_dir.glob('**/*.txt'))\n#&gt; [S3Path('s3://drivendata-public-assets/nested_dir/copy_file.txt'),\n#&gt;  S3Path('s3://drivendata-public-assets/odsc-west-2019/DATA_DICTIONARY.txt')]\n\n# but, we can remove it\nnew_file_copy.unlink()\n\n# no longer there\nlist(root_dir.glob('**/*.txt'))\n#&gt; [S3Path('s3://drivendata-public-assets/odsc-west-2019/DATA_DICTIONARY.txt')]\n</code></pre>"},{"location":"#supported-methods-and-properties","title":"Supported methods and properties","text":"<p>Most methods and properties from <code>pathlib.Path</code> are supported except for the ones that don't make sense in a cloud context. There are a few additional methods or properties that relate to specific cloud services or specifically for cloud paths.</p> Methods + properties <code>AzureBlobPath</code> <code>S3Path</code> <code>GSPath</code> <code>absolute</code> \u2705 \u2705 \u2705 <code>anchor</code> \u2705 \u2705 \u2705 <code>as_uri</code> \u2705 \u2705 \u2705 <code>drive</code> \u2705 \u2705 \u2705 <code>exists</code> \u2705 \u2705 \u2705 <code>glob</code> \u2705 \u2705 \u2705 <code>is_absolute</code> \u2705 \u2705 \u2705 <code>is_dir</code> \u2705 \u2705 \u2705 <code>is_file</code> \u2705 \u2705 \u2705 <code>is_relative_to</code> \u2705 \u2705 \u2705 <code>iterdir</code> \u2705 \u2705 \u2705 <code>joinpath</code> \u2705 \u2705 \u2705 <code>match</code> \u2705 \u2705 \u2705 <code>mkdir</code> \u2705 \u2705 \u2705 <code>name</code> \u2705 \u2705 \u2705 <code>open</code> \u2705 \u2705 \u2705 <code>parent</code> \u2705 \u2705 \u2705 <code>parents</code> \u2705 \u2705 \u2705 <code>parts</code> \u2705 \u2705 \u2705 <code>read_bytes</code> \u2705 \u2705 \u2705 <code>read_text</code> \u2705 \u2705 \u2705 <code>relative_to</code> \u2705 \u2705 \u2705 <code>rename</code> \u2705 \u2705 \u2705 <code>replace</code> \u2705 \u2705 \u2705 <code>resolve</code> \u2705 \u2705 \u2705 <code>rglob</code> \u2705 \u2705 \u2705 <code>rmdir</code> \u2705 \u2705 \u2705 <code>samefile</code> \u2705 \u2705 \u2705 <code>stat</code> \u2705 \u2705 \u2705 <code>stem</code> \u2705 \u2705 \u2705 <code>suffix</code> \u2705 \u2705 \u2705 <code>suffixes</code> \u2705 \u2705 \u2705 <code>touch</code> \u2705 \u2705 \u2705 <code>unlink</code> \u2705 \u2705 \u2705 <code>with_name</code> \u2705 \u2705 \u2705 <code>with_stem</code> \u2705 \u2705 \u2705 <code>with_suffix</code> \u2705 \u2705 \u2705 <code>write_bytes</code> \u2705 \u2705 \u2705 <code>write_text</code> \u2705 \u2705 \u2705 <code>as_posix</code> \u274c \u274c \u274c <code>chmod</code> \u274c \u274c \u274c <code>cwd</code> \u274c \u274c \u274c <code>expanduser</code> \u274c \u274c \u274c <code>group</code> \u274c \u274c \u274c <code>hardlink_to</code> \u274c \u274c \u274c <code>home</code> \u274c \u274c \u274c <code>is_block_device</code> \u274c \u274c \u274c <code>is_char_device</code> \u274c \u274c \u274c <code>is_fifo</code> \u274c \u274c \u274c <code>is_mount</code> \u274c \u274c \u274c <code>is_reserved</code> \u274c \u274c \u274c <code>is_socket</code> \u274c \u274c \u274c <code>is_symlink</code> \u274c \u274c \u274c <code>lchmod</code> \u274c \u274c \u274c <code>link_to</code> \u274c \u274c \u274c <code>lstat</code> \u274c \u274c \u274c <code>owner</code> \u274c \u274c \u274c <code>readlink</code> \u274c \u274c \u274c <code>root</code> \u274c \u274c \u274c <code>symlink_to</code> \u274c \u274c \u274c <code>as_url</code> \u2705 \u2705 \u2705 <code>clear_cache</code> \u2705 \u2705 \u2705 <code>cloud_prefix</code> \u2705 \u2705 \u2705 <code>copy</code> \u2705 \u2705 \u2705 <code>copytree</code> \u2705 \u2705 \u2705 <code>download_to</code> \u2705 \u2705 \u2705 <code>etag</code> \u2705 \u2705 \u2705 <code>fspath</code> \u2705 \u2705 \u2705 <code>is_junction</code> \u2705 \u2705 \u2705 <code>is_valid_cloudpath</code> \u2705 \u2705 \u2705 <code>rmtree</code> \u2705 \u2705 \u2705 <code>upload_from</code> \u2705 \u2705 \u2705 <code>validate</code> \u2705 \u2705 \u2705 <code>walk</code> \u2705 \u2705 \u2705 <code>with_segments</code> \u2705 \u2705 \u2705 <code>blob</code> \u2705 \u274c \u2705 <code>bucket</code> \u274c \u2705 \u2705 <code>container</code> \u2705 \u274c \u274c <code>key</code> \u274c \u2705 \u274c <code>md5</code> \u2705 \u274c \u274c <p><sup>Icon made by srip from www.flaticon.com.</sup> <sup>Sample code block generated using the reprexpy package.</sup></p>"},{"location":"anypath-polymorphism/","title":"AnyPath Polymorphic Class","text":"<p><code>cloudpathlib</code> implements a special <code>AnyPath</code> polymorphic class. This class will automatically instantiate a cloud path instance or a <code>pathlib.Path</code> instance appropriately from your input. It's also a virtual superclass of <code>CloudPath</code> and <code>Path</code>, so <code>isinstance</code> and <code>issubclass</code> checks will work in the expected way.</p> <p>This functionality can be handy for situations when you want to support both local filepaths and cloud storage filepaths. If you use <code>AnyPath</code>, your code can switch between them seamlessly based on the contents of provided filepaths without the need of any <code>if</code>-<code>else</code> conditional blocks.</p>"},{"location":"anypath-polymorphism/#example","title":"Example","text":"<pre><code>from cloudpathlib import AnyPath\n\npath = AnyPath(\"mydir/myfile.txt\")\npath\n#&gt; PosixPath('mydir/myfile.txt')\n\ncloud_path = AnyPath(\"s3://mybucket/myfile.txt\")\ncloud_path\n#&gt; S3Path('s3://mybucket/myfile.txt')\n\nisinstance(path, AnyPath)\n#&gt; True\nisinstance(cloud_path, AnyPath)\n#&gt; True\n</code></pre>"},{"location":"anypath-polymorphism/#file-uri-scheme","title":"<code>file:</code> URI Scheme","text":"<p><code>AnyPath</code> also supports the <code>file:</code> URI scheme for paths that can be referenced with pathlib and returns a <code>Path</code> instance for those paths. If you need to roundtrip back to a <code>file:</code> URI, you can use the <code>Path.as_uri</code> method after any path manipulations that you do.</p> <p>For example:</p> <pre><code>from cloudpathlib import AnyPath\n\n# hostname omitted variant\npath = AnyPath(\"file:/root/mydir/myfile.txt\")\npath\n#&gt; PosixPath('/root/mydir/myfile.txt')\n\n# explicit local path variant\npath = AnyPath(\"file:///root/mydir/myfile.txt\")\npath\n#&gt; PosixPath('/root/mydir/myfile.txt')\n\n# manipulate the path and return the file:// URI\nparent_uri = path.parent.as_uri()\nparent_uri\n#&gt; 'file:///root/mydir'\n</code></pre>"},{"location":"anypath-polymorphism/#how-it-works","title":"How It Works","text":"<p>The constructor for <code>AnyPath</code> will first attempt to run the input through the <code>CloudPath</code> base class' constructor, which will validate the input against registered concrete <code>CloudPath</code> implementations. This will accept inputs that are already a cloud path class or a string with the appropriate URI scheme prefix (e.g., <code>s3://</code>). If no implementation validates successfully, it will then try to run the input through the <code>Path</code> constructor. If the <code>Path</code> constructor fails and raises a <code>TypeError</code>, then the <code>AnyPath</code> constructor will raise an <code>AnyPathTypeError</code> exception.</p> <p>The virtual superclass functionality with <code>isinstance</code> and <code>issubclass</code> with the <code>__instancecheck__</code> and <code>__subclasscheck__</code> special methods per PEP 3119's specification.</p> <p><sup>Examples created with reprexlite</sup></p>"},{"location":"authentication/","title":"Authentication","text":"<p>For standard use, we recommend using environment variables to authenticate with the cloud storage services. This way, <code>cloudpathlib</code> will be able to automatically read those credentials and authenticate without you needing to do anything else. Passing credentials via environment variables is also generally a security best practice for avoiding accidental sharing.</p> <p><code>cloudpathlib</code> supports the standard environment variables used by each respective cloud service SDK.</p> Cloud Environment Variables SDK Documentation Amazon S3 <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> or <code>AWS_PROFILE</code> with credentials file Link Azure Blob Storage <code>AZURE_STORAGE_CONNECTION_STRING</code> Link Google Cloud Storage <code>GOOGLE_APPLICATION_CREDENTIALS</code> Link"},{"location":"authentication/#advanced-use","title":"Advanced Use","text":"<p>The communication between <code>cloudpathlib</code> and cloud storage services are handled by <code>Client</code> objects. Each cloud storage service has its own <code>Client</code> class implementation. See the linked API documentation pages for additional authentication options.</p> Cloud Client API Documentation Amazon S3 <code>S3Client</code> Link Azure Blob Storage <code>AzureBlobClient</code> Link Google Cloud Storage <code>GSClient</code> Link <p>A client object holds the authenticated connection with a cloud service, as well as the configuration for the local cache. When you create instantiate a cloud path instance for the first time, a default client object is created for the respective cloud service.</p> <pre><code>from cloudpathlib import CloudPath\n\ncloud_path = CloudPath(\"s3://cloudpathlib-test-bucket/\")   # same for S3Path(...)\ncloud_path.client\n#&gt; &lt;cloudpathlib.s3.s3client.S3Client at 0x7feac3d1fb90&gt;\n</code></pre> <p>All subsequent instances of that service's cloud paths (in the example, all subsequent <code>S3Path</code> instances) will reference the same client instance.</p> <p>You can also explicitly instantiate a client instance. You will need to do so if you want to authenticate using any option other than the environment variables from the table in the previous section. (To see what those options are, check out the API documentation pages linked to in the table above.) You can then use that client instance's cloud path factory method, or pass it into a cloud path instantiation.</p> <pre><code>from cloudpathlib import S3Client\n\nclient = S3Client(aws_access_key_id=\"myaccesskey\", aws_secret_access_key=\"mysecretkey\")\n\n# these next two commands are equivalent\n# use client's factory method\ncp1 = client.CloudPath(\"s3://cloudpathlib-test-bucket/\")\n# or pass client as keyword argument\ncp2 = CloudPath(\"s3://cloudpathlib-test-bucket/\", client=client)\n</code></pre> <p>If you have instantiated a client instance explicitly, you can also set it as the default client. Then, future cloud paths without a client specified will use that client instance.</p> <pre><code>client = S3Client(aws_access_key_id=\"myaccesskey\", aws_secret_access_key=\"mysecretkey\")\nclient.set_as_default_client()\n</code></pre> <p>If you need a reference to the default client:</p> <pre><code>S3Client.get_default_client()\n#&gt; &lt;cloudpathlib.s3.s3client.S3Client at 0x7feac3d1fb90&gt;\n</code></pre>"},{"location":"authentication/#accessing-public-s3-buckets-without-credentials","title":"Accessing public S3 buckets without credentials","text":"<p>For most operations, you will need to have your S3 credentials configured. However, for buckets that provide public access, you can use <code>cloudpathlib</code> without credentials. To do so, you need to instantiate a client and pass the kwarg <code>no_sign_request=True</code>. Failure to do so will result in a <code>NoCredentialsError</code> being thrown.</p> <pre><code>from cloudpathlib import CloudPath\n\n# this file deinitely exists, but credentials are not configured\nCloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0001_5a63d42e-27c6-448a-84f1-bfc632125b8e.jpg\").exists()\n\n#&gt; NoCredentialsError\n</code></pre> <p>Instead, you must either configure credentials or instantiate a client object using <code>no_sign_request=True</code>:</p> <pre><code>from cloudpathlib import S3Client\n\nc = S3Client(no_sign_request=True)\n\n# use this client object to create the CloudPath\nc.CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0001_5a63d42e-27c6-448a-84f1-bfc632125b8e.jpg\").exists()\n#&gt; True\n</code></pre> <p>Note: Many public buckets do not allow listing of the bucket contents by anonymous users. If this is the case, any listing operation on a directory will fail with an error like <code>ClientError: An error occurred (AccessDenied) when calling the ListObjectsV2 operation: Access Denied</code> when you try to do an operation, even with <code>no_sign_request=True</code>. In this case, you can generally only work with <code>CloudPath</code> objects that refer to the files themselves (instead of directories). You can contact the bucket owner to request that they allow listing, or write your code in a way that only references files you know will exist.</p> <p>As noted above, you can also call <code>.set_as_default_client()</code> on the client object that you create and then it will be used by default without your having to explicitly use the client object that you created.</p>"},{"location":"authentication/#requester-pays-buckets-on-s3","title":"Requester Pays buckets on S3","text":"<p>S3 supports Requester Pays buckets where you must have credentials to access the bucket and any costs are passed on to you rather than the owner of the bucket.</p> <p>For a requester pays bucket, you need to pass extras telling cloudpathlib you will pay for any operations.</p> <p>For example, on the requester pays bucket <code>arxiv</code>, just trying to list the contents will result in a <code>ClientError</code>:</p> <pre><code>from cloudpathlib import CloudPath\n\ntars = list(CloudPath(\"s3://arxiv/src/\").iterdir())\nprint(tars)\n\n#&gt; ClientError: An error occurred (AccessDenied) ...\n</code></pre> <p>To indicate that the request payer will be the \"requester,\" pass the extra args to an <code>S3Client</code> and use that client to instantiate paths:</p> <pre><code>from cloudpathlib import S3Client\n\nc = S3Client(extra_args={\"RequestPayer\": \"requester\"})\n\n# use the client we created to build the path\ntars = list(c.CloudPath(\"s3://arxiv/src/\").iterdir())\nprint(tars)\n</code></pre> <p>As noted above, you can also call <code>.set_as_default_client()</code> on the client object that you create and then it will be used by default without your having to explicitly use the client object that you created.</p>"},{"location":"authentication/#other-s3-extraargs-in-boto3","title":"Other S3 <code>ExtraArgs</code> in <code>boto3</code>","text":"<p>The S3 SDK, <code>boto3</code> supports a set of <code>ExtraArgs</code> for uploads, downloads, and listing operations. When you instatiate a client, you can pass the <code>extra_args</code> keyword argument with any of those extra args that you want to set. We will pass these on to the upload, download, and list methods insofar as those methods support the specific args.</p> <p>The args supported for uploads are the same as <code>boto3.s3.transfer.S3Transfer.ALLOWED_UPLOAD_ARGS</code>, see the <code>boto3</code> documentation for the latest, but as of the time of writing, these are:</p> <ul> <li><code>ACL</code></li> <li><code>CacheControl</code></li> <li><code>ChecksumAlgorithm</code></li> <li><code>ContentDisposition</code></li> <li><code>ContentEncoding</code></li> <li><code>ContentLanguage</code></li> <li><code>ContentType</code></li> <li><code>ExpectedBucketOwner</code></li> <li><code>Expires</code></li> <li><code>GrantFullControl</code></li> <li><code>GrantRead</code></li> <li><code>GrantReadACP</code></li> <li><code>GrantWriteACP</code></li> <li><code>Metadata</code></li> <li><code>ObjectLockLegalHoldStatus</code></li> <li><code>ObjectLockMode</code></li> <li><code>ObjectLockRetainUntilDate</code></li> <li><code>RequestPayer</code></li> <li><code>ServerSideEncryption</code></li> <li><code>StorageClass</code></li> <li><code>SSECustomerAlgorithm</code></li> <li><code>SSECustomerKey</code></li> <li><code>SSECustomerKeyMD5</code></li> <li><code>SSEKMSKeyId</code></li> <li><code>SSEKMSEncryptionContext</code></li> <li><code>Tagging</code></li> <li><code>WebsiteRedirectLocation</code></li> </ul> <p>The args supported for downloads are the same as <code>boto3.s3.transfer.S3Transfer.ALLOWED_DOWNLOAD_ARGS</code>, see the <code>boto3</code> documentation for the latest, but as of the time of writing, these are:</p> <ul> <li><code>ChecksumMode</code></li> <li><code>VersionId</code></li> <li><code>SSECustomerAlgorithm</code></li> <li><code>SSECustomerKey</code></li> <li><code>SSECustomerKeyMD5</code></li> <li><code>RequestPayer</code></li> <li><code>ExpectedBucketOwner</code></li> </ul> <p>To use any of these extra args, pass them as a dict to <code>extra_args</code> when instantiating and <code>S3Client</code>.</p> <pre><code>from cloudpathlib import S3Client\n\nc = S3Client(extra_args={\n    \"ChecksumMode\": \"ENABLED\",  # download extra arg, only used when downloading\n    \"ACL\": \"public-read\",       # upload extra arg, only used when uploading\n})\n\n# use these extras for all CloudPaths\nc.set_as_default_client()\n</code></pre> <p>Note: The <code>extra_args</code> kwargs accepts the union of upload and download args, and will only pass on the relevant subset to the <code>boto3</code> method that is called by the internals of <code>S3Client</code>.</p> <p>Note: The ExtraArgs on the client will be used for every call that client makes. If you need to set different <code>ExtraArgs</code> in different code paths, we recommend creating separate explicit client objects and using those to create and manage the CloudPath objects with different needs.</p> <p>Note: To explicitly set the <code>ContentType</code> and <code>ContentEncoding</code>, we recommend using the <code>content_type_method</code> kwarg when instantiating the client. If instead you want to set this for all uploads via the extras, you must additionally pass <code>content_type_method=None</code> to the <code>S3Client</code> so we don't try to guess these automatically.</p>"},{"location":"authentication/#accessing-custom-s3-compatible-object-stores","title":"Accessing custom S3-compatible object stores","text":"<p>It might happen so that you need to access a customly deployed S3 object store (MinIO, Ceph or any other). In such cases, the service endpoint will be different from the AWS object store endpoints (used by default). To specify a custom endpoint address, you will need to manually instantiate <code>Client</code> with the <code>endpoint_url</code> parameter, provinding http/https URL including port.</p> <pre><code>from cloudpathlib import S3Client, CloudPath\n\n# create a client pointing to the endpoint\nclient = S3Client(endpoint_url=\"http://my.s3.server:1234\")\n\n# option 1: use the client to create paths\ncp1 = client.CloudPath(\"s3://cloudpathlib-test-bucket/\")\n\n# option 2: pass the client as keyword argument\ncp2 = CloudPath(\"s3://cloudpathlib-test-bucket/\", client=client)\n\n# option3: set this client as the default so it is used in any future paths\nclient.set_as_default_client()\ncp3 = CloudPath(\"s3://cloudpathlib-test-bucket/\")\n</code></pre>"},{"location":"authentication/#accessing-azure-datalake-storage-gen2-adls-gen2-storage-with-hierarchical-namespace-enabled","title":"Accessing Azure DataLake Storage Gen2 (ADLS Gen2) storage with hierarchical namespace enabled","text":"<p>Some Azure storage accounts are configured with \"hierarchical namespace\" enabled. This means that the storage account is backed by the Azure DataLake Storage Gen2 product rather than Azure Blob Storage. For many operations, the two are the same and one can use the Azure Blob Storage API. However, for some operations, a developer will need to use the Azure DataLake Storage API. The <code>AzureBlobClient</code> class implemented in cloudpathlib is designed to detect if hierarchical namespace is enabled and use the Azure DataLake Storage API in the places where it is necessary or it provides a performance improvement. Usually, a user of cloudpathlib will not need to know if hierarchical namespace is enabled and the storage account is backed by Azure DataLake Storage Gen2 or Azure Blob Storage.</p> <p>If needed, the Azure SDK provided <code>DataLakeServiceClient</code> object can be accessed via the <code>AzureBlobClient.data_lake_client</code>. The Azure SDK provided <code>BlobServiceClient</code> object can be accessed via <code>AzureBlobClient.service_client</code>.</p>"},{"location":"authentication/#pickling-cloudpath-objects","title":"Pickling <code>CloudPath</code> objects","text":"<p>You can pickle and unpickle <code>CloudPath</code> objects normally, for example:</p> <pre><code>from pathlib import Path\nimport pickle\n\nfrom cloudpathlib import CloudPath\n\n\nwith Path(\"cloud_path.pkl\").open(\"wb\") as f:\n    pickle.dump(CloudPath(\"s3://my-awesome-bucket/cool-file.txt\"), f)\n\nwith Path(\"cloud_path.pkl\").open(\"rb\") as f:\n    pickled = pickle.load(f)\n\nassert pickled.bucket == \"my-awesome-bucket\"\n</code></pre> <p>The associated <code>client</code>, however, is not pickled. When a <code>CloudPath</code> is  unpickled, the client on the unpickled object will be set to the default  client for that class.</p> <p>For example, this will not work:</p> <pre><code>from pathlib import Path\nimport pickle\n\nfrom cloudpathlib import S3Client, CloudPath\n\n\n# create a custom client pointing to the endpoint\nclient = S3Client(endpoint_url=\"http://my.s3.server:1234\")\n\n# use that client when creating a cloud path\np = CloudPath(\"s3://cloudpathlib-test-bucket/cool_file.txt\", client=client)\np.write_text(\"hello!\")\n\nwith Path(\"cloud_path.pkl\").open(\"wb\") as f:\n    pickle.dump(p, f)\n\nwith Path(\"cloud_path.pkl\").open(\"rb\") as f:\n    pickled = pickle.load(f)\n\n# this will be False, because it will use the default `S3Client`\nassert pickled.exists() == False\n</code></pre> <p>To get this to work, you need to set the custom <code>client</code> to the default before unpickling:</p> <pre><code># set the custom client as the default before unpickling\nclient.set_as_default_client()\n\nwith (\"cloud_path.pkl\").open(\"rb\") as f:\n    pickled2 = pickle.load(f)\n\nassert pickled2.exists()\nassert pickled2.client == client\n</code></pre>"},{"location":"caching/","title":"Caching","text":"In\u00a0[1]: Copied! <pre>from cloudpathlib import CloudPath\nfrom itertools import islice\n\nladi = CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349\")\n\n# list first 5 images for this incident\nfor p in islice(ladi.iterdir(), 5):\n    print(p)\n</pre> from cloudpathlib import CloudPath from itertools import islice  ladi = CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349\")  # list first 5 images for this incident for p in islice(ladi.iterdir(), 5):     print(p) <pre>s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0001_5a63d42e-27c6-448a-84f1-bfc632125b8e.jpg\ns3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\ns3://ladi/Images/FEMA_CAP/2020/70349/DSC_0003_02c30af6-911e-4e01-8c24-7644da2b8672.jpg\ns3://ladi/Images/FEMA_CAP/2020/70349/DSC_0004_d37c02b9-01a8-4672-b06f-2690d70e5e6b.jpg\ns3://ladi/Images/FEMA_CAP/2020/70349/DSC_0005_d05609ce-1c45-4de3-b0f1-401c2bb3412c.jpg\n</pre> <p>Just because we saw these images are available, it doesn't mean we have downloaded any of this data yet.</p> In\u00a0[2]: Copied! <pre># Nothing in the cache yet\n!tree {ladi.fspath}\n</pre> # Nothing in the cache yet !tree {ladi.fspath} <pre>/var/folders/8g/v8lwvfhj6_l6ct_zd_rs84mw0000gn/T/tmpn5oh1rkm/ladi/Images/FEMA_CAP/2020/70349 [error opening dir]\r\n\r\n0 directories, 0 files\r\n</pre> <p>Now let's look at just the first image from this dataset, confirming that the file exists on S3.</p> In\u00a0[3]: Copied! <pre>flood_image = ladi / \"DSC_0001_5a63d42e-27c6-448a-84f1-bfc632125b8e.jpg\"\nflood_image.exists()\n</pre> flood_image = ladi / \"DSC_0001_5a63d42e-27c6-448a-84f1-bfc632125b8e.jpg\" flood_image.exists() Out[3]: <pre>True</pre> In\u00a0[4]: Copied! <pre># Still nothing in the cache\n!tree {ladi.fspath}\n</pre> # Still nothing in the cache !tree {ladi.fspath} <pre>/var/folders/8g/v8lwvfhj6_l6ct_zd_rs84mw0000gn/T/tmpn5oh1rkm/ladi/Images/FEMA_CAP/2020/70349 [error opening dir]\r\n\r\n0 directories, 0 files\r\n</pre> <p>Even though we refer to a specific file and make sure it exists in the cloud, we can still do all of that work without actually downloading the file.</p> <p>In order to read the file, we do have to download the data. Let's actually display the image:</p> In\u00a0[5]: Copied! <pre>%%time\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom PIL import Image\n\nwith flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    plt.imshow(i)\n</pre> %%time %matplotlib inline import matplotlib.pyplot as plt from PIL import Image  with flood_image.open(\"rb\") as f:     i = Image.open(f)     plt.imshow(i) <pre>CPU times: user 1.35 s, sys: 435 ms, total: 1.78 s\nWall time: 1.76 s\n</pre> In\u00a0[6]: Copied! <pre># Downloaded image file in the cache\n!tree {ladi.fspath}\n</pre> # Downloaded image file in the cache !tree {ladi.fspath} <pre>/var/folders/8g/v8lwvfhj6_l6ct_zd_rs84mw0000gn/T/tmpn5oh1rkm/ladi/Images/FEMA_CAP/2020/70349\r\n\u2514\u2500\u2500 DSC_0001_5a63d42e-27c6-448a-84f1-bfc632125b8e.jpg\r\n\r\n0 directories, 1 file\r\n</pre> <p>Just by using <code>open</code>, we've downloaded the file in the background to the cache. Now that it is local, we won't redownload that file unless it changes on the server. We can confirm that by checking if the file is faster to read a second time.</p> In\u00a0[7]: Copied! <pre>%%time\nwith flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    plt.imshow(i)\n</pre> %%time with flood_image.open(\"rb\") as f:     i = Image.open(f)     plt.imshow(i) <pre>CPU times: user 233 ms, sys: 69.7 ms, total: 303 ms\nWall time: 491 ms\n</pre> <p>Notice that the second display is much faster since we use the cached version!</p> In\u00a0[8]: Copied! <pre>from cloudpathlib import S3Client\n\n# explicitly instantiate a client that always uses the local cache\nclient = S3Client(local_cache_dir=\"data\")\n\nladi = client.CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349\")\n</pre> from cloudpathlib import S3Client  # explicitly instantiate a client that always uses the local cache client = S3Client(local_cache_dir=\"data\")  ladi = client.CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349\") In\u00a0[9]: Copied! <pre># Again, nothing in the cache yet, but we the path is now in the \"data\" folder\n!tree {ladi.fspath}\n</pre> # Again, nothing in the cache yet, but we the path is now in the \"data\" folder !tree {ladi.fspath} <pre>data/ladi/Images/FEMA_CAP/2020/70349 [error opening dir]\r\n\r\n0 directories, 0 files\r\n</pre> <p>Now let's look at just the first image from this dataset. Note that paths created by using the <code>ladi</code> root (e.g., by using the <code>/</code> operator below or calls like <code>iterdir</code> and <code>glob</code>) will inherit the same <code>Client</code> instance, and therefore the same <code>local_cache_dir</code> without our having to do extra work.</p> In\u00a0[10]: Copied! <pre>flood_image = ladi / \"DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n\nwith flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    plt.imshow(i)\n</pre> flood_image = ladi / \"DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"  with flood_image.open(\"rb\") as f:     i = Image.open(f)     plt.imshow(i) In\u00a0[11]: Copied! <pre># Now just this one image file is in the cache\n!tree {ladi.fspath}\n</pre> # Now just this one image file is in the cache !tree {ladi.fspath} <pre>data/ladi/Images/FEMA_CAP/2020/70349\r\n\u2514\u2500\u2500 DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\r\n\r\n0 directories, 1 file\r\n</pre> In\u00a0[12]: Copied! <pre># let's explicitly cleanup this directory, since it is not handled for us\n!rm -rf data\n</pre> # let's explicitly cleanup this directory, since it is not handled for us !rm -rf data In\u00a0[13]: Copied! <pre>from cloudpathlib.enums import FileCacheMode\n\nprint(\"\\n\".join(FileCacheMode))\n</pre> from cloudpathlib.enums import FileCacheMode  print(\"\\n\".join(FileCacheMode)) <pre>persistent\ntmp_dir\ncloudpath_object\nclose_file\n</pre> In\u00a0[14]: Copied! <pre># pass as string to client instantiation\nno_cache_client = S3Client(file_cache_mode=\"close_file\")\n\nflood_image = no_cache_client.CloudPath(\n    \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n)\n\nwith flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    print(\"Image loaded...\")\n\n# Illustrate that even though we read the file, the cache version does not exist\nprint(\"Cache file exists after finished reading: \", flood_image._local.exists())\n</pre> # pass as string to client instantiation no_cache_client = S3Client(file_cache_mode=\"close_file\")  flood_image = no_cache_client.CloudPath(     \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" )  with flood_image.open(\"rb\") as f:     i = Image.open(f)     print(\"Image loaded...\")  # Illustrate that even though we read the file, the cache version does not exist print(\"Cache file exists after finished reading: \", flood_image._local.exists()) <pre>Image loaded...\nCache file exists after finished reading:  False\n</pre> In\u00a0[15]: Copied! <pre># pass enum to client instantiation\ncloud_path_client = S3Client(file_cache_mode=FileCacheMode.cloudpath_object)\n\nflood_image = cloud_path_client.CloudPath(\n    \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n)\n\nwith flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    print(\"Image loaded...\")\n\n\n# cache exists while the CloudPath object persists\nlocal_cached_file = flood_image._local\nprint(\"Cache file exists after finished reading: \", local_cached_file.exists())\n\n# decrement reference count so garbage collector cleans up the file\ndel flood_image\n\n# file is now cleaned up\nprint(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists())\n</pre> # pass enum to client instantiation cloud_path_client = S3Client(file_cache_mode=FileCacheMode.cloudpath_object)  flood_image = cloud_path_client.CloudPath(     \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" )  with flood_image.open(\"rb\") as f:     i = Image.open(f)     print(\"Image loaded...\")   # cache exists while the CloudPath object persists local_cached_file = flood_image._local print(\"Cache file exists after finished reading: \", local_cached_file.exists())  # decrement reference count so garbage collector cleans up the file del flood_image  # file is now cleaned up print(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists()) <pre>Image loaded...\nCache file exists after finished reading:  True\nCache file exists after CloudPath is no longer referenced:  False\n</pre> In\u00a0[16]: Copied! <pre>tmp_dir_client = S3Client()\n\nflood_image = tmp_dir_client.CloudPath(\n    \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n)\n\nwith flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    print(\"Image loaded...\")\n\n# cache exists while the CloudPath object persists\nlocal_cached_file = flood_image._local\nprint(\"Cache file exists after finished reading: \", local_cached_file.exists())\n\n# decrement reference count so garbage collection runs\ndel flood_image\n\n# file still exists\nprint(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists())\n\n# decrement reference count so garbage collector removes the client\ndel tmp_dir_client\n\n# file still exists\nprint(\"Cache file exists after Client is no longer referenced: \", local_cached_file.exists())\n</pre> tmp_dir_client = S3Client()  flood_image = tmp_dir_client.CloudPath(     \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" )  with flood_image.open(\"rb\") as f:     i = Image.open(f)     print(\"Image loaded...\")  # cache exists while the CloudPath object persists local_cached_file = flood_image._local print(\"Cache file exists after finished reading: \", local_cached_file.exists())  # decrement reference count so garbage collection runs del flood_image  # file still exists print(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists())  # decrement reference count so garbage collector removes the client del tmp_dir_client  # file still exists print(\"Cache file exists after Client is no longer referenced: \", local_cached_file.exists()) <pre>Image loaded...\nCache file exists after finished reading:  True\nCache file exists after CloudPath is no longer referenced:  True\nCache file exists after Client is no longer referenced:  False\n</pre> In\u00a0[17]: Copied! <pre>persistent_client = S3Client(local_cache_dir=\"./cache\")\n\n# cache mode set automatically to persistent if local_cache_dir and not explicit\nprint(\"Client cache mode set to: \", persistent_client.file_cache_mode)\n\n# Just uses default client\nflood_image = persistent_client.CloudPath(\n    \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n)\n\nwith flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    print(\"Image loaded...\")\n\n# cache exists while the CloudPath object persists\nlocal_cached_file = flood_image._local\nprint(\"Cache file exists after finished reading: \", local_cached_file.exists())\n\n# decrement reference count so garbage collection runs\ndel flood_image\n\n# file still exists\nprint(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists())\n\n# decrement reference count so garbage collector removes the client\nclient_cache_dir = persistent_client._local_cache_dir\ndel persistent_client\n\n# file still exists\nprint(\"Cache file exists after Client is no longer referenced: \", local_cached_file.exists())\n\n\n# explicitly remove persistent cache file\nimport shutil\n\nshutil.rmtree(client_cache_dir)\n</pre> persistent_client = S3Client(local_cache_dir=\"./cache\")  # cache mode set automatically to persistent if local_cache_dir and not explicit print(\"Client cache mode set to: \", persistent_client.file_cache_mode)  # Just uses default client flood_image = persistent_client.CloudPath(     \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" )  with flood_image.open(\"rb\") as f:     i = Image.open(f)     print(\"Image loaded...\")  # cache exists while the CloudPath object persists local_cached_file = flood_image._local print(\"Cache file exists after finished reading: \", local_cached_file.exists())  # decrement reference count so garbage collection runs del flood_image  # file still exists print(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists())  # decrement reference count so garbage collector removes the client client_cache_dir = persistent_client._local_cache_dir del persistent_client  # file still exists print(\"Cache file exists after Client is no longer referenced: \", local_cached_file.exists())   # explicitly remove persistent cache file import shutil  shutil.rmtree(client_cache_dir) <pre>Client cache mode set to:  FileCacheMode.persistent\nImage loaded...\nCache file exists after finished reading:  True\nCache file exists after CloudPath is no longer referenced:  True\nCache file exists after Client is no longer referenced:  True\n</pre> <p>We show an example below of <code>InvalidConfigurationException</code> being raised with the mode being interpreted from the environment variable.</p> In\u00a0[18]: Copied! <pre>import os\n\n# set the mode here so that it will be used when we instantiate the client\nos.environ[\"CLOUDPATHLIB_FILE_CACHE_MODE\"] = \"persistent\"\n\ntmp_dir_client = S3Client()\n</pre> import os  # set the mode here so that it will be used when we instantiate the client os.environ[\"CLOUDPATHLIB_FILE_CACHE_MODE\"] = \"persistent\"  tmp_dir_client = S3Client() <pre>\n---------------------------------------------------------------------------\nInvalidConfigurationException             Traceback (most recent call last)\nCell In[18], line 6\n      3 # set the mode here so that it will be used when we instantiate the client\n      4 os.environ[\"CLOUDPATHLIB_FILE_CACHE_MODE\"] = \"persistent\"\n----&gt; 6 tmp_dir_client = S3Client()\n\nFile ~/code/cloudpathlib/cloudpathlib/s3/s3client.py:129, in S3Client.__init__(self, aws_access_key_id, aws_secret_access_key, aws_session_token, no_sign_request, botocore_session, profile_name, boto3_session, file_cache_mode, local_cache_dir, endpoint_url, boto3_transfer_config, content_type_method, extra_args)\n    121 # listing ops (list_objects_v2, filter, delete) only accept these extras:\n    122 # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html\n    123 self.boto3_list_extra_args = {\n    124     k: self._extra_args[k]\n    125     for k in [\"RequestPayer\", \"ExpectedBucketOwner\"]\n    126     if k in self._extra_args\n    127 }\n--&gt; 129 super().__init__(\n    130     local_cache_dir=local_cache_dir,\n    131     content_type_method=content_type_method,\n    132     file_cache_mode=file_cache_mode,\n    133 )\n\nFile ~/code/cloudpathlib/cloudpathlib/client.py:57, in Client.__init__(self, file_cache_mode, local_cache_dir, content_type_method)\n     54     file_cache_mode = FileCacheMode.persistent\n     56 if file_cache_mode == FileCacheMode.persistent and local_cache_dir is None:\n---&gt; 57     raise InvalidConfigurationException(\n     58         f\"If you use the '{FileCacheMode.persistent}' cache mode, you must pass a `local_cache_dir` when you instantiate the client.\"\n     59     )\n     61 # if no explicit local dir, setup caching in temporary dir\n     62 if local_cache_dir is None:\n\nInvalidConfigurationException: If you use the 'FileCacheMode.persistent' cache mode, you must pass a `local_cache_dir` when you instantiate the client.</pre>"},{"location":"caching/#caching","title":"Caching\u00b6","text":"<p>Interacting with files on a cloud provider can mean a lot of waiting on files downloading and uploading. <code>cloudpathlib</code> provides seamless on-demand caching of cloud content that can be persistent across processes and sessions to make sure you only download or upload when you need to.</p>"},{"location":"caching/#are-we-synced","title":"Are we synced?\u00b6","text":"<p>Before <code>cloudpathlib</code>, we spent a lot of time syncing our remote and local files. There was no great solution. For example, I just need one file, but I only have a script that downloads the entire 800GB bucket (or worse, you can't remember exactly which files you need \ud83e\udd2e). Or even worse, you have all the files synced to your local machine, but you suspect that some are are up-to-date and some are stale. More often that I'd like to admit, the simplest answer was to blast the whole data directory and download all over again. Bandwidth doesn't grow on trees!</p>"},{"location":"caching/#cache-me-if-you-can","title":"Cache me if you can\u00b6","text":"<p>Part of what makes <code>cloudpathlib</code> so useful is that it takes care of all of that, leaving your precious mental resources free to do other things! It maintains a local cache and only downloads a file if the local version and remote versions are out of sync. Every time you read or write a file, <code>cloudpathlib</code> goes through these steps:</p> <ul> <li>Does the file exist in the cache already?</li> <li>If no, download it to the cache.</li> <li>If yes, does the cached version have the same modified time as the cloud version?</li> <li>If it is older, re-download the file and replace the old cached version with the updated version from the cloud.</li> <li>If the local one is newer, something is up! We don't want to overwrite your local changes with the version from the cloud. If we see this scenario, we'll raise an error and offer some options to resolve the versions.</li> </ul>"},{"location":"caching/#supporting-reading-and-writing","title":"Supporting reading and writing\u00b6","text":"<p>The cache logic also support writing to cloud files seamlessly in addition to reading. We do this by tracking when a <code>CloudPath</code> is opened and on the close of that file, we will upload the new version to the cloud if it has changed.</p> <p>Warning we don't upload files that weren't opened for write by <code>cloudpathlib</code>. For example, if you edit a file in the cache manually in a text edior, <code>cloudpathlib</code> won't know to update that file on the cloud. If you want to write to a file in the cloud, you should use the <code>open</code> or <code>write</code> methods, for example:</p> <pre>with my_cloud_path.open(\"w\") as f:\n    f.write(\"My new text!\")\n</pre> <p>This will download the file, write the text to the local version in the cache, and when that file is closed we know to upload the changed version to the cloud.</p> <p>As an example, let's look at using the Low Altitude Disaster Imagery open dataset on S3. We'll view one images available of a flooding incident available on S3.</p>"},{"location":"caching/#keeping-the-cache-around","title":"Keeping the cache around\u00b6","text":"<p>By default, the cache uses <code>tempfile</code> this means at some point either Python or your operating system will remove whatever files you have cached. This is helpful in that it means the downloaded files get cleaned up regularly and don't necessarily clutter up your local hard drive. If you want more control over how and when the cache is removed, see the Clearing the file cache section.</p> <p>However, sometimes I don't want to have to re-download files I know won't change. For example, in the LADI dataset, I may want to use the images in a Jupyter notebook and every time I restart the notebook I want to always have the downloaded files. I don't want to ever re-download since I know the LADI images won't be changing on S3. I want these to be there, even if I restart my whole machine.</p> <p>We can do this just by using a <code>Client</code> that does all the downloading/uploading to a specfic folder on our local machine. We set the cache folder by passing <code>local_cache_dir</code> to the <code>Client</code> when instantiating. You can also set a default for all clients by setting the <code>CLOUDPATHLIB_LOCAL_CACHE_DIR</code> to a path. (This is only recommended with (1) an absolute path, so you know where the cache is no matter where your code is running, and (2) if you only use the default client for one cloud provider and don't instantiate multiple. In this case, the clients will use the same cache dir and could overwrite each other's content. Setting <code>CLOUDPATHLIB_LOCAL_CACHE_DIR</code> to an empty string will be treated as it not being set.)</p>"},{"location":"caching/#accessing-the-cached-version-directly-read-only","title":"Accessing the cached version directly (read-only)\u00b6","text":"<p>Many Python libraries don't properly handle <code>PathLike</code> objects. These libraries often only expect a <code>str</code> to be passed when working with files or, even worse, they will call <code>str(p)</code> on a Path that is passed before using it.</p> <p>To use <code>cloudpathlib</code> with these libraries, you can pass <code>.fspath</code> which will provide the path to the cached version of the file as a string.</p> <p>Warning: Using the <code>.fspath</code> property will download the file from the cloud if it does not exist yet in the cache.</p> <p>Warning: Since we are no longer in control of opening/closing the file, we cannot upload any changes when the file is closed. Therefore, you should treat any code where you use <code>fspath</code> as read only. Writes directly to <code>fspath</code> will not be uploaded to the cloud.</p>"},{"location":"caching/#handling-conflicts","title":"Handling conflicts\u00b6","text":"<p>We try to be conservative in terms of not losing data\u2014especially data stored on the cloud, which is likely to be the canonical version. Given this, we will raise exceptions in two scenarios:</p> <p><code>OverwriteNewerLocalError</code> This exception is raised if we are asked to download a file, but our local version in the cache is newer. This likely means that the cached version has been updated, but not pushed to the cloud. To work around this you could remove the cache version explicitly if you know you don't need that data. If you did write changes you need, make sure your code uses the <code>cloudpathlib</code> versions of the <code>open</code>, <code>write_text</code>, or <code>write_bytes</code> methods, which will upload your changes to the cloud automatically.</p> <p>The <code>CloudPath.open</code> method supports a <code>force_overwrite_from_cloud</code> kwarg to force overwriting your local version.</p> <p>You can make overwriting the cache with the cloud copy the default by setting the environment variable <code>CLOUDPATHLIB_FORCE_OVERWRITE_FROM_CLOUD=1</code> or <code>CLOUDPATHLIB_FORCE_OVERWRITE_FROM_CLOUD=True</code>.</p> <p><code>OverwriteNewerCloudError</code> This exception is raised if we are asked to upload a file, but the one on the cloud is newer than our local version. This likely means that a separate process has updated the cloud version, and we don't want to overwrite and lose that new data in the cloud.</p> <p>The <code>CloudPath.open</code> method supports a <code>force_overwrite_to_cloud</code> kwarg to force overwriting the cloud version.</p> <p>You can make overwriting the cloud copy with the local one being uploaded by setting the environment variable <code>CLOUDPATHLIB_FORCE_OVERWRITE_TO_CLOUD=1</code> or <code>CLOUDPATHLIB_FORCE_OVERWRITE_TO_CLOUD=True</code>.</p>"},{"location":"caching/#clearing-the-file-cache","title":"Clearing the file cache\u00b6","text":"<p>There's no perfect strategy for when to clear the file cache, and different applications will have different requirements and preferences. <code>cloudpathlib</code> provides fine-grained control over when cloud files are removed from the local disk. The cache can be emptied both manually and automatically. Because <code>cloudpathlib</code> uploads any changed files opened with <code>CloudPath.open</code> to the cloud as soon as they are closed, it is safe to delete the cached version of the file at any point as long as the file is not opened for writing at the time you are trying to remove it. If necessary, the file will be re-downloaded next time it is needed.</p> <p>We provide a number of ways to clear the file cache that can be useful if you want to reclaim disk space or if you don't need to keep cached files around.</p>"},{"location":"caching/#manually","title":"Manually\u00b6","text":"<p>It is recommended you pick an automatic strategy that works for your application. However, if you need to, you can clear the cache manually in three different ways: for individual CloudPath files, at the level of a <code>Client</code> instance, or at the operating system level.</p> <ul> <li><code>CloudPath.clear_cache()</code> - for an individual <code>CloudPath</code> remove the cached version of the file if it exists.</li> <li><code>*Client.clear_cache()</code> - All files downloaded by this specific client instance will be removed from the cache. If you didn't create a client instance yourself, you can get the one that is used by a cloudpath with <code>CloudPath.client</code> or get the default one for a particular provider with <code>get_default_client</code>, for example by calling <code>S3Client.get_default_client().clear_cache()</code>.</li> <li>By deleting the cached file itself or the containing directory using any normal method. To see where on a disk the cache is, you can use <code>CloudPath.fspath</code> for an individual file or use <code>*Client._local_cache_dir</code> for the client's cache. You can then use any method you like to delete these local files.</li> </ul> <p>However, for most cases, you shouldn't need to manage the file cache manually. By setting the automatic cache clearing beahvior to the most appropriate one for your use case below, you can have the cache automatically cleared.</p>"},{"location":"caching/#automatically","title":"Automatically\u00b6","text":"<p>We provide a number of different ways for the cache to get cleared automatically for you depending on your use case. These range from no cache clearing done by <code>cloudpathlib</code> (<code>\"persistent\"</code>), to the most aggressive (<code>\"close_file\"</code>), which deletes a file from the cache as soon as the file handle is closed and the file is uploaded to the cloud, if it was changed).</p> <p>The modes are defined in the <code>FileCacheMode</code> enum, which you can use directly or you can use the corresponding string value. Examples of both methods are included below.</p> <p>Note: There is not currently a cache mode that never writes a file to disk and only keeps it in memory.</p> <ul> <li><code>\"persistent\"</code> - <code>cloudpathlib</code> does not clear the cache at all. In this case, you must also pass a <code>local_cache_dir</code> when you instantiate the client.</li> <li><code>\"tmp_dir\"</code> (default) - Cached files are saved using Python's <code>TemporaryDirectory</code>. This provides three potential avenues for the cache to get cleared. First, cached files are removed by <code>cloudpathlib</code> when the <code>*Client</code> object is garbage collected. This happens on the next garbage collection run after the object leaves scope or <code>del</code> is called. Second, Python clears a temporary directory if all references to that directory leave scope. Finally since the folder is in an operating system temp directory, it will be cleared by the OS (which, depending on the OS, may not happen until system restart).</li> <li><code>\"cloudpath_object\"</code> - cached files are removed when the <code>CloudPath</code> object is garbage collected. This happens on the next garbage collection run after the object leaves scope or <code>del</code> is called.</li> <li><code>\"close_file\"</code> - since we only download a file to the cache on read/write, we can ensure the cache is empty by removing the cached file as soon as the read/write is finished. Reading/writing the same <code>CloudPath</code> multiple times will result in re-downloading the file from the cloud. Note: For this to work, <code>cloudpath</code> needs to be in control of the reading/writing of files. This means your code base should use the <code>CloudPath.write_*</code>, <code>CloudPath.read_*</code>, and <code>CloudPath.open</code> methods. Using <code>CloudPath.fspath</code> (or passing the <code>CloudPath</code> as a <code>PathLike</code> object to another library) will not clear the cache on file close since it was not opened by <code>cloudpathlib</code>.</li> </ul> <p>Note: Although we use it in the examples below, for <code>\"cloudpath_object\"</code> and <code>\"tmp_dir\"</code> you normally shouldn't need to explicitly call <code>del</code>. Letting Python garbage collection run on its own once all references to the object leave scope should be sufficient. See details in the Python docs).</p>"},{"location":"caching/#setting-the-cache-clearing-method","title":"Setting the cache clearing method\u00b6","text":"<p>You can set the cache clearing method either through the environment variable <code>CLOUDPATHLIB_FILE_CACHE_MODE</code> or by passing the mode to the <code>*Client</code> when you instantiate it. See below for an example.</p> <p>You can set <code>CLOUDPATHLIB_FILE_CACHE_MODE</code> to any of the supported values, which are printed below.</p>"},{"location":"caching/#file-cache-mode-close_file","title":"File cache mode: <code>\"close_file\"</code>\u00b6","text":"<p>Example instantiation by passing a string to the client.</p> <p>Local cache file is gone as soon as file is closed for reading.</p>"},{"location":"caching/#file-cache-mode-cloudpath_object","title":"File cache mode: <code>\"cloudpath_object\"</code>\u00b6","text":"<p>Example instantiation by passing enum member to the client.</p> <p>Local cache file exists after file is closed for reading.</p> <p>Local cache file is gone after <code>CloudPath</code> is no longer referenced (done explicitly with <code>del</code> here, but is usually called automatically by the garbage collector. See details in the Python docs).</p>"},{"location":"caching/#file-cache-mode-tmp_dir-default","title":"File cache mode: <code>\"tmp_dir\"</code> (default)\u00b6","text":"<p>Local cache file exists after file is closed for reading.</p> <p>Local cache file exists after <code>CloudPath</code> is no longer referenced.</p> <p>Local cache file is gone after the <code>Client</code> object is no longer referenced (done explicitly with <code>del</code> here, but is usually called automatically by the garbage collector. See details in the Python docs).</p>"},{"location":"caching/#file-cache-mode-persistent","title":"File cache mode: <code>\"persistent\"</code>\u00b6","text":"<p>If <code>local_cache_dir</code> is specificed, but <code>file_cache_mode</code> is not, then the mode is set to <code>\"persistent\"</code> automatically. Conversely, if you set the mode to <code>\"persistent\"</code> explicitly, you must also pass <code>local_cache_dir</code> or the <code>Client</code> will raise <code>InvalidConfigurationException</code>.</p> <p>Local cache file exists after file is closed for reading.</p> <p>Local cache file exists after <code>CloudPath</code> is no longer referenced.</p> <p>Local cache exists after the <code>Client</code> object is no longer referenced.</p>"},{"location":"caching/#caveats","title":"Caveats\u00b6","text":"<ul> <li><p>Automatic cache clearing works in most contexts, but there can be cases where execution of a program is halted before <code>cloudpathlib</code>'s cache clearing code is able to run. It is a good practice to monitor your cache folders and, if using temporary directories, trigger your operating system's temp directory clean up (which is OS-dependent, but restarting is usually sufficient).</p> </li> <li><p>Using <code>with CloudPath.open()</code> as a context manager to open files for read or write is the best way to ensure that automatic cache clearing happens consistently. The <code>\"close_file\"</code> cache clearing mode will not work as expected if you use another method to open files (e.g., calling the Python built-in <code>open</code>, using <code>CloudPath.fspath</code>, or where another library handles the opening/closing of the file).</p> </li> <li><p>The <code>download_to</code> and <code>upload_from</code> methods do not cache the file, since we assume if you are downloading or uploading you explicitly want the file to be in a particular location or know where it is already.</p> </li> </ul>"},{"location":"changelog/","title":"cloudpathlib Changelog","text":""},{"location":"changelog/#unreleased","title":"UNRELEASED","text":"<ul> <li>Allow <code>CloudPath</code> objects to be loaded/dumped through pickle format repeatedly. (Issue #450)</li> <li>Fixes typo in <code>FileCacheMode</code> where values were being filled by envvar <code>CLOUPATHLIB_FILE_CACHE_MODE</code> instead of <code>CLOUDPATHLIB_FILE_CACHE_MODE</code>. (PR #424</li> <li>Fix <code>CloudPath</code> cleanup via <code>CloudPath.__del__</code> when <code>Client</code> encounters an exception during initialization and does not create a <code>file_cache_mode</code> attribute. (Issue #372, thanks to @bryanwweber)</li> <li>Drop support for Python 3.7; pin minimal <code>boto3</code> version to Python 3.8+ versions. (PR #407)</li> <li>fix: use native <code>exists()</code> method in <code>GSClient</code>. (PR #420)</li> <li>Enhancement: lazy instantiation of default client (PR #432, Issue #428)</li> <li>Adds existence check before downloading in <code>download_to</code> (Issue #430, PR #432)</li> <li>Add env vars <code>CLOUDPATHLIB_FORCE_OVERWRITE_FROM_CLOUD</code> and <code>CLOUDPATHLIB_FORCE_OVERWRITE_TO_CLOUD</code>. (Issue #393, PR #437)</li> <li>Fixed <code>glob</code> for <code>cloudpathlib.local.LocalPath</code> and subclass implementations to match behavior of cloud versions for parity in testing. (Issue #415, PR #436)</li> <li>Changed how <code>cloudpathlib.local.LocalClient</code> and subclass implementations track the default local storage directory (used to simulate the cloud) used when no local storage directory is explicitly provided. (PR #436, PR #462)<ul> <li>Changed <code>LocalClient</code> so that client instances using the default storage access the default local storage directory through the <code>get_default_storage_dir</code> rather than having an explicit reference to the path set at instantiation. This means that calling <code>get_default_storage_dir</code> will reset the local storage for all clients using the default local storage, whether the client has already been instantiated or is instantiated after resetting. This fixes unintuitive behavior where <code>reset_local_storage</code> did not reset local storage when using the default client. (Issue #414)</li> <li>Added a new <code>local_storage_dir</code> property to <code>LocalClient</code>. This will return the current local storage directory used by that client instance. by reference through the `get_default_ rather than with an explicit.</li> </ul> </li> <li>Added Azure Data Lake Storage Gen2 support (Issue #161, PR #450), thanks to @M0dEx for PR #447 and PR #449</li> </ul>"},{"location":"changelog/#v0181-2024-02-26","title":"v0.18.1 (2024-02-26)","text":"<ul> <li>Fixed import error due to incompatible <code>google-cloud-storage</code> by not using <code>transfer_manager</code> if it is not available. (Issue #408, PR #410)</li> </ul> <p>Includes all changes from v0.18.0.</p> <p>Note: This is the last planned Python 3.7 compatible release version.</p>"},{"location":"changelog/#0180-2024-02-25-yanked","title":"0.18.0 (2024-02-25) (Yanked)","text":"<ul> <li>Implement sliced downloads in GSClient. (Issue #387, PR #389)</li> <li>Implement <code>as_url</code> with presigned parameter for all backends. (Issue #235, PR #236)</li> <li>Stream to and from Azure Blob Storage. (PR #403)</li> <li>Implement <code>file:</code> URI scheme support for <code>AnyPath</code>. (Issue #401, PR #404)</li> </ul> <p>Note: This version was yanked due to incompatibility with google-cloud-storage &lt;2.7.0 that causes an import error.</p>"},{"location":"changelog/#0170-2023-12-21","title":"0.17.0 (2023-12-21)","text":"<ul> <li>Fix <code>S3Client</code> cleanup via <code>Client.__del__</code> when <code>S3Client</code> encounters an exception during initialization. (Issue #372, PR #373, thanks to @bryanwweber)</li> <li>Skip mtime checks during upload when force_overwrite_to_cloud is set to improve upload performance. (Issue #379, PR #380, thanks to @Gilthans)</li> </ul>"},{"location":"changelog/#v0160-2023-10-09","title":"v0.16.0 (2023-10-09)","text":"<ul> <li>Add \"CloudPath\" as return type on <code>__init__</code> for mypy issues. (Issue #179, PR #342)</li> <li>Add <code>with_stem</code> to all path types when python version supports it (&gt;=3.9). (Issue #287, PR #290, thanks to @Gilthans)</li> <li>Add <code>newline</code> parameter to the <code>write_text</code> method to align to <code>pathlib</code> functionality as of Python 3.10. PR #362, thanks to @pricemg.</li> <li>Add support for Python 3.12 (PR #364)</li> <li>Add <code>CLOUDPATHLIB_LOCAL_CACHE_DIR</code> env var for setting local_cache_dir default for clients (Issue #352, PR #357)</li> <li>Add <code>CONTRIBUTING.md</code> instructions for contributors (Issue #213, PR #367)</li> </ul>"},{"location":"changelog/#v0151-2023-07-12","title":"v0.15.1 (2023-07-12)","text":"<ul> <li>Compatibility with pydantic &gt;= 2.0.0. (PR #349)</li> </ul>"},{"location":"changelog/#v0150-2023-06-16","title":"v0.15.0 (2023-06-16)","text":"<ul> <li>Changed return type for <code>CloudPathMeta.__call__</code> to fix problems with pyright/pylance (PR #330)</li> <li>Make <code>CloudPath.is_valid_cloudpath</code> a TypeGuard so that type checkers can know the subclass if <code>is_valid_cloudpath</code> is called (PR #337)</li> <li>Added <code>follow_symlinks</code> to <code>stat</code> for 3.11.4 compatibility (see bpo 39906)</li> <li>Add <code>follow_symlinks</code> to <code>is_dir</code> implementation for CPython <code>glob</code> compatibility (see CPython PR #104512)</li> </ul>"},{"location":"changelog/#v0140-2023-05-13","title":"v0.14.0 (2023-05-13)","text":"<ul> <li>Changed to pyproject.toml-based build.</li> <li>Changed type hints from custom type variable <code>DerivedCloudPath</code> to <code>typing.Self</code> (PEP 673). This adds a dependency on the typing-extensions backport package from Python versions lower than 3.11.</li> <li>Fixed a runtime key error when an S3 object does not have the <code>Content-Type</code> metadata set. (Issue #331, PR #332)</li> </ul>"},{"location":"changelog/#v0130-2023-02-15","title":"v0.13.0 (2023-02-15)","text":"<ul> <li>Implement <code>file_cache_mode</code>s to give users finer-grained control over when and how the cache is cleared. (Issue #10, PR #314)</li> <li>Speed up listing directories for Google Cloud Storage. (PR #318)</li> <li>Add compatibility for Python 3.11 (PR #317)</li> </ul>"},{"location":"changelog/#v0121-2023-01-04","title":"v0.12.1 (2023-01-04)","text":"<ul> <li>Fix glob logic for buckets; add regression test; add error on globbing all buckets (Issue #311, PR #312)</li> </ul>"},{"location":"changelog/#v0120-2022-12-30","title":"v0.12.0 (2022-12-30)","text":"<ul> <li>API Change: <code>S3Client</code> supports an <code>extra_args</code> kwarg now to pass extra args down to <code>boto3</code> functions; this enables Requester Pays bucket access and bucket encryption. (Issues #254, #180; PR #307)</li> <li>Speed up glob! (Issue #274, PR #304)</li> <li>Ability to list buckets/containers a user has access to. (Issue #48, PR #307)</li> <li>Remove overly specific status check and assert in production code on remove. (Issue #212, PR #307)</li> <li>Update docs, including accessing public buckets. (Issue #271, PR #307)</li> </ul>"},{"location":"changelog/#v0110-2022-12-18","title":"v0.11.0 (2022-12-18)","text":"<ul> <li>API change: Add <code>ignore</code> parameter to <code>CloudPath.copytree</code> in order to match <code>shutil</code> API. (Issue #145, PR #272)</li> <li>Use the V2 version for listing objects <code>list_objects_v2</code> in <code>S3Client</code>. (Issue #155, PR #302)</li> <li>Add abilty to use <code>.exists</code> to check for a raw bucket/container (no additional path components). (Issue #291, PR #302)</li> <li>Prevent data loss when renaming by skipping files that would be renamed to the same thing. (Issue #277, PR #278)</li> <li>Speed up common <code>glob</code>/<code>rglob</code> patterns. (Issue #274, PR #276)</li> </ul>"},{"location":"changelog/#v0100-2022-08-18","title":"v0.10.0 (2022-08-18)","text":"<ul> <li>API change: Make <code>stat</code> on base class method instead of property to follow <code>pathlib</code> (Issue #234, PR #250)</li> <li>Fixed \"S3Path.exists() returns True on partial matches.\" (Issue #208, PR #244)</li> <li>Make <code>AnyPath</code> subclass of <code>AnyPath</code> (Issue #246, PR #251)</li> <li>Skip docstrings if not present to avoid failing under <code>-00</code> (Issue #238, PR #249)</li> <li>Add <code>py.typed</code> file so mypy runs (Issue #243, PR #248)</li> </ul>"},{"location":"changelog/#v090-2022-06-03","title":"v0.9.0 (2022-06-03)","text":"<ul> <li>Added <code>absolute</code> to <code>CloudPath</code> (does nothing as <code>CloudPath</code> is always absolute) (PR #230)</li> <li>Added <code>resolve</code> to <code>CloudPath</code> (does nothing as <code>CloudPath</code> is resolved in advance) (Issue #151, PR #230)</li> <li>Added <code>relative_to</code> to <code>CloudPath</code> which returns a <code>PurePosixPath</code> (Issue #149, PR #230)</li> <li>Added <code>is_relative_to</code> to <code>CloudPath</code> (Issue #149, PR #230)</li> <li>Added <code>is_absolute</code> to <code>CloudPath</code> (always true as <code>CloudPath</code> is always absolute) (PR #230)</li> <li>Accept and delegate <code>read_text</code> parameters to cached file (PR #230)</li> <li>Added <code>exist_ok</code> parameter to <code>touch</code> (PR #230)</li> <li>Added <code>missing_ok</code> parameter to <code>unlink</code>, which defaults to True. This diverges from pathlib to maintain backward compatibility (PR #230)</li> <li>Fixed missing root object entries in documentation's Intersphinx inventory (Issue #211, PR #237)</li> </ul>"},{"location":"changelog/#v080-2022-05-19","title":"v0.8.0 (2022-05-19)","text":"<ul> <li>Fixed pickling of <code>CloudPath</code> objects not working. (Issue #223, PR #224)</li> <li>Added functionality to [push the MIME (media) type to the content type property on cloud providers by default. (Issue #222, PR #226)</li> </ul>"},{"location":"changelog/#v071-2022-04-06","title":"v0.7.1 (2022-04-06)","text":"<ul> <li>Fixed inadvertent inclusion of tests module in package. (Issue #173, PR #219)</li> </ul>"},{"location":"changelog/#v070-2022-02-16","title":"v0.7.0 (2022-02-16)","text":"<ul> <li>Fixed <code>glob</code> and <code>rglob</code> functions by using pathlib's globbing logic rather than fnmatch. (Issue #154)</li> <li>Fixed <code>iterdir</code> to not include self. (Issue #15)</li> <li>Fixed error when calling <code>suffix</code> and <code>suffixes</code> on a cloud path with no suffix. (Issue #120)</li> <li>Changed <code>parents</code> return type from list to tuple, to better match pathlib's tuple-like <code>_PathParents</code> return type.</li> <li>Remove support for Python 3.6. Issue #186</li> </ul>"},{"location":"changelog/#v065-2022-01-25","title":"v0.6.5 (2022-01-25)","text":"<ul> <li>Fixed error when \"directories\" created on AWS S3 were reported as files. (Issue #148, PR #190)</li> <li>Fixed bug where GCE machines can instantiate default client, but we don't attempt it. (Issue #191</li> <li>Support <code>AWS_ENDPOINT_URL</code> environment variable to set the <code>endpoint_url</code> for <code>S3Client</code>. (PR #193)</li> </ul>"},{"location":"changelog/#v064-2021-12-29","title":"v0.6.4 (2021-12-29)","text":"<ul> <li>Fixed error where <code>BlobProperties</code> type hint causes import error if Azure dependencies not installed.</li> </ul>"},{"location":"changelog/#v063-2021-12-29","title":"v0.6.3 (2021-12-29)","text":"<ul> <li>Fixed error when using <code>rmtree</code> on nested directories for Google Cloud Storage and Azure Blob Storage. (Issue #184, PR #185)</li> <li>Fixed broken builds due mypy errors in azure dependency (PR #177)</li> <li>Fixed dev tools for building and serving documentation locally (PR #178)</li> </ul>"},{"location":"changelog/#v062-2021-09-20","title":"v0.6.2 (2021-09-20)","text":"<ul> <li>Fixed error when importing <code>cloudpathlib</code> for missing <code>botocore</code> dependency when not installed with S3 dependencies. (PR #168)</li> </ul>"},{"location":"changelog/#v061-2021-09-17","title":"v0.6.1 (2021-09-17)","text":"<ul> <li>Fixed absolute documentation URLs to point to the new versioned documentation pages.</li> <li>Fixed bug where <code>no_sign_request</code> couldn't be used to download files since our code required list permissions to the bucket to do so. (Issue #169, PR #168).</li> </ul>"},{"location":"changelog/#v060-2021-09-07","title":"v0.6.0 (2021-09-07)","text":"<ul> <li>Added <code>no_sign_request</code> parameter to <code>S3Client</code> instantiation for anonymous requests for public resources on S3. See documentation for more details. (#164)</li> </ul>"},{"location":"changelog/#v050-2021-08-31","title":"v0.5.0 (2021-08-31)","text":"<ul> <li>Added <code>boto3_transfer_config</code> parameter to <code>S3Client</code> instantiation, which allows passing a <code>boto3.s3.transfer.TransferConfig</code> object and is useful for controlling multipart and thread use in uploads and downloads. See documentation for more details. (#150)</li> </ul>"},{"location":"changelog/#v041-2021-05-29","title":"v0.4.1 (2021-05-29)","text":"<ul> <li>Added support for custom S3-compatible object stores. This functionality is available via the <code>endpoint_url</code> keyword argument when instantiating an <code>S3Client</code> instance. See documentation for more details. (#138 thanks to @YevheniiSemendiak)</li> <li>Added <code>CloudPath.upload_from</code> which uploads the passed path to this CloudPath (issuse #58)</li> <li>Added support for common file transfer functions based on <code>shutil</code>. Issue #108. PR #142.</li> <li><code>CloudPath.copy</code> copy a file from one location to another. Can be cloud -&gt; local or cloud -&gt; cloud. If <code>client</code> is not the same, the file transits through the local machine.</li> <li><code>CloudPath.copytree</code> reucrsively copy a directory from one location to another. Can be cloud -&gt; local or cloud -&gt; cloud. Uses <code>CloudPath.copy</code> so if <code>client</code> is not the same, the file transits through the local machine.</li> </ul>"},{"location":"changelog/#v040-2021-03-13","title":"v0.4.0 (2021-03-13)","text":"<ul> <li>Added rich comparison operator support to cloud paths, which means you can now use them with <code>sorted</code>. (#129)</li> <li>Added polymorphic class <code>AnyPath</code> which creates a cloud path or <code>pathlib.Path</code> instance appropriately for an input filepath. See new documentation for details and example usage. (#130)</li> <li>Added integration with Pydantic. See new documentation for details and example usage. (#130)</li> <li>Exceptions: (#131)<ul> <li>Changed all custom <code>cloudpathlib</code> exceptions to be located in new <code>cloudpathlib.exceptions</code> module.</li> <li>Changed all custom <code>cloudpathlib</code> exceptions to subclass from new base <code>CloudPathException</code>. This allows for easy catching of any custom exception from <code>cloudpathlib</code>.</li> <li>Changed all custom exceptions names to end with <code>Error</code> as recommended by PEP 8.</li> <li>Changed various functions to throw new <code>CloudPathFileExistsError</code>, <code>CloudPathIsADirectoryError</code> or <code>CloudPathNotADirectoryError</code> exceptions instead of a generic <code>ValueError</code>.</li> <li>Removed exception exports from the root <code>cloudpathlib</code> package namespace. Import from <code>cloudpathlib.exceptions</code> instead if needed.</li> </ul> </li> <li>Fixed <code>download_to</code> method to handle case when source is a file and destination is a directory. (#121 thanks to @genziano)</li> <li>Fixed bug where <code>hash(...)</code> of a cloud path was not consistent with the equality operator. (#129)</li> <li>Fixed <code>AzureBlobClient</code> instantiation to throw new error <code>MissingCredentialsError</code> when no credentials are provided, instead of <code>AttributeError</code>. <code>LocalAzureBlobClient</code> has also been changed to accordingly error under those conditions. (#131)</li> <li>Fixed <code>GSClient</code> to instantiate as anonymous with public access only when instantiated with no credentials, instead of erroring. (#131)</li> </ul>"},{"location":"changelog/#v030-2021-01-29","title":"v0.3.0 (2021-01-29)","text":"<ul> <li>Added a new module <code>cloudpathlib.local</code> with utilities for mocking cloud paths in tests. The module has \"Local\" substitute classes that use the local filesystem in place of cloud storage. See the new documentation article \"Testing code that uses cloudpathlib\" to learn more about how to use them. (#107)</li> </ul>"},{"location":"changelog/#v021-2021-01-25","title":"v0.2.1 (2021-01-25)","text":"<ul> <li>Fixed bug where a <code>NameError</code> was raised if the Google Cloud Storage dependencies were not installed (even if using a different storage provider).</li> </ul>"},{"location":"changelog/#v020-2021-01-23","title":"v0.2.0 (2021-01-23)","text":"<ul> <li>Added support for Google Cloud Storage. Instantiate with URIs prefixed by <code>gs://</code> or explicitly using the <code>GSPath</code> class. (#113 thanks to @wolfgangwazzlestrauss)</li> <li>Changed backend logic to reduce number of network calls to cloud. This should result in faster cloud path operations, especially when dealing with many small files. (#110, #111)</li> </ul>"},{"location":"changelog/#v012-2020-11-14","title":"v0.1.2 (2020-11-14)","text":"<ul> <li>Fixed <code>CloudPath</code> instantiation so that reinstantiating with an existing <code>CloudPath</code> instance will reuse the same client, if a new client is not explicitly passed. This addresses the edge case of non-idempotency when reinstantiating a <code>CloudPath</code> instance with a non-default client. (#104)</li> </ul>"},{"location":"changelog/#v011-2020-10-15","title":"v0.1.1 (2020-10-15)","text":"<ul> <li>Fixed a character-encoding bug when building from source on Windows. (#98)</li> </ul>"},{"location":"changelog/#v010-2020-10-06","title":"v0.1.0 (2020-10-06)","text":"<ul> <li>Initial release of cloudpathlib with support for Amazon S3 and Azure Blob Storage! \ud83c\udf89</li> </ul>"},{"location":"contributing/","title":"<code>cloudpathlib</code> Contribution Guidelines","text":"<p>Thanks for offering to help on <code>cloudpathlib</code>! We welcome contributions. This document will help you get started finding issues, developing fixes, and submitting PRs.</p> <p>First, a few guidelines:</p> <ul> <li>Follow the code of conduct.</li> <li>PRs from outside contributors will not be accepted without an issue. We respect your time and want to make sure any work you do will be reviewed, so please wait for a maintainer to sign off on the issue before getting started. </li> <li>If you are looking just to make a contribution, look at issues with label \"good first issue\".</li> </ul>"},{"location":"contributing/#how-to-contribute","title":"How to contribute","text":"<ol> <li>As noted above, please file an issue if you are not fixing an existing issue.</li> <li>Fork the repo, clone it locally, and create a local environment.</li> <li>Make changes in your local version of the repository.</li> <li>Make sure that the tests pass locally.</li> <li>Update the package documentation, if applicable.</li> <li>Go through the items in the final PR checklist.</li> <li>Submit a PR</li> </ol> <p>For some guidance on working with the code, see the sections on code standards and code architecture.</p>"},{"location":"contributing/#local-development","title":"Local development","text":"<p>Create a Python environment to work in. If you're working on Windows, a bash shell will make your life easier. We recommend git bash, cygwin, or WSL so that you can use the <code>make</code> commands, but it is totally optional.</p> <p>You can see all the available developer commands by running <code>make</code>:</p> <pre><code>\u276f make\nclean                remove all build, test, coverage and Python artifacts\nclean-build          remove build artifacts\nclean-pyc            remove Python file artifacts\nclean-test           remove test and coverage artifacts\ndist                 builds source and wheel package\ndocs-setup           setup docs pages based on README.md and HISTORY.md\ndocs                 build the static version of the docs\ndocs-serve           serve documentation to livereload while you work\nformat               run black to format codebase\ninstall              install the package to the active Python's site-packages\nlint                 check style with black, flake8, and mypy\nrelease              package and upload a release\nreqs                 install development requirements\ntest                 run tests with mocked cloud SDKs\ntest-debug           rerun tests that failed in last run and stop with pdb at failures\ntest-live-cloud      run tests on live cloud backends\nperf                 run performance measurement suite for s3 and save results to perf-results.csv\n</code></pre> <p>Once you have your Python environment, you can install all the dev dependencies with:</p> <pre><code>make reqs\n</code></pre> <p>This will also install an editable version of <code>cloudpathlib</code> with all the extras into your environment.</p>"},{"location":"contributing/#tests","title":"Tests","text":""},{"location":"contributing/#commands","title":"Commands","text":"<p>There is a robust test suite that covers most of the core functionality of the library. There are a few different testing commands that are useful to developers.</p> <p>The most common way when developing is to run the full test suite with mocked, local backends (no network calls):</p> <pre><code>make test\n</code></pre> <p>If you have a test fail or want to be able to interactively debug if a test fails, you can use a different command. This will run pytest with <code>pdb</code>, and <code>last-fail</code> so it will drop you into a debugger if a test fails and only run the tests that failed last time:</p> <pre><code>make test-debug\n</code></pre> <p>Finally, you may want to run your tests against live servers to ensure that the behavior against a provider's server is not different from the mocked provider. You will need credentials configured for each of the providers you run against. You can run the live tests with:</p> <pre><code>make test-live-cloud\n</code></pre>"},{"location":"contributing/#azure-live-backend-tests","title":"Azure live backend tests","text":"<p>For Azure, you can test both against Azure Blob Storage backends and Azure Data Lake Storage Gen2 backends. To run these tests, you need to set connection strings for both of the backends by setting the following environment variables (in your <code>.env</code> file for local development). If <code>AZURE_STORAGE_GEN2_CONNECTION_STRING</code> is not set, only the blob storage backend will be tested. To set up a storage account with ADLS Gen2, go through the normal creation flow for a storage account in the Azure portal and select \"Enable Hierarchical Namespace\" in the \"Advanced\" tab of the settings when configuring the account.</p> <pre><code>AZURE_STORAGE_CONNECTION_STRING=your_connection_string\nAZURE_STORAGE_GEN2_CONNECTION_STRING=your_connection_string\n</code></pre> <p>You can copy <code>.env.example</code> to <code>.env</code> and fill in the credentials and bucket/container names for the providers you want to test against.  Note that the live tests will create and delete files on the cloud provider.</p> <p>You can also skip providers you do not have accounts for by commenting them out in the <code>rig</code> and <code>s3_like_rig</code> variables defined at the end of <code>tests/conftest.py</code>. </p>"},{"location":"contributing/#test-rigs","title":"Test rigs","text":"<p>Since we want behavior parity across providers, nearly all of the tests are written in a provider-agnositc way. Each test is passed a test rig as a fixture, and the rig provides the correct way for generating cloudpaths for testing. The test rigs are defined in <code>conftest.py</code>.</p> <p>Almost none of the tests instantiate <code>CloudPath</code> or a <code>*Client</code> class directly.</p> <p>When a test suite runs against the rig, the rig does the following steps on setup:  - Creates a folder on the provider with a unique ID just for this test run.  - Copies the contents of <code>tests/assets/</code> into that folder.  - Sets the rigs <code>client_class</code> using <code>set_as_default_client</code> so that any <code>CloudPath</code> with the right prefix created in the test will use the rig's client.</p> <p>When the tests finish, if it is using a live server, the test files will be deleted from the provider.</p> <p>If you want to speed up your testing during development, you may comment out some of the rigs in <code>conftest.py</code>. Don't commit this change, and make sure you run against all the rigs before submitting a PR.</p>"},{"location":"contributing/#authoring-tests","title":"Authoring tests","text":"<p>We want our test suite coverage to be comprehensive, so PRs need to add tests if they add new functionality. If you are adding a new feature, you will need to add tests for it. If you are changing an existing feature, you will need to update the tests to match the new behavior.</p> <p>The tests are written in <code>pytest</code>. You can read the pytest documentation for more information on how to write tests with pytest.</p> <p>Your best guide will be reading the existing test suite, and making sure you're using the rig when possible.</p> <p>For example, if you want a <code>CloudPath</code> referring to a file that actually exists, you may want to do something like this:</p> <pre><code># note that dir_0/file0_0.txt is in tests/assets, so you can expect to exist on the provider\ncp = rig.create_cloud_path(\"dir_0/file0_0.txt\")\n\n# if you don't need the file to actually exist, you can use the same method\ncp2 = rig.create_cloud_path(\"path/that/does/not/exist.txt\")\n</code></pre> <p>If you are testing functionality on the <code>*Client</code> class, you can get an instance of the class for the rig with:</p> <pre><code>new_client = rig.client_class()\n</code></pre>"},{"location":"contributing/#documentation","title":"Documentation","text":"<p>We also aim to have robust and comprehensive documentation. For public API functions, we provide docstrings that explain how things work to end users, and these are automatically built into the documentation. For more complex topics, we write specific documentation.</p> <p>We use mkdocs to generate the documentation.</p>"},{"location":"contributing/#building","title":"Building","text":"<p>To build the latest version of the documentation, you can run:</p> <pre><code>make docs\n</code></pre> <p>If you add/remove a method on the <code>CloudPath</code> class or any subclass, run <code>python docs/make_support_table.py</code> and paste the updated table into README.md.</p>"},{"location":"contributing/#serving","title":"Serving","text":"<p>While you are developing, you can serve a local version of the docs to see what your changes look like. This will auto-reload for most changes to the docs:</p> <pre><code>make docs-serve\n</code></pre> <p>Note that the main page (<code>index.md</code>), the changelog (<code>HISTORY.md</code>), and the contributing page (<code>CONTRIBUTING.md</code>) are all generated from the files in the project root. If you want to make changes to the documentation, you should make them in the root of the project and then run <code>make docs-setup</code> to update the other files. The dev server does not automatically pick up these changes. You will need to stop the server and restart it to see the changes.</p>"},{"location":"contributing/#authoring","title":"Authoring","text":"<p>Documentation pages can either be authored in normal Markdown or in a runnable jupyter notebook. Notebooks are useful for showing examples of how to use the library. You can see an example of a notebook in <code>docs/docs/why_cloudpathlib.ipynb</code>.</p> <p>Note: generating the documentation does not execute any notebooks, it just converts them. You need to restart and run all notebook cells to make sure the notebook executes top-to-bottom and has the latest results before committing it.</p>"},{"location":"contributing/#docstrings","title":"Docstrings","text":"<p>For public APIs, please add a docstring that will appear in the documentation automatically. Since public APIs are type-hinted, there is no need to list the function parameters, their types, and the return types in a specific format in the docstring. Instead, you should describe what the function does and any relevant information for the user.</p>"},{"location":"contributing/#submitting-a-pr","title":"Submitting a PR","text":"<p>Once you have everything working as expected locally, submit a PR. The PR will be automatically tested by GitHub Actions. If the tests fail, you will need to fix the issue and push a new commit to the PR. If the tests pass (except the live tests, as noted below), you will need to get a maintainer to review the PR and merge it.</p>"},{"location":"contributing/#pr-checklist","title":"PR checklist","text":"<p>Here's a checklist from the PR template to make sure that you did all the required steps:</p> <pre><code> - [ ] I have read and understood `CONTRIBUTING.md`\n - [ ] Confirmed an issue exists for the PR, and the text `Closes #issue` appears in the PR summary (e.g., `Closes #123`).\n - [ ] Confirmed PR is rebased onto the latest base\n - [ ] Confirmed failure before change and success after change\n - [ ] Any generic new functionality is replicated across cloud providers if necessary\n - [ ] Tested manually against live server backend for at least one provider\n - [ ] Added tests for any new functionality\n - [ ] Linting passes locally\n - [ ] Tests pass locally\n - [ ] Updated `HISTORY.md` with the issue that is addressed and the PR you are submitting. If the top section is not `## UNRELEASED``, then you need to add a new section to the top of the document for your change.\n</code></pre>"},{"location":"contributing/#pr-cicd-test-run","title":"PR CI/CD test run","text":"<p>If you are not a maintainer, a maintainer will have to approve your PR to run the test suite in GitHub Actions. No need to ping a maintainer, it will be seen as part of our regular review.</p> <p>Even once the tests run, two jobs will fail. This is expected. The failures are: (1) The live tests, and (2) the install tests. Both of these require access to the live backends, which are not available to outside contributors. If everything else passes, you can ignore these failiures. A mainter will take the following steps:</p> <ul> <li>Create a branch off the main repo for your PR's changes</li> <li>Merge your PR into that new branch</li> <li>Run CI/CD on the repo-local branch which has access to the live backends</li> <li>Confirm the live tests pass as expected. (If not, you will need to fix the issue and create another PR into this reopo-local branch.)</li> <li>Once they pass, merge the repo-local branch into the main branch.</li> </ul> <p>For example, see a repo-local branch running the live tests in this PR.</p>"},{"location":"contributing/#code-standards-and-tips","title":"Code standards and tips","text":""},{"location":"contributing/#adding-dependencies","title":"Adding dependencies","text":"<p>We want <code>cloudpathlib</code> to be as lightweight as possible. Our strong preference is to not take any external dependencies for the library outside of the official software development kit (SDK) for the cloud provider. If you want to add a dependency, please open an issue to discuss it first. Library depencies are tracked in <code>pyproject.toml</code>.</p> <p>Dependencies that are only needed for building documentation, development, linting, formatting, or testing can be added to <code>requirements-dev.txt</code>, and are not subject to the same scrutiny.</p>"},{"location":"contributing/#linting-and-formatting","title":"Linting and formatting","text":"<p>Any code changes need to follow the code standards of the project. We use <code>black</code> for formatting code (as configured in <code>pyproject.toml</code>), and <code>flake8</code> for linting (as configured in <code>setup.cfg</code>).</p> <p>To apply these styles to your code, you can run:</p> <pre><code>make format\n</code></pre> <p>To ensure that your code is properly formatted and linted and passes type checking, you can run:</p> <pre><code>make lint\n</code></pre>"},{"location":"contributing/#type-hinting","title":"Type hinting","text":"<p>Any public APIs need to have proper type annotations, which are checked by <code>mypy</code> (as configured in <code>pyproject.toml</code>) when you run the <code>make lint</code> command. These need to pass. If you are adding a new public API, you will need to add type annotations to it. If you are changing an existing public API, you will need to update the type annotations to match the new API. If you are adding a private API, you do not need to add type annotations, but you should consider it. Only use <code># type: ignore</code> or <code>Any</code> if there is not other way.</p> <p>As mentioned, to ensure your code passes <code>mypy</code> type checking, you can run:</p> <pre><code>make lint\n</code></pre>"},{"location":"contributing/#interactive-testing","title":"Interactive testing","text":"<p>To interactively test the library, we recommend creating a Jupyter notebook in the root of the project called <code>sandbox.ipynb</code>. We <code>.gitignore</code> a <code>sandbox.ipynb</code> file by default for this purpose. You can import the library and run commands in the notebook to test functionality. This is useful for testing new features or debugging issues.</p> <p>It's best to start the notebook with cells to autoreload the library:</p> <pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>Then you can import the library and work with the CloudPath class:</p> <pre><code>from cloudpathlib import CloudPath\n\ncp = CloudPath(\"s3://my-test-bucket/\")\n</code></pre>"},{"location":"contributing/#credentials-and-cloud-access","title":"Credentials and cloud access","text":"<p>For certain tests and development scenarios, you will need to have access to the relevant cloud provider. You can put the authentication credentials into a <code>.env</code> file in the root of the project. The <code>.env</code> file is ignored by git, so you can put your credentials there without worrying about them being committed. See the authentication documentation for information on what variables to set.</p> <p>If you need to test against a cloud provider you do not have access to, you can reach out to the maintainers, who may be willing to grant credentials for testing purposes.</p>"},{"location":"contributing/#mocking-providers-in-tests","title":"Mocking providers in tests","text":"<p>All of the cloud providers have mocked versions of their SDKs that are used for running local tests. These are in <code>tests/mock_clients</code>. If you are adding a feature that makes a call to the underlying client SDK, and it is not already mocked, you will need to mock it.</p> <p>In general, these mocks actually store files in a temporary directory on the file system and then use those files to behave like the real versions of the library do. This way we can run full test suites without making any network calls.</p> <p>The mocks are set up in each rig in <code>conftest.py</code> as long as <code>USE_LIVE_CLOUD</code> is not set to <code>\"1\"</code>.</p>"},{"location":"contributing/#performance-testing","title":"Performance testing","text":"<p>Listing files with <code>client._list_dir</code>, <code>CloudPath.glob</code>, <code>CloudPath.rglob</code>, and <code>CloudPath.walk</code> are all performance-sensitive operations for large directories on cloud storage. If you change code related to any of these methods, make sure to run the performance tests. In your PR description, include the results on your machine prior to making your changes and the results with your changes.</p> <p>These can be run with <code>make perf</code>. This will generate a report like:</p> <pre><code>                                  Performance suite results: (2023-10-08T13:18:04.774823)                                  \n\u250f\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2533\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2513\n\u2503 Test Name      \u2503 Config Name                \u2503 Iterations \u2503           Mean \u2503              Std \u2503            Max \u2503 N Items \u2503\n\u2521\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2547\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2529\n\u2502 List Folders   \u2502 List shallow recursive     \u2502         10 \u2502 0:00:00.862476 \u2502 \u00b1 0:00:00.020222 \u2502 0:00:00.898143 \u2502   5,500 \u2502\n\u2502 List Folders   \u2502 List shallow non-recursive \u2502         10 \u2502 0:00:00.884997 \u2502 \u00b1 0:00:00.086678 \u2502 0:00:01.117775 \u2502   5,500 \u2502\n\u2502 List Folders   \u2502 List normal recursive      \u2502         10 \u2502 0:00:01.248844 \u2502 \u00b1 0:00:00.095575 \u2502 0:00:01.506868 \u2502   7,877 \u2502\n\u2502 List Folders   \u2502 List normal non-recursive  \u2502         10 \u2502 0:00:00.060042 \u2502 \u00b1 0:00:00.003986 \u2502 0:00:00.064052 \u2502     113 \u2502\n\u2502 List Folders   \u2502 List deep recursive        \u2502         10 \u2502 0:00:02.004731 \u2502 \u00b1 0:00:00.130264 \u2502 0:00:02.353263 \u2502   7,955 \u2502\n\u2502 List Folders   \u2502 List deep non-recursive    \u2502         10 \u2502 0:00:00.054268 \u2502 \u00b1 0:00:00.003314 \u2502 0:00:00.062116 \u2502      31 \u2502\n\u2502 Glob scenarios \u2502 Glob shallow recursive     \u2502         10 \u2502 0:00:01.056946 \u2502 \u00b1 0:00:00.160470 \u2502 0:00:01.447082 \u2502   5,500 \u2502\n\u2502 Glob scenarios \u2502 Glob shallow non-recursive \u2502         10 \u2502 0:00:00.978217 \u2502 \u00b1 0:00:00.091849 \u2502 0:00:01.230822 \u2502   5,500 \u2502\n\u2502 Glob scenarios \u2502 Glob normal recursive      \u2502         10 \u2502 0:00:01.510334 \u2502 \u00b1 0:00:00.101108 \u2502 0:00:01.789393 \u2502   7,272 \u2502\n\u2502 Glob scenarios \u2502 Glob normal non-recursive  \u2502         10 \u2502 0:00:00.058301 \u2502 \u00b1 0:00:00.002621 \u2502 0:00:00.063299 \u2502      12 \u2502\n\u2502 Glob scenarios \u2502 Glob deep recursive        \u2502         10 \u2502 0:00:02.784629 \u2502 \u00b1 0:00:00.099764 \u2502 0:00:02.981882 \u2502   7,650 \u2502\n\u2502 Glob scenarios \u2502 Glob deep non-recursive    \u2502         10 \u2502 0:00:00.051322 \u2502 \u00b1 0:00:00.002653 \u2502 0:00:00.054844 \u2502      25 \u2502\n\u2502 Walk scenarios \u2502 Walk shallow               \u2502         10 \u2502 0:00:00.905571 \u2502 \u00b1 0:00:00.076332 \u2502 0:00:01.113957 \u2502   5,500 \u2502\n\u2502 Walk scenarios \u2502 Walk normal                \u2502         10 \u2502 0:00:01.441215 \u2502 \u00b1 0:00:00.014923 \u2502 0:00:01.470414 \u2502   7,272 \u2502\n\u2502 Walk scenarios \u2502 Walk deep                  \u2502         10 \u2502 0:00:02.461520 \u2502 \u00b1 0:00:00.031832 \u2502 0:00:02.539132 \u2502   7,650 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>To see how it is used in PR, you can see an example here.</p>"},{"location":"contributing/#exceptions","title":"Exceptions","text":"<p>Different backends may raise different exception classses when something goes wrong. To make it easy for users to catch exceptions that are agnostic of the backend, we generally will catch and raise a specific exception from <code>exceptions.py</code> for any exception that we understand. You can add new exceptions to this file if any are needed for new features.</p>"},{"location":"contributing/#code-architecture","title":"Code architecture","text":"<p>The best guide to the style and architecture is to read the code itself, but we provide some overarching guidance here.</p>"},{"location":"contributing/#cloud-provider-abstraction","title":"Cloud provider abstraction","text":"<p>We want to support as many providers as possible. Therefore, our goal is to keep the surface area of the <code>*Client</code> class small so it is easy to build extensions and new backends.</p> <p>Generic functionality like setting defaults and caching are implemented in the <code>Client</code> class. This also defines the interface that the <code>*Client</code> backends for each provider needs to implement.</p> <p>Each provider has its own <code>*Client</code> class that implements the interface defined in <code>Client</code>. The <code>*Client</code> classes are responsible for implementing the interface defined in <code>Client</code> for that specific backend. For an example, you could look at the <code>S3Client</code>.</p>"},{"location":"contributing/#cloudpath-abstraction","title":"<code>CloudPath</code> abstraction","text":"<p>The core <code>cloudpath.py</code> file provides most of the method implementations in a provider-agnostic way. Most feature changes will happen in the <code>CloudPath</code> class, unless there is a provider specific issue. There are a number of ways that functionality gets implemented:</p> <ul> <li>Some methods are implemented from scratch for cloud paths</li> <li>Some methods are implemented by calling the <code>pathlib.Path</code> version on either (1) the file in the cache if concrete, or (2) a <code>PurePosixPath</code> conversion of the CloudPath if not concrete.</li> <li>Some methods that are not relevant for a cloud backend are not implemented.</li> </ul> <p>Any code that needs to interact with the provider does so by calling methods on the <code>CloudPath.client</code>, which is an instance of the <code>Client</code> class so all the methods are provider-agnostic.</p> <p>Some methods are implemented on the <code>*Path</code> class for the specific provider. This is reserved for two cases: (1) provider-specific properties, like <code>S3Path.bucket</code> or <code>AzureBlobPath.container</code>, and (2) methods that are more efficiently implemented in a provider-specific way, like <code>S3Path.stat</code>. </p>"},{"location":"contributing/#adding-a-new-provider","title":"Adding a new provider","text":"<p>Adding a new provider is relatively straightforward. If you are extending <code>cloudpathlib</code>, but don't intend to make your provider part of the core library, implement the following pieces.</p>"},{"location":"contributing/#a-client-class-that-inherits-from-client","title":"A <code>*Client</code> class that inherits from <code>Client</code>","text":"<pre><code>from cloudpathlib.client import Client\n\nclass MyClient(Client):\n    # implementation here...\n</code></pre>"},{"location":"contributing/#a-path-class-that-inherits-from-cloudpath","title":"A <code>*Path</code> class that inherits from <code>CloudPath</code>","text":"<pre><code>from cloudpathlib.cloudpath import CloudPath\n\nclass MyPath(CloudPath):\n    cloud_prefix: str = \"my-prefix://\"\n    client: \"MyClient\"\n\n    # implementation here...\n</code></pre> <p>This <code>*Path</code> class should also be registered, if you want dispatch from <code>CloudPath</code> to work (see the next section).</p> <p>If you do intend to make your provider part of the code library, you will also need to do the following steps:</p> <ul> <li>Export the client and path classes in <code>cloudpathlib/__init__.py</code></li> <li>Write a mock backend for local backend testing in <code>tests/mock_clients</code></li> <li>Add a rig to run tests against the backend in <code>tests/conftest.py</code></li> <li>Ensure documentation is updated</li> </ul>"},{"location":"contributing/#register-your-provider-for-dispatch-from-cloudpath","title":"Register your provider for dispatch from <code>CloudPath</code>","text":"<p>Register your provider so <code>CloudPath(\"my-prefix://my-bucket/my-file.txt\")</code> will return a <code>MyPath</code> object:</p> <pre><code>from cloudpathlib.client import Client, register_client_class\nfrom cloudpathlib.cloudpath import CloudPath, register_path_class\n\n@register_client_class(\"my-prefix\")\nclass MyClient(Client):\n    # implementation here...\n\n@register_path_class(\"my-prefix\")\nclass MyPath(CloudPath):\n    cloud_prefix: str = \"my-prefix://\"\n    client: \"MyClient\"\n\n    # implementation here...\n</code></pre> <p>If you are submitting a PR to add this to the official <code>cloudpathlib</code> library, you will also need to do the following steps:</p> <ul> <li>Export the client and path classes in <code>cloudpathlib/__init__.py</code></li> <li>Write a mock backend for local backend testing in <code>tests/mock_clients</code></li> <li>Add a rig to run tests against the backend in <code>tests/conftest.py</code></li> <li>Ensure documentation is updated</li> </ul>"},{"location":"contributing/#governance","title":"Governance","text":"<p>Ultimately, the maintainers of <code>cloudpathlib</code> need to use their discretion when accepting new features. Proposed contributions may not be accepted for a variety of reasons. Some proposed contributions to the library may be judged to introduce too large a maintenance burden. Some proposed contributions may be judged to be out of scope for the project. Still other contributions may be judged to to not fit the API for stylistic reasons or the technical direction of the project.</p> <p>We appreciate your understanding if your contribution is not accepted. The maintainers will do our best to explain our reasoning, but ultimately we need to make decisions that we feel are in the best interest of the project as a whole. It can be frustrating if you don't agree, but we hope you will understand that we are trying to make the best decisions we can.</p>"},{"location":"integrations/","title":"Integrations with Other Libraries","text":""},{"location":"integrations/#pydantic","title":"Pydantic","text":"<p><code>cloudpathlib</code> integrates with Pydantic's data validation. You can declare fields with cloud path classes, and Pydantic's validation mechanisms will run inputs through the cloud path's constructor.</p> <pre><code>from cloudpathlib import S3Path\nfrom pydantic import BaseModel\n\nclass MyModel(BaseModel):\n    s3_file: S3Path\n\ninst = MyModel(s3_file=\"s3://mybucket/myfile.txt\")\ninst.s3_file\n#&gt; S3Path('s3://mybucket/myfile.txt')\n</code></pre> <p>This also works with the <code>AnyPath</code> polymorphic class. Inputs will get dispatched and instantiated as the appropriate class.</p> <pre><code>from cloudpathlib import AnyPath\nfrom pydantic import BaseModel\n\nclass FancyModel(BaseModel):\n    path: AnyPath\n\nfancy1 = FancyModel(path=\"s3://mybucket/myfile.txt\")\nfancy1.path\n#&gt; S3Path('s3://mybucket/myfile.txt')\n\nfancy2 = FancyModel(path=\"mydir/myfile.txt\")\nfancy2.path\n#&gt; PosixPath('mydir/myfile.txt')\n</code></pre> <p><sup>Examples created with reprexlite</sup></p>"},{"location":"other_client_settings/","title":"Other <code>Client</code> settings","text":""},{"location":"other_client_settings/#content-type-guessing-content_type_method","title":"Content type guessing (<code>content_type_method</code>)","text":"<p>All of the clients support passing a <code>content_type_method</code> when they are instantiated. This is a method that is used to guess the MIME (media) type (often called the \"content type\") of the file and set that on the cloud provider.</p> <p>By default, <code>content_type_method</code> use the Python built-in  <code>guess_type</code> to set this content type. This guesses based on the file extension, and may not always get the correct type. In these cases, you can set <code>content_type_method</code> to your own function that gets the proper type; for example, by  reading the file content or by looking it up in a dictionary of filename-to-media-type mappings that you maintain.</p> <p>If you set a custom method, it should follow the signature of <code>guess_type</code> and return a tuple of the form:  <code>(content_type, content_encoding)</code>; for example, <code>(\"text/css\", None)</code>.</p> <p>If you set <code>content_type_method</code> to None, it will do whatever the default of the cloud provider's SDK does. This varies from provider to provider.</p> <p>Here is an example of using a custom <code>content_type_method</code>.</p> <pre><code>import mimetypes\nfrom pathlib import Path\n\nfrom cloudpathlib import S3Client, CloudPath\n\ndef my_content_type(path):\n    # do lookup for content types I define; fallback to\n    # guess_type for anything else\n    return {\n        \".potato\": (\"application/potato\", None),\n    }.get(Path(path).suffix, mimetypes.guess_type(path))\n\n\n# create a client with my custom content type\nclient = S3Client(content_type_method=my_content_type)\n\n# To use this same method for every cloud path, set our client as the default.\n# This is optional, and you could use client.CloudPath to create paths instead.\nclient.set_as_default_client()\n\n# create a cloud path\ncp1 = CloudPath(\"s3://cloudpathlib-test-bucket/i_am_a.potato\")\ncp1.write_text(\"hello\")\n\n# check content type with boto3\nprint(client.s3.Object(cp1.bucket, cp1.key).content_type)\n#&gt; application/potato\n</code></pre>"},{"location":"testing_mocked_cloudpathlib/","title":"Testing code that uses cloudpathlib","text":"<p>Testing code that interacts with external resources can be a pain. For automated unit tests, the best practice is to mock connections. We provide some tools in cloudpathlib to make mocking easier.</p> <p>In this section, we will show a few examples of how to mock cloudpathlib classes in the popular pytest framework using its monkeypatch feature. The general principles should work equivalently if you are using unittest.mock from the Python standard library. If you are new to mocking or having trouble applying it, we recommend you read and understand \"Where to patch\".</p> In\u00a0[1]: hide-input Copied! <pre>import ipytest\n\nipytest.autoconfig()\n</pre> import ipytest  ipytest.autoconfig() <p>In this example, we are testing a function <code>write</code> that directly instantiates a path using the <code>S3Path</code> constructor.</p> <p>Normally, calling <code>write</code> would either write to the real S3 bucket if able to authenticate, or it would fail with an error like <code>botocore.exceptions.NoCredentialsError</code>.</p> <p>We use <code>monkeypatch</code> to replace the reference to <code>S3Path</code> being used with <code>LocalS3Path</code>. Our write succeeds (despite not being authenticated), and we can double-check that the cloud path object returned is actually an instance of <code>LocalS3Path</code>.</p> <p>Note that if you are writing tests for a package, and you import <code>write</code> from another module, you should patch the reference to <code>S3Path</code> from that module instead.</p> In\u00a0[2]: Copied! <pre>%%run_pytest[clean]\n\nimport cloudpathlib\nfrom cloudpathlib.local import LocalS3Path\n\n\ndef write(uri: str):\n    \"\"\"Function that uses S3Path.\"\"\"\n    cloud_path = cloudpathlib.S3Path(uri)\n    cloud_path.write_text(\"cumulonimbus\")\n    return cloud_path\n\n\ndef test_write_monkeypatch(monkeypatch):\n    \"\"\"Testing function using S3Path, patching with LocalS3Path.\"\"\"\n\n    monkeypatch.setattr(cloudpathlib, \"S3Path\", LocalS3Path)\n\n    cloud_path = write(\"s3://cloudpathlib-test-bucket/cumulonimbus.txt\")\n    assert isinstance(cloud_path, LocalS3Path)\n    assert cloud_path.read_text() == \"cumulonimbus\"\n</pre> %%run_pytest[clean]  import cloudpathlib from cloudpathlib.local import LocalS3Path   def write(uri: str):     \"\"\"Function that uses S3Path.\"\"\"     cloud_path = cloudpathlib.S3Path(uri)     cloud_path.write_text(\"cumulonimbus\")     return cloud_path   def test_write_monkeypatch(monkeypatch):     \"\"\"Testing function using S3Path, patching with LocalS3Path.\"\"\"      monkeypatch.setattr(cloudpathlib, \"S3Path\", LocalS3Path)      cloud_path = write(\"s3://cloudpathlib-test-bucket/cumulonimbus.txt\")     assert isinstance(cloud_path, LocalS3Path)     assert cloud_path.read_text() == \"cumulonimbus\"  <pre>.                                                                                                                                   [100%]\n1 passed in 0.01s\n</pre> In\u00a0[3]: Copied! <pre>from cloudpathlib import implementation_registry\n\nimplementation_registry\n</pre> from cloudpathlib import implementation_registry  implementation_registry Out[3]: <pre>defaultdict(cloudpathlib.cloudpath.CloudImplementation,\n            {'azure': &lt;cloudpathlib.cloudpath.CloudImplementation at 0x7fe850d29ee0&gt;,\n             's3': &lt;cloudpathlib.cloudpath.CloudImplementation at 0x7fe8527ee040&gt;,\n             'gs': &lt;cloudpathlib.cloudpath.CloudImplementation at 0x7fe852d38a00&gt;})</pre> <p>We use <code>monkeypatch</code> to replace the <code>CloudImplementation</code> object in the registry that is keyed to <code>\"s3\"</code> with the <code>local_s3_implementation</code> object that we import from the <code>cloudpathlib.local</code> module. Our write succeeds, and we can double-check that the created cloud path object is indeed a <code>LocalS3Path</code> instance.</p> In\u00a0[4]: Copied! <pre>%%run_pytest[clean]\n\nfrom cloudpathlib import CloudPath, implementation_registry\nfrom cloudpathlib.local import LocalS3Path, local_s3_implementation\n\n\ndef write_with_dispatch(uri: str):\n    \"\"\"Function that uses CloudPath to dispatch to S3Path.\"\"\"\n    cloud_path = CloudPath(uri)\n    cloud_path.write_text(\"cirrocumulus\")\n    return cloud_path\n\n\ndef test_write_with_dispatch_monkeypatch(monkeypatch):\n    \"\"\"Testing function using CloudPath dispatch, patching registered implementation. Will pass.\"\"\"\n\n    monkeypatch.setitem(implementation_registry, \"s3\", local_s3_implementation)\n\n    cloud_path = write_with_dispatch(\"s3://cloudpathlib-test-bucket/cirrocumulus.txt\")\n    assert isinstance(cloud_path, LocalS3Path)\n    assert cloud_path.read_text() == \"cirrocumulus\"\n</pre> %%run_pytest[clean]  from cloudpathlib import CloudPath, implementation_registry from cloudpathlib.local import LocalS3Path, local_s3_implementation   def write_with_dispatch(uri: str):     \"\"\"Function that uses CloudPath to dispatch to S3Path.\"\"\"     cloud_path = CloudPath(uri)     cloud_path.write_text(\"cirrocumulus\")     return cloud_path   def test_write_with_dispatch_monkeypatch(monkeypatch):     \"\"\"Testing function using CloudPath dispatch, patching registered implementation. Will pass.\"\"\"      monkeypatch.setitem(implementation_registry, \"s3\", local_s3_implementation)      cloud_path = write_with_dispatch(\"s3://cloudpathlib-test-bucket/cirrocumulus.txt\")     assert isinstance(cloud_path, LocalS3Path)     assert cloud_path.read_text() == \"cirrocumulus\"  <pre>.                                                                                                                                   [100%]\n1 passed in 0.01s\n</pre> <p>In this example, we set up test assets in a pytest fixture before running our tests. (We also do the monkeypatching in the fixture\u2014a code pattern for better reuse.)</p> <p>There are two options for interacting with the storage backend for the local path classes. The example fixture below shows both options in action.</p> In\u00a0[5]: Copied! <pre>%%run_pytest[clean]\n\nimport pytest\n\nfrom cloudpathlib import CloudPath, implementation_registry\nfrom cloudpathlib.local import LocalS3Client, LocalS3Path, local_s3_implementation\n\n\n@pytest.fixture\ndef cloud_asset_file(monkeypatch):\n    \"\"\"Fixture that patches CloudPath dispatch and also sets up test assets in LocalS3Client's\n    local storage directory.\"\"\"\n\n    monkeypatch.setitem(implementation_registry, \"s3\", local_s3_implementation)\n\n    # Option 1: Use LocalS3Path to set up test assets directly\n    local_cloud_path = LocalS3Path(\"s3://cloudpathlib-test-bucket/altostratus.txt\")\n    local_cloud_path.write_text(\"altostratus\")\n    \n    # Option 2: Use the pathlib.Path object that points to the local storage directory\n    local_pathlib_path: Path = (\n        LocalS3Client.get_default_storage_dir() / \"cloudpathlib-test-bucket\" / \"nimbostratus.txt\"\n    )\n    local_pathlib_path.parent.mkdir(exist_ok=True, parents=True)\n    local_pathlib_path.write_text(\"nimbostratus\")\n\n    yield\n\n    LocalS3Client.reset_default_storage_dir()  # clean up temp directory and replace with new one\n\n\ndef test_with_assets(cloud_asset_file):\n    \"\"\"Testing that a patched CloudPath finds the test asset created in the fixture.\"\"\"\n\n    cloud_path_1 = CloudPath(\"s3://cloudpathlib-test-bucket/altostratus.txt\")\n    assert isinstance(cloud_path_1, LocalS3Path)\n    assert cloud_path_1.exists()\n    assert cloud_path_1.read_text() == \"altostratus\"\n    \n    cloud_path_2 = CloudPath(\"s3://cloudpathlib-test-bucket/nimbostratus.txt\")\n    assert isinstance(cloud_path_2, LocalS3Path)\n    assert cloud_path_2.exists()\n    assert cloud_path_2.read_text() == \"nimbostratus\"\n</pre> %%run_pytest[clean]  import pytest  from cloudpathlib import CloudPath, implementation_registry from cloudpathlib.local import LocalS3Client, LocalS3Path, local_s3_implementation   @pytest.fixture def cloud_asset_file(monkeypatch):     \"\"\"Fixture that patches CloudPath dispatch and also sets up test assets in LocalS3Client's     local storage directory.\"\"\"      monkeypatch.setitem(implementation_registry, \"s3\", local_s3_implementation)      # Option 1: Use LocalS3Path to set up test assets directly     local_cloud_path = LocalS3Path(\"s3://cloudpathlib-test-bucket/altostratus.txt\")     local_cloud_path.write_text(\"altostratus\")          # Option 2: Use the pathlib.Path object that points to the local storage directory     local_pathlib_path: Path = (         LocalS3Client.get_default_storage_dir() / \"cloudpathlib-test-bucket\" / \"nimbostratus.txt\"     )     local_pathlib_path.parent.mkdir(exist_ok=True, parents=True)     local_pathlib_path.write_text(\"nimbostratus\")      yield      LocalS3Client.reset_default_storage_dir()  # clean up temp directory and replace with new one   def test_with_assets(cloud_asset_file):     \"\"\"Testing that a patched CloudPath finds the test asset created in the fixture.\"\"\"      cloud_path_1 = CloudPath(\"s3://cloudpathlib-test-bucket/altostratus.txt\")     assert isinstance(cloud_path_1, LocalS3Path)     assert cloud_path_1.exists()     assert cloud_path_1.read_text() == \"altostratus\"          cloud_path_2 = CloudPath(\"s3://cloudpathlib-test-bucket/nimbostratus.txt\")     assert isinstance(cloud_path_2, LocalS3Path)     assert cloud_path_2.exists()     assert cloud_path_2.read_text() == \"nimbostratus\"  <pre>.                                                                                                                                   [100%]\n1 passed in 0.01s\n</pre>"},{"location":"testing_mocked_cloudpathlib/#testing-code-that-uses-cloudpathlib","title":"Testing code that uses cloudpathlib\u00b6","text":""},{"location":"testing_mocked_cloudpathlib/#cloudpathliblocal-module","title":"cloudpathlib.local module\u00b6","text":"<p>In the <code>cloudpathlib.local</code> module, we provide \"Local\" classes that use the local filesystem in place of cloud storage. These classes are drop-in replacements for the normal cloud path classes, with the intent that you can use them as mock or monkeypatch substitutes in your tests.</p> <p>We also provide <code>CloudImplementation</code> objects which can be used to replace a registered implementation in the <code>cloudpathlib.implementation_registry</code> dictionary. Replacing the registered implementation will make <code>CloudPath</code>'s automatic dispatching use the replacement.</p> <p>See the examples below for how to use these replacements in your tests.</p> Cloud Provider Standard Classes Local Classes Local Implementation Object Azure Blob Storage <code>AzureBlobClient</code><code>AzureBlobPath</code> <code>LocalAzureBlobCient</code><code>LocalAzureBlobPath</code> <code>local_azure_blob_implementation</code> Google Cloud Storage <code>GSClient</code><code>GSPath</code> <code>LocalGSClient</code><code>LocalGSPath</code> <code>local_gs_implementation</code> Amazon S3 <code>S3Client</code><code>S3Path</code> <code>LocalS3Client</code><code>LocalS3Path</code> <code>local_s3_implementation</code>"},{"location":"testing_mocked_cloudpathlib/#examples-monkeypatching-in-pytest","title":"Examples: Monkeypatching in pytest\u00b6","text":""},{"location":"testing_mocked_cloudpathlib/#patching-direct-instantiation","title":"Patching direct instantiation\u00b6","text":""},{"location":"testing_mocked_cloudpathlib/#patching-cloudpath-dispatch","title":"Patching CloudPath dispatch\u00b6","text":"<p>In this example, we are testing a function <code>write_with_dispatch</code> that uses the <code>CloudPath</code> constructor which dispatches to <code>S3Path</code> based on the <code>\"s3://\"</code> URI scheme.</p> <p>In order to change the dispatch behavior, we need to patch the cloudpathlib <code>implementation_registry</code>. The registry object is a dictionary (actually <code>defaultdict</code>) that holds meta <code>CloudImplementation</code> objects for each cloud storage service.</p>"},{"location":"testing_mocked_cloudpathlib/#setting-up-test-assets","title":"Setting up test assets\u00b6","text":""},{"location":"testing_mocked_cloudpathlib/#1-use-local-path-class-methods","title":"1. Use local path class methods.\u00b6","text":"<p>This is the easiest and most direct approach. For example, <code>LocalS3Path</code> is fully functional and implements the same methods as <code>S3Path</code>.</p>"},{"location":"testing_mocked_cloudpathlib/#2-get-a-pathlibpath-object-that-points-to-the-local-storage-directory","title":"2. Get a <code>pathlib.Path</code> object that points to the local storage directory.\u00b6","text":"<p>Each <code>LocalClient</code> class has a <code>TemporaryDirectory</code> instance that serves as its default local storage location. This is stored as an attribute of the class so that it persists across client instances. (For real cloud clients, authenticating multiple times to the same storage location doesn't affect the contents.)</p> <p>You can use the <code>get_default_storage_dir</code> class method to get back a <code>pathlib.Path</code> object for that directory. Then you can use whatever <code>pathlib</code> or <code>shutil</code> functions to interact with it.</p> <p>Finally, the <code>reset_default_storage_dir</code> class method will clean up the current local storage temporary directory and set up a new one. We recommend you do this in the teardown of the test fixture.</p>"},{"location":"why_cloudpathlib/","title":"Why cloudpathlib?","text":"In\u00a0[1]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path <p>For example, we can easily list all the files in a directory.</p> In\u00a0[2]: Copied! <pre>list(Path(\".\").glob(\"*\"))\n</pre> list(Path(\".\").glob(\"*\")) Out[2]: <pre>[PosixPath('caching.ipynb'),\n PosixPath('why_cloudpathlib.ipynb'),\n PosixPath('api-reference'),\n PosixPath('.ipynb_checkpoints'),\n PosixPath('logo-no-text.svg'),\n PosixPath('logo.svg'),\n PosixPath('authentication.ipynb'),\n PosixPath('favicon.svg'),\n PosixPath('stylesheets')]</pre> <p>There are methods to quickly learn everything there is to know about a filesystem path, and even do simple file manipulations.</p> In\u00a0[3]: Copied! <pre>notebook = Path(\"why_cloudpathlib.ipynb\").resolve()\n\nprint(f\"{'Path:':15}{notebook}\")\nprint(f\"{'Name:':15}{notebook.name}\")\nprint(f\"{'Stem:':15}{notebook.stem}\")\nprint(f\"{'Suffix:':15}{notebook.suffix}\")\nprint(f\"{'With suffix:':15}{notebook.with_suffix('.cpp')}\")\nprint(f\"{'Parent:':15}{notebook.parent}\")\nprint(f\"{'Read_text:'}\\n{notebook.read_text()[:200]}\\n\")\n</pre> notebook = Path(\"why_cloudpathlib.ipynb\").resolve()  print(f\"{'Path:':15}{notebook}\") print(f\"{'Name:':15}{notebook.name}\") print(f\"{'Stem:':15}{notebook.stem}\") print(f\"{'Suffix:':15}{notebook.suffix}\") print(f\"{'With suffix:':15}{notebook.with_suffix('.cpp')}\") print(f\"{'Parent:':15}{notebook.parent}\") print(f\"{'Read_text:'}\\n{notebook.read_text()[:200]}\\n\") <pre>Path:          /Users/bull/code/cloudpathlib/docs/docs/why_cloudpathlib.ipynb\nName:          why_cloudpathlib.ipynb\nStem:          why_cloudpathlib\nSuffix:        .ipynb\nWith suffix:   /Users/bull/code/cloudpathlib/docs/docs/why_cloudpathlib.cpp\nParent:        /Users/bull/code/cloudpathlib/docs/docs\nRead_text:\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Why cloudpathlib?\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## We \ud83d\ude0d pathl\n\n</pre> <p>If you're new to pathlib, we highly recommend it over the older <code>os.path</code> module. We find that it has a much more intuitive and convenient interface. The official documentation is a helpful reference, and we also recommend this excellent cheat sheet by Chris Moffitt.</p> In\u00a0[4]: Copied! <pre># load environment variables from .env file;\n# not required, just where we keep our creds\nfrom dotenv import load_dotenv, find_dotenv\n\nload_dotenv(find_dotenv())\n</pre> # load environment variables from .env file; # not required, just where we keep our creds from dotenv import load_dotenv, find_dotenv  load_dotenv(find_dotenv()) Out[4]: <pre>True</pre> In\u00a0[5]: Copied! <pre>from cloudpathlib import S3Path\n\ns3p = S3Path(\"s3://cloudpathlib-test-bucket/why_cloudpathlib/file.txt\")\ns3p.name\n</pre> from cloudpathlib import S3Path  s3p = S3Path(\"s3://cloudpathlib-test-bucket/why_cloudpathlib/file.txt\") s3p.name Out[5]: <pre>'file.txt'</pre> In\u00a0[6]: Copied! <pre># Nothing there yet...\ns3p.exists()\n</pre> # Nothing there yet... s3p.exists() Out[6]: <pre>False</pre> In\u00a0[7]: Copied! <pre># Touch (just like with `pathlib.Path`)\ns3p.touch()\n</pre> # Touch (just like with `pathlib.Path`) s3p.touch() In\u00a0[8]: Copied! <pre># Bingo!\ns3p.exists()\n</pre> # Bingo! s3p.exists() Out[8]: <pre>True</pre> In\u00a0[9]: Copied! <pre># list all the files in the directory\n[p for p in s3p.parent.iterdir()]\n</pre> # list all the files in the directory [p for p in s3p.parent.iterdir()] Out[9]: <pre>[S3Path('s3://cloudpathlib-test-bucket/why_cloudpathlib/file.txt')]</pre> In\u00a0[10]: Copied! <pre>stat = s3p.stat()\nprint(f\"File size in bytes: {stat.st_size}\")\nstat\n</pre> stat = s3p.stat() print(f\"File size in bytes: {stat.st_size}\") stat <pre>File size in bytes: 0\n</pre> Out[10]: <pre>os.stat_result(st_mode=None, st_ino=None, st_dev='s3://', st_nlink=None, st_uid=None, st_gid=None, st_size=0, st_atime=None, st_mtime=1601853143.0, st_ctime=None)</pre> In\u00a0[11]: Copied! <pre>s3p.write_text(\"Hello to all of my friends!\")\n</pre> s3p.write_text(\"Hello to all of my friends!\") Out[11]: <pre>27</pre> In\u00a0[12]: Copied! <pre>stat = s3p.stat()\nprint(f\"File size in bytes: {stat.st_size}\")\nstat\n</pre> stat = s3p.stat() print(f\"File size in bytes: {stat.st_size}\") stat <pre>File size in bytes: 27\n</pre> Out[12]: <pre>os.stat_result(st_mode=None, st_ino=None, st_dev='s3://', st_nlink=None, st_uid=None, st_gid=None, st_size=27, st_atime=None, st_mtime=1601853144.0, st_ctime=None)</pre> In\u00a0[13]: Copied! <pre># Delete (again just like with `pathlib.Path`)\ns3p.unlink()\n</pre> # Delete (again just like with `pathlib.Path`) s3p.unlink() In\u00a0[14]: Copied! <pre>s3p.exists()\n</pre> s3p.exists() Out[14]: <pre>False</pre> In\u00a0[15]: Copied! <pre>from cloudpathlib import AzureBlobPath\n\nazp = AzureBlobPath(\"az://cloudpathlib-test-container/file.txt\")\nazp.name\n</pre> from cloudpathlib import AzureBlobPath  azp = AzureBlobPath(\"az://cloudpathlib-test-container/file.txt\") azp.name Out[15]: <pre>'file.txt'</pre> In\u00a0[16]: Copied! <pre>azp.exists()\n</pre> azp.exists() Out[16]: <pre>False</pre> In\u00a0[17]: Copied! <pre>azp.write_text(\"I'm on Azure, boss.\")\n</pre> azp.write_text(\"I'm on Azure, boss.\") Out[17]: <pre>19</pre> In\u00a0[18]: Copied! <pre>azp.exists()\n</pre> azp.exists() Out[18]: <pre>True</pre> In\u00a0[19]: Copied! <pre># list all the files in the directory\n[p for p in azp.parent.iterdir()]\n</pre> # list all the files in the directory [p for p in azp.parent.iterdir()] Out[19]: <pre>[AzureBlobPath('az://cloudpathlib-test-container/file.txt')]</pre> In\u00a0[20]: Copied! <pre>azp.exists()\n</pre> azp.exists() Out[20]: <pre>True</pre> In\u00a0[21]: Copied! <pre>azp.unlink()\n</pre> azp.unlink() In\u00a0[22]: Copied! <pre>from cloudpathlib import CloudPath\n\ncloud_directory = CloudPath(\"s3://cloudpathlib-test-bucket/why_cloudpathlib/\")\n\nupload = cloud_directory / \"user_upload.txt\"\nupload.write_text(\"A user made this file!\")\n\nassert upload.exists()\nupload.unlink()\nassert not upload.exists()\n</pre> from cloudpathlib import CloudPath  cloud_directory = CloudPath(\"s3://cloudpathlib-test-bucket/why_cloudpathlib/\")  upload = cloud_directory / \"user_upload.txt\" upload.write_text(\"A user made this file!\")  assert upload.exists() upload.unlink() assert not upload.exists() In\u00a0[23]: Copied! <pre>from cloudpathlib import CloudPath\n\n# Changing this root path is the ONLY change!\ncloud_directory = CloudPath(\"az://cloudpathlib-test-container/why_cloudpathlib/\")\n\nupload = cloud_directory / \"user_upload.txt\"\nupload.write_text(\"A user made this file!\")\n\nassert upload.exists()\nupload.unlink()\nassert not upload.exists()\n</pre> from cloudpathlib import CloudPath  # Changing this root path is the ONLY change! cloud_directory = CloudPath(\"az://cloudpathlib-test-container/why_cloudpathlib/\")  upload = cloud_directory / \"user_upload.txt\" upload.write_text(\"A user made this file!\")  assert upload.exists() upload.unlink() assert not upload.exists()"},{"location":"why_cloudpathlib/#why-cloudpathlib","title":"Why cloudpathlib?\u00b6","text":""},{"location":"why_cloudpathlib/#we-pathlib","title":"We \ud83d\ude0d pathlib\u00b6","text":"<p><code>pathlib</code> a wonderful tool for working with filesystem paths, available from the Python 3 standard library.</p>"},{"location":"why_cloudpathlib/#cross-platform-support","title":"Cross-platform support\u00b6","text":"<p>One great feature about using <code>pathlib</code> over regular strings is that it lets you write code with cross-platform file paths. It \"just works\" on Windows too. Write path manipulations that can run on anyone's machine!</p> <pre>path = Path.home()\npath\n&gt;&gt;&gt; C:\\Users\\DrivenData\\\n\ndocs = path / 'Documents'\ndocs\n&gt;&gt;&gt; C:\\Users\\DrivenData\\Documents\n</pre>"},{"location":"why_cloudpathlib/#we-also-cloud-storage","title":"We also \ud83d\ude0d cloud storage\u00b6","text":"<p>This is great, but I live in the future. Not every file I care about is on my machine. What do I do when I am working on S3? Do I have to explicitly download every file before I can do things with them?</p> <p>Of course not, if you use cloudpathlib!</p>"},{"location":"why_cloudpathlib/#cross-cloud-support","title":"Cross-cloud support\u00b6","text":"<p>That's cool, but I use Azure Blob Storage\u2015what can I do?</p>"},{"location":"why_cloudpathlib/#cloud-hopping","title":"Cloud hopping\u00b6","text":"<p>Moving between cloud storage providers should be a simple as moving between disks on your computer. Let's say that the Senior Vice President of Tomfoolery comes to me and says, \"We've got a mandate to migrate our application to Azure Blob Storage from S3!\"</p> <p>No problem, if I used <code>cloudpathlib</code>! The <code>CloudPath</code> class constructor automatically dispatches to the appropriate concrete class, the same way that <code>pathlib.Path</code> does for different operating systems.</p>"},{"location":"api-reference/anypath/","title":"cloudpathlib.AnyPath","text":"<p>Polymorphic virtual superclass for CloudPath and pathlib.Path. Constructing an instance will automatically dispatch to CloudPath or Path based on the input. It also supports both isinstance and issubclass checks.</p> <p>This class also integrates with Pydantic. When used as a type declaration for a Pydantic BaseModel, the Pydantic validation process will appropriately run inputs through this class' constructor and dispatch to CloudPath or Path.</p> Source code in <code>cloudpathlib/anypath.py</code> <pre><code>class AnyPath(ABC):\n    \"\"\"Polymorphic virtual superclass for CloudPath and pathlib.Path. Constructing an instance will\n    automatically dispatch to CloudPath or Path based on the input. It also supports both\n    isinstance and issubclass checks.\n\n    This class also integrates with Pydantic. When used as a type declaration for a Pydantic\n    BaseModel, the Pydantic validation process will appropriately run inputs through this class'\n    constructor and dispatch to CloudPath or Path.\n    \"\"\"\n\n    def __new__(cls, *args, **kwargs) -&gt; Union[CloudPath, Path]:  # type: ignore\n        try:\n            return CloudPath(*args, **kwargs)  # type: ignore\n        except InvalidPrefixError as cloudpath_exception:\n            try:\n                if isinstance(args[0], str) and args[0].lower().startswith(\"file:\"):\n                    path = path_from_fileurl(args[0], **kwargs)\n                    for part in args[1:]:\n                        path /= part\n                    return path\n\n                return Path(*args, **kwargs)\n            except TypeError as path_exception:\n                raise AnyPathTypeError(\n                    \"Invalid input for both CloudPath and Path. \"\n                    f\"CloudPath exception: {repr(cloudpath_exception)} \"\n                    f\"Path exception: {repr(path_exception)}\"\n                )\n\n    # ===========  pydantic integration special methods ===============\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _source_type: Any, _handler):\n        \"\"\"Pydantic special method. See\n        https://docs.pydantic.dev/2.0/usage/types/custom/\"\"\"\n        try:\n            from pydantic_core import core_schema\n\n            return core_schema.no_info_after_validator_function(\n                cls.validate,\n                core_schema.any_schema(),\n            )\n        except ImportError:\n            return None\n\n    @classmethod\n    def validate(cls, v: str) -&gt; Union[CloudPath, Path]:\n        \"\"\"Pydantic special method. See\n        https://docs.pydantic.dev/2.0/usage/types/custom/\"\"\"\n        try:\n            return cls.__new__(cls, v)\n        except AnyPathTypeError as e:\n            # type errors no longer converted to validation errors\n            #  https://docs.pydantic.dev/2.0/migration/#typeerror-is-no-longer-converted-to-validationerror-in-validators\n            raise ValueError(e)\n\n    @classmethod\n    def __get_validators__(cls):\n        \"\"\"Pydantic special method. See\n        https://pydantic-docs.helpmanual.io/usage/types/#custom-data-types\"\"\"\n        yield cls._validate\n\n    @classmethod\n    def _validate(cls, value) -&gt; Union[CloudPath, Path]:\n        \"\"\"Used as a Pydantic validator. See\n        https://pydantic-docs.helpmanual.io/usage/types/#custom-data-types\"\"\"\n        # Note __new__ is static method and not a class method\n        return cls.__new__(cls, value)\n</code></pre>"},{"location":"api-reference/anypath/#cloudpathlib.anypath.AnyPath-methods","title":"Methods","text":""},{"location":"api-reference/anypath/#cloudpathlib.anypath.AnyPath.validate","title":"<code>validate(v: str) -&gt; Union[cloudpathlib.cloudpath.CloudPath, pathlib.Path]</code>  <code>classmethod</code>","text":"<p>Pydantic special method. See https://docs.pydantic.dev/2.0/usage/types/custom/</p> Source code in <code>cloudpathlib/anypath.py</code> <pre><code>@classmethod\ndef validate(cls, v: str) -&gt; Union[CloudPath, Path]:\n    \"\"\"Pydantic special method. See\n    https://docs.pydantic.dev/2.0/usage/types/custom/\"\"\"\n    try:\n        return cls.__new__(cls, v)\n    except AnyPathTypeError as e:\n        # type errors no longer converted to validation errors\n        #  https://docs.pydantic.dev/2.0/migration/#typeerror-is-no-longer-converted-to-validationerror-in-validators\n        raise ValueError(e)\n</code></pre>"},{"location":"api-reference/azblobclient/","title":"cloudpathlib.AzureBlobClient","text":"<p>Client class for Azure Blob Storage which handles authentication with Azure for <code>AzureBlobPath</code> instances. See documentation for the <code>__init__</code> method for detailed authentication options.</p> Source code in <code>cloudpathlib/azure/azblobclient.py</code> <pre><code>class AzureBlobClient(Client):\n    \"\"\"Client class for Azure Blob Storage which handles authentication with Azure for\n    [`AzureBlobPath`](../azblobpath/) instances. See documentation for the\n    [`__init__` method][cloudpathlib.azure.azblobclient.AzureBlobClient.__init__] for detailed\n    authentication options.\n    \"\"\"\n\n    def __init__(\n        self,\n        account_url: Optional[str] = None,\n        credential: Optional[Any] = None,\n        connection_string: Optional[str] = None,\n        blob_service_client: Optional[\"BlobServiceClient\"] = None,\n        data_lake_client: Optional[\"DataLakeServiceClient\"] = None,\n        file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n        local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n        content_type_method: Optional[Callable] = mimetypes.guess_type,\n    ):\n        \"\"\"Class constructor. Sets up a [`BlobServiceClient`](\n        https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python).\n        Supports the following authentication methods of `BlobServiceClient`.\n\n        - Environment variable `\"\"AZURE_STORAGE_CONNECTION_STRING\"` containing connecting string\n        with account credentials. See [Azure Storage SDK documentation](\n        https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python#copy-your-credentials-from-the-azure-portal).\n        - Connection string via `connection_string`, authenticated either with an embedded SAS\n        token or with credentials passed to `credentials`.\n        - Account URL via `account_url`, authenticated either with an embedded SAS token, or with\n        credentials passed to `credentials`.\n        - Instantiated and already authenticated [`BlobServiceClient`](\n        https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python) or\n        [`DataLakeServiceClient`](https://learn.microsoft.com/en-us/python/api/azure-storage-file-datalake/azure.storage.filedatalake.datalakeserviceclient).\n\n        If multiple methods are used, priority order is reverse of list above (later in list takes\n        priority). If no methods are used, a [`MissingCredentialsError`][cloudpathlib.exceptions.MissingCredentialsError]\n        exception will be raised raised.\n\n        Args:\n            account_url (Optional[str]): The URL to the blob storage account, optionally\n                authenticated with a SAS token. See documentation for [`BlobServiceClient`](\n                https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python).\n            credential (Optional[Any]): Credentials with which to authenticate. Can be used with\n                `account_url` or `connection_string`, but is unnecessary if the other already has\n                an SAS token. See documentation for [`BlobServiceClient`](\n                https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python)\n                or [`BlobServiceClient.from_connection_string`](\n                https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python#from-connection-string-conn-str--credential-none----kwargs-).\n            connection_string (Optional[str]): A connection string to an Azure Storage account. See\n                [Azure Storage SDK documentation](\n                https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python#copy-your-credentials-from-the-azure-portal).\n            blob_service_client (Optional[BlobServiceClient]): Instantiated [`BlobServiceClient`](\n                https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python).\n            data_lake_client (Optional[DataLakeServiceClient]): Instantiated [`DataLakeServiceClient`](\n                https://learn.microsoft.com/en-us/python/api/azure-storage-file-datalake/azure.storage.filedatalake.datalakeserviceclient).\n                If None and `blob_service_client` is passed, we will create based on that.\n                Otherwise, will create based on passed credential, account_url, connection_string, or AZURE_STORAGE_CONNECTION_STRING env var\n            file_cache_mode (Optional[Union[str, FileCacheMode]]): How often to clear the file cache; see\n                [the caching docs](https://cloudpathlib.drivendata.org/stable/caching/) for more information\n                about the options in cloudpathlib.eums.FileCacheMode.\n            local_cache_dir (Optional[Union[str, os.PathLike]]): Path to directory to use as cache\n                for downloaded files. If None, will use a temporary directory. Default can be set with\n                the `CLOUDPATHLIB_LOCAL_CACHE_DIR` environment variable.\n            content_type_method (Optional[Callable]): Function to call to guess media type (mimetype) when\n                writing a file to the cloud. Defaults to `mimetypes.guess_type`. Must return a tuple (content type, content encoding).\n        \"\"\"\n        super().__init__(\n            local_cache_dir=local_cache_dir,\n            content_type_method=content_type_method,\n            file_cache_mode=file_cache_mode,\n        )\n\n        if connection_string is None:\n            connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\", None)\n\n        self.data_lake_client = None  # only needs to end up being set if HNS is enabled\n\n        if blob_service_client is not None:\n            self.service_client = blob_service_client\n\n            # create from blob service client if not passed\n            if data_lake_client is None:\n                self.data_lake_client = DataLakeServiceClient(\n                    account_url=self.service_client.url.replace(\".blob.\", \".dfs.\", 1),\n                    credential=AzureNamedKeyCredential(\n                        blob_service_client.credential.account_name,\n                        blob_service_client.credential.account_key,\n                    ),\n                )\n            else:\n                self.data_lake_client = data_lake_client\n\n        elif data_lake_client is not None:\n            self.data_lake_client = data_lake_client\n\n            if blob_service_client is None:\n                self.service_client = BlobServiceClient(\n                    account_url=self.data_lake_client.url.replace(\".dfs.\", \".blob.\", 1),\n                    credential=AzureNamedKeyCredential(\n                        data_lake_client.credential.account_name,\n                        data_lake_client.credential.account_key,\n                    ),\n                )\n\n        elif connection_string is not None:\n            self.service_client = BlobServiceClient.from_connection_string(\n                conn_str=connection_string, credential=credential\n            )\n            self.data_lake_client = DataLakeServiceClient.from_connection_string(\n                conn_str=connection_string, credential=credential\n            )\n        elif account_url is not None:\n            if \".dfs.\" in account_url:\n                self.service_client = BlobServiceClient(\n                    account_url=account_url.replace(\".dfs.\", \".blob.\"), credential=credential\n                )\n                self.data_lake_client = DataLakeServiceClient(\n                    account_url=account_url, credential=credential\n                )\n            elif \".blob.\" in account_url:\n                self.service_client = BlobServiceClient(\n                    account_url=account_url, credential=credential\n                )\n                self.data_lake_client = DataLakeServiceClient(\n                    account_url=account_url.replace(\".blob.\", \".dfs.\"), credential=credential\n                )\n            else:\n                # assume default to blob; HNS not supported\n                self.service_client = BlobServiceClient(\n                    account_url=account_url, credential=credential\n                )\n\n        else:\n            raise MissingCredentialsError(\n                \"AzureBlobClient does not support anonymous instantiation. \"\n                \"Credentials are required; see docs for options.\"\n            )\n\n        self._hns_enabled = None\n\n    def _check_hns(self) -&gt; Optional[bool]:\n        if self._hns_enabled is None:\n            account_info = self.service_client.get_account_information()  # type: ignore\n            self._hns_enabled = account_info.get(\"is_hns_enabled\", False)  # type: ignore\n\n        return self._hns_enabled\n\n    def _get_metadata(\n        self, cloud_path: AzureBlobPath\n    ) -&gt; Union[\"BlobProperties\", \"FileProperties\", Dict[str, Any]]:\n        if self._check_hns():\n\n            # works on both files and directories\n            fsc = self.data_lake_client.get_file_system_client(cloud_path.container)  # type: ignore\n\n            if fsc is not None:\n                properties = fsc.get_file_client(cloud_path.blob).get_file_properties()\n\n            # no content settings on directory\n            properties[\"content_type\"] = properties.get(\n                \"content_settings\", {\"content_type\": None}\n            ).get(\"content_type\")\n\n        else:\n            blob = self.service_client.get_blob_client(\n                container=cloud_path.container, blob=cloud_path.blob\n            )\n            properties = blob.get_blob_properties()\n\n            properties[\"content_type\"] = properties.content_settings.content_type\n\n        return properties\n\n    @staticmethod\n    def _partial_filename(local_path) -&gt; Path:\n        return Path(str(local_path) + \".part\")\n\n    def _download_file(\n        self, cloud_path: AzureBlobPath, local_path: Union[str, os.PathLike]\n    ) -&gt; Path:\n        blob = self.service_client.get_blob_client(\n            container=cloud_path.container, blob=cloud_path.blob\n        )\n\n        download_stream = blob.download_blob()\n\n        local_path = Path(local_path)\n\n        local_path.parent.mkdir(exist_ok=True, parents=True)\n\n        try:\n            partial_local_path = self._partial_filename(local_path)\n            with partial_local_path.open(\"wb\") as data:\n                download_stream.readinto(data)\n\n            partial_local_path.replace(local_path)\n        except:  # noqa: E722\n            # remove any partial download\n            if partial_local_path.exists():\n                partial_local_path.unlink()\n            raise\n\n        return local_path\n\n    def _is_file_or_dir(self, cloud_path: AzureBlobPath) -&gt; Optional[str]:\n        # short-circuit the root-level container\n        if not cloud_path.blob:\n            return \"dir\"\n\n        try:\n            meta = self._get_metadata(cloud_path)\n\n            # if hns, has is_directory property; else if not hns, _get_metadata will raise if not a file\n            return (\n                \"dir\"\n                if meta.get(\"is_directory\", False)\n                or meta.get(\"metadata\", {}).get(\"hdi_isfolder\", False)\n                else \"file\"\n            )\n\n        # thrown if not HNS and file does not exist _or_ is dir; check if is dir instead\n        except ResourceNotFoundError:\n            prefix = cloud_path.blob\n            if prefix and not prefix.endswith(\"/\"):\n                prefix += \"/\"\n\n            # not a file, see if it is a directory\n            container_client = self.service_client.get_container_client(cloud_path.container)\n\n            try:\n                next(container_client.list_blobs(name_starts_with=prefix))\n                return \"dir\"\n            except StopIteration:\n                return None\n\n    def _exists(self, cloud_path: AzureBlobPath) -&gt; bool:\n        # short circuit when only the container\n        if not cloud_path.blob:\n            return self.service_client.get_container_client(cloud_path.container).exists()\n\n        return self._is_file_or_dir(cloud_path) in [\"file\", \"dir\"]\n\n    def _list_dir(\n        self, cloud_path: AzureBlobPath, recursive: bool = False\n    ) -&gt; Iterable[Tuple[AzureBlobPath, bool]]:\n        if not cloud_path.container:\n            for container in self.service_client.list_containers():\n                yield self.CloudPath(f\"az://{container.name}\"), True\n\n                if not recursive:\n                    continue\n\n                yield from self._list_dir(self.CloudPath(f\"az://{container.name}\"), recursive=True)\n            return\n\n        container_client = self.service_client.get_container_client(cloud_path.container)\n\n        prefix = cloud_path.blob\n        if prefix and not prefix.endswith(\"/\"):\n            prefix += \"/\"\n\n        if self._check_hns():\n            file_system_client = self.data_lake_client.get_file_system_client(cloud_path.container)  # type: ignore\n            paths = file_system_client.get_paths(path=cloud_path.blob, recursive=recursive)\n\n            for path in paths:\n                yield self.CloudPath(f\"az://{cloud_path.container}/{path.name}\"), path.is_directory\n\n        else:\n            if not recursive:\n                blobs = container_client.walk_blobs(name_starts_with=prefix)\n            else:\n                blobs = container_client.list_blobs(name_starts_with=prefix)\n\n            for blob in blobs:\n                # walk_blobs returns folders with a trailing slash\n                blob_path = blob.name.rstrip(\"/\")\n                blob_cloud_path = self.CloudPath(f\"az://{cloud_path.container}/{blob_path}\")\n\n                yield blob_cloud_path, (\n                    isinstance(blob, BlobPrefix)\n                    if not recursive\n                    else False  # no folders from list_blobs in non-hns storage accounts\n                )\n\n    def _move_file(\n        self, src: AzureBlobPath, dst: AzureBlobPath, remove_src: bool = True\n    ) -&gt; AzureBlobPath:\n        # just a touch, so \"REPLACE\" metadata\n        if src == dst:\n            blob_client = self.service_client.get_blob_client(\n                container=src.container, blob=src.blob\n            )\n\n            blob_client.set_blob_metadata(\n                metadata=dict(last_modified=str(datetime.utcnow().timestamp()))\n            )\n\n        # we can use rename API when the same account on adls gen2\n        elif remove_src and (src.client is dst.client) and self._check_hns():\n            fsc = self.data_lake_client.get_file_system_client(src.container)  # type: ignore\n\n            if src.is_dir():\n                fsc.get_directory_client(src.blob).rename_directory(f\"{dst.container}/{dst.blob}\")\n            else:\n                dst.parent.mkdir(parents=True, exist_ok=True)\n                fsc.get_file_client(src.blob).rename_file(f\"{dst.container}/{dst.blob}\")\n\n        else:\n            target = self.service_client.get_blob_client(container=dst.container, blob=dst.blob)\n\n            source = self.service_client.get_blob_client(container=src.container, blob=src.blob)\n\n            target.start_copy_from_url(source.url)\n\n            if remove_src:\n                self._remove(src)\n\n        return dst\n\n    def _mkdir(\n        self, cloud_path: AzureBlobPath, parents: bool = False, exist_ok: bool = False\n    ) -&gt; None:\n        if self._check_hns():\n            file_system_client = self.data_lake_client.get_file_system_client(cloud_path.container)  # type: ignore\n            directory_client = file_system_client.get_directory_client(cloud_path.blob)\n\n            if not exist_ok and directory_client.exists():\n                raise FileExistsError(f\"Directory already exists: {cloud_path}\")\n\n            if not parents:\n                if not self._exists(cloud_path.parent):\n                    raise FileNotFoundError(\n                        f\"Parent directory does not exist ({cloud_path.parent}). To create parent directories, use `parents=True`.\"\n                    )\n\n            directory_client.create_directory()\n        else:\n            # consistent with other mkdir no-op behavior on other backends if not supported\n            pass\n\n    def _remove(self, cloud_path: AzureBlobPath, missing_ok: bool = True) -&gt; None:\n        file_or_dir = self._is_file_or_dir(cloud_path)\n        if file_or_dir == \"dir\":\n            if self._check_hns():\n                _hns_rmtree(self.data_lake_client, cloud_path.container, cloud_path.blob)\n                return\n\n            blobs = [\n                b.blob for b, is_dir in self._list_dir(cloud_path, recursive=True) if not is_dir\n            ]\n            container_client = self.service_client.get_container_client(cloud_path.container)\n            container_client.delete_blobs(*blobs)\n        elif file_or_dir == \"file\":\n            blob = self.service_client.get_blob_client(\n                container=cloud_path.container, blob=cloud_path.blob\n            )\n\n            blob.delete_blob()\n        else:\n            # Does not exist\n            if not missing_ok:\n                raise FileNotFoundError(f\"File does not exist: {cloud_path}\")\n\n    def _upload_file(\n        self, local_path: Union[str, os.PathLike], cloud_path: AzureBlobPath\n    ) -&gt; AzureBlobPath:\n        blob = self.service_client.get_blob_client(\n            container=cloud_path.container, blob=cloud_path.blob\n        )\n\n        extra_args = {}\n        if self.content_type_method is not None:\n            content_type, content_encoding = self.content_type_method(str(local_path))\n\n            if content_type is not None:\n                extra_args[\"content_type\"] = content_type\n            if content_encoding is not None:\n                extra_args[\"content_encoding\"] = content_encoding\n\n        content_settings = ContentSettings(**extra_args)\n\n        with Path(local_path).open(\"rb\") as data:\n            blob.upload_blob(data, overwrite=True, content_settings=content_settings)  # type: ignore\n\n        return cloud_path\n\n    def _get_public_url(self, cloud_path: AzureBlobPath) -&gt; str:\n        blob_client = self.service_client.get_blob_client(\n            container=cloud_path.container, blob=cloud_path.blob\n        )\n        return blob_client.url\n\n    def _generate_presigned_url(\n        self, cloud_path: AzureBlobPath, expire_seconds: int = 60 * 60\n    ) -&gt; str:\n        sas_token = generate_blob_sas(\n            self.service_client.account_name,\n            container_name=cloud_path.container,\n            blob_name=cloud_path.blob,\n            account_key=self.service_client.credential.account_key,\n            permission=BlobSasPermissions(read=True),\n            expiry=datetime.utcnow() + timedelta(seconds=expire_seconds),\n        )\n        url = f\"{self._get_public_url(cloud_path)}?{sas_token}\"\n        return url\n</code></pre>"},{"location":"api-reference/azblobclient/#cloudpathlib.azure.azblobclient.AzureBlobClient-methods","title":"Methods","text":""},{"location":"api-reference/azblobclient/#cloudpathlib.azure.azblobclient.AzureBlobClient.AzureBlobPath","title":"<code>AzureBlobPath(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code>","text":"Source code in <code>cloudpathlib/azure/azblobclient.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre>"},{"location":"api-reference/azblobclient/#cloudpathlib.azure.azblobclient.AzureBlobClient.__init__","title":"<code>__init__(self, account_url: Optional[str] = None, credential: Optional[Any] = None, connection_string: Optional[str] = None, blob_service_client: Optional[BlobServiceClient] = None, data_lake_client: Optional[DataLakeServiceClient] = None, file_cache_mode: Union[str, cloudpathlib.enums.FileCacheMode] = None, local_cache_dir: Union[str, os.PathLike] = None, content_type_method: Optional[Callable] = &lt;function guess_type at 0x7f08e5d20790&gt;)</code>  <code>special</code>","text":"<p>Class constructor. Sets up a <code>BlobServiceClient</code>. Supports the following authentication methods of <code>BlobServiceClient</code>.</p> <ul> <li>Environment variable <code>\"\"AZURE_STORAGE_CONNECTION_STRING\"</code> containing connecting string with account credentials. See Azure Storage SDK documentation.</li> <li>Connection string via <code>connection_string</code>, authenticated either with an embedded SAS token or with credentials passed to <code>credentials</code>.</li> <li>Account URL via <code>account_url</code>, authenticated either with an embedded SAS token, or with credentials passed to <code>credentials</code>.</li> <li>Instantiated and already authenticated <code>BlobServiceClient</code> or <code>DataLakeServiceClient</code>.</li> </ul> <p>If multiple methods are used, priority order is reverse of list above (later in list takes priority). If no methods are used, a <code>MissingCredentialsError</code> exception will be raised raised.</p> <p>Parameters:</p> Name Type Description Default <code>account_url</code> <code>Optional[str]</code> <p>The URL to the blob storage account, optionally authenticated with a SAS token. See documentation for <code>BlobServiceClient</code>.</p> <code>None</code> <code>credential</code> <code>Optional[Any]</code> <p>Credentials with which to authenticate. Can be used with <code>account_url</code> or <code>connection_string</code>, but is unnecessary if the other already has an SAS token. See documentation for <code>BlobServiceClient</code> or <code>BlobServiceClient.from_connection_string</code>.</p> <code>None</code> <code>connection_string</code> <code>Optional[str]</code> <p>A connection string to an Azure Storage account. See Azure Storage SDK documentation.</p> <code>None</code> <code>blob_service_client</code> <code>Optional[BlobServiceClient]</code> <p>Instantiated <code>BlobServiceClient</code>.</p> <code>None</code> <code>data_lake_client</code> <code>Optional[DataLakeServiceClient]</code> <p>Instantiated <code>DataLakeServiceClient</code>. If None and <code>blob_service_client</code> is passed, we will create based on that. Otherwise, will create based on passed credential, account_url, connection_string, or AZURE_STORAGE_CONNECTION_STRING env var</p> <code>None</code> <code>file_cache_mode</code> <code>Optional[Union[str, FileCacheMode]]</code> <p>How often to clear the file cache; see the caching docs for more information about the options in cloudpathlib.eums.FileCacheMode.</p> <code>None</code> <code>local_cache_dir</code> <code>Optional[Union[str, os.PathLike]]</code> <p>Path to directory to use as cache for downloaded files. If None, will use a temporary directory. Default can be set with the <code>CLOUDPATHLIB_LOCAL_CACHE_DIR</code> environment variable.</p> <code>None</code> <code>content_type_method</code> <code>Optional[Callable]</code> <p>Function to call to guess media type (mimetype) when writing a file to the cloud. Defaults to <code>mimetypes.guess_type</code>. Must return a tuple (content type, content encoding).</p> <code>&lt;function guess_type at 0x7f08e5d20790&gt;</code> Source code in <code>cloudpathlib/azure/azblobclient.py</code> <pre><code>def __init__(\n    self,\n    account_url: Optional[str] = None,\n    credential: Optional[Any] = None,\n    connection_string: Optional[str] = None,\n    blob_service_client: Optional[\"BlobServiceClient\"] = None,\n    data_lake_client: Optional[\"DataLakeServiceClient\"] = None,\n    file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n    local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n    content_type_method: Optional[Callable] = mimetypes.guess_type,\n):\n    \"\"\"Class constructor. Sets up a [`BlobServiceClient`](\n    https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python).\n    Supports the following authentication methods of `BlobServiceClient`.\n\n    - Environment variable `\"\"AZURE_STORAGE_CONNECTION_STRING\"` containing connecting string\n    with account credentials. See [Azure Storage SDK documentation](\n    https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python#copy-your-credentials-from-the-azure-portal).\n    - Connection string via `connection_string`, authenticated either with an embedded SAS\n    token or with credentials passed to `credentials`.\n    - Account URL via `account_url`, authenticated either with an embedded SAS token, or with\n    credentials passed to `credentials`.\n    - Instantiated and already authenticated [`BlobServiceClient`](\n    https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python) or\n    [`DataLakeServiceClient`](https://learn.microsoft.com/en-us/python/api/azure-storage-file-datalake/azure.storage.filedatalake.datalakeserviceclient).\n\n    If multiple methods are used, priority order is reverse of list above (later in list takes\n    priority). If no methods are used, a [`MissingCredentialsError`][cloudpathlib.exceptions.MissingCredentialsError]\n    exception will be raised raised.\n\n    Args:\n        account_url (Optional[str]): The URL to the blob storage account, optionally\n            authenticated with a SAS token. See documentation for [`BlobServiceClient`](\n            https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python).\n        credential (Optional[Any]): Credentials with which to authenticate. Can be used with\n            `account_url` or `connection_string`, but is unnecessary if the other already has\n            an SAS token. See documentation for [`BlobServiceClient`](\n            https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python)\n            or [`BlobServiceClient.from_connection_string`](\n            https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python#from-connection-string-conn-str--credential-none----kwargs-).\n        connection_string (Optional[str]): A connection string to an Azure Storage account. See\n            [Azure Storage SDK documentation](\n            https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python#copy-your-credentials-from-the-azure-portal).\n        blob_service_client (Optional[BlobServiceClient]): Instantiated [`BlobServiceClient`](\n            https://docs.microsoft.com/en-us/python/api/azure-storage-blob/azure.storage.blob.blobserviceclient?view=azure-python).\n        data_lake_client (Optional[DataLakeServiceClient]): Instantiated [`DataLakeServiceClient`](\n            https://learn.microsoft.com/en-us/python/api/azure-storage-file-datalake/azure.storage.filedatalake.datalakeserviceclient).\n            If None and `blob_service_client` is passed, we will create based on that.\n            Otherwise, will create based on passed credential, account_url, connection_string, or AZURE_STORAGE_CONNECTION_STRING env var\n        file_cache_mode (Optional[Union[str, FileCacheMode]]): How often to clear the file cache; see\n            [the caching docs](https://cloudpathlib.drivendata.org/stable/caching/) for more information\n            about the options in cloudpathlib.eums.FileCacheMode.\n        local_cache_dir (Optional[Union[str, os.PathLike]]): Path to directory to use as cache\n            for downloaded files. If None, will use a temporary directory. Default can be set with\n            the `CLOUDPATHLIB_LOCAL_CACHE_DIR` environment variable.\n        content_type_method (Optional[Callable]): Function to call to guess media type (mimetype) when\n            writing a file to the cloud. Defaults to `mimetypes.guess_type`. Must return a tuple (content type, content encoding).\n    \"\"\"\n    super().__init__(\n        local_cache_dir=local_cache_dir,\n        content_type_method=content_type_method,\n        file_cache_mode=file_cache_mode,\n    )\n\n    if connection_string is None:\n        connection_string = os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\", None)\n\n    self.data_lake_client = None  # only needs to end up being set if HNS is enabled\n\n    if blob_service_client is not None:\n        self.service_client = blob_service_client\n\n        # create from blob service client if not passed\n        if data_lake_client is None:\n            self.data_lake_client = DataLakeServiceClient(\n                account_url=self.service_client.url.replace(\".blob.\", \".dfs.\", 1),\n                credential=AzureNamedKeyCredential(\n                    blob_service_client.credential.account_name,\n                    blob_service_client.credential.account_key,\n                ),\n            )\n        else:\n            self.data_lake_client = data_lake_client\n\n    elif data_lake_client is not None:\n        self.data_lake_client = data_lake_client\n\n        if blob_service_client is None:\n            self.service_client = BlobServiceClient(\n                account_url=self.data_lake_client.url.replace(\".dfs.\", \".blob.\", 1),\n                credential=AzureNamedKeyCredential(\n                    data_lake_client.credential.account_name,\n                    data_lake_client.credential.account_key,\n                ),\n            )\n\n    elif connection_string is not None:\n        self.service_client = BlobServiceClient.from_connection_string(\n            conn_str=connection_string, credential=credential\n        )\n        self.data_lake_client = DataLakeServiceClient.from_connection_string(\n            conn_str=connection_string, credential=credential\n        )\n    elif account_url is not None:\n        if \".dfs.\" in account_url:\n            self.service_client = BlobServiceClient(\n                account_url=account_url.replace(\".dfs.\", \".blob.\"), credential=credential\n            )\n            self.data_lake_client = DataLakeServiceClient(\n                account_url=account_url, credential=credential\n            )\n        elif \".blob.\" in account_url:\n            self.service_client = BlobServiceClient(\n                account_url=account_url, credential=credential\n            )\n            self.data_lake_client = DataLakeServiceClient(\n                account_url=account_url.replace(\".blob.\", \".dfs.\"), credential=credential\n            )\n        else:\n            # assume default to blob; HNS not supported\n            self.service_client = BlobServiceClient(\n                account_url=account_url, credential=credential\n            )\n\n    else:\n        raise MissingCredentialsError(\n            \"AzureBlobClient does not support anonymous instantiation. \"\n            \"Credentials are required; see docs for options.\"\n        )\n\n    self._hns_enabled = None\n</code></pre>"},{"location":"api-reference/azblobpath/","title":"cloudpathlib.AzureBlobPath","text":"<p>Class for representing and operating on Azure Blob Storage URIs, in the style of the Python standard library's <code>pathlib</code> module. Instances represent a path in Blob Storage with filesystem path semantics, and convenient methods allow for basic operations like joining, reading, writing, iterating over contents, etc. This class almost entirely mimics the <code>pathlib.Path</code> interface, so most familiar properties and methods should be available and behave in the expected way.</p> <p>The <code>AzureBlobClient</code> class handles authentication with Azure. If a client instance is not explicitly specified on <code>AzureBlobPath</code> instantiation, a default client is used. See <code>AzureBlobClient</code>'s documentation for more details.</p> Source code in <code>cloudpathlib/azure/azblobpath.py</code> <pre><code>class AzureBlobPath(CloudPath):\n    \"\"\"Class for representing and operating on Azure Blob Storage URIs, in the style of the Python\n    standard library's [`pathlib` module](https://docs.python.org/3/library/pathlib.html).\n    Instances represent a path in Blob Storage with filesystem path semantics, and convenient\n    methods allow for basic operations like joining, reading, writing, iterating over contents,\n    etc. This class almost entirely mimics the [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)\n    interface, so most familiar properties and methods should be available and behave in the\n    expected way.\n\n    The [`AzureBlobClient`](../azblobclient/) class handles authentication with Azure. If a\n    client instance is not explicitly specified on `AzureBlobPath` instantiation, a default client\n    is used. See `AzureBlobClient`'s documentation for more details.\n    \"\"\"\n\n    cloud_prefix: str = \"az://\"\n    client: \"AzureBlobClient\"\n\n    @property\n    def drive(self) -&gt; str:\n        return self.container\n\n    def is_dir(self) -&gt; bool:\n        return self.client._is_file_or_dir(self) == \"dir\"\n\n    def is_file(self) -&gt; bool:\n        return self.client._is_file_or_dir(self) == \"file\"\n\n    def mkdir(self, parents=False, exist_ok=False):\n        self.client._mkdir(self, parents=parents, exist_ok=exist_ok)\n\n    def touch(self, exist_ok: bool = True):\n        if self.exists():\n            if not exist_ok:\n                raise FileExistsError(f\"File exists: {self}\")\n            self.client._move_file(self, self)\n        else:\n            tf = TemporaryDirectory()\n            p = Path(tf.name) / \"empty\"\n            p.touch()\n\n            self.client._upload_file(p, self)\n\n            tf.cleanup()\n\n    def stat(self):\n        try:\n            meta = self.client._get_metadata(self)\n        except ResourceNotFoundError:\n            raise NoStatError(\n                f\"No stats available for {self}; it may be a directory or not exist.\"\n            )\n\n        return os.stat_result(\n            (\n                None,  # mode\n                None,  # ino\n                self.cloud_prefix,  # dev,\n                None,  # nlink,\n                None,  # uid,\n                None,  # gid,\n                meta.get(\"size\", 0),  # size,\n                None,  # atime,\n                meta.get(\"last_modified\", 0).timestamp(),  # mtime,\n                None,  # ctime,\n            )\n        )\n\n    def replace(self, target: \"AzureBlobPath\") -&gt; \"AzureBlobPath\":\n        try:\n            return super().replace(target)\n\n        # we can rename directories on ADLS Gen2\n        except CloudPathIsADirectoryError:\n            if self.client._check_hns():\n                return self.client._move_file(self, target)\n            else:\n                raise\n\n    @property\n    def container(self) -&gt; str:\n        return self._no_prefix.split(\"/\", 1)[0]\n\n    @property\n    def blob(self) -&gt; str:\n        key = self._no_prefix_no_drive\n\n        # key should never have starting slash for\n        if key.startswith(\"/\"):\n            key = key[1:]\n\n        return key\n\n    @property\n    def etag(self):\n        return self.client._get_metadata(self).get(\"etag\", None)\n\n    @property\n    def md5(self) -&gt; str:\n        return self.client._get_metadata(self).get(\"content_settings\", {}).get(\"content_md5\", None)\n</code></pre>"},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath-attributes","title":"Attributes","text":""},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.blob","title":"<code>blob: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.cloud_prefix","title":"<code>cloud_prefix: str</code>","text":""},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.container","title":"<code>container: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.drive","title":"<code>drive: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.etag","title":"<code>etag</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.md5","title":"<code>md5: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath-methods","title":"Methods","text":""},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.is_dir","title":"<code>is_dir(self) -&gt; bool</code>","text":"<p>Whether this path is a directory.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/azure/azblobpath.py</code> <pre><code>def is_dir(self) -&gt; bool:\n    return self.client._is_file_or_dir(self) == \"dir\"\n</code></pre>"},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.is_file","title":"<code>is_file(self) -&gt; bool</code>","text":"<p>Whether this path is a regular file (also True for symlinks pointing to regular files).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/azure/azblobpath.py</code> <pre><code>def is_file(self) -&gt; bool:\n    return self.client._is_file_or_dir(self) == \"file\"\n</code></pre>"},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.mkdir","title":"<code>mkdir(self, parents = False, exist_ok = False)</code>","text":"<p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/azure/azblobpath.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    self.client._mkdir(self, parents=parents, exist_ok=exist_ok)\n</code></pre>"},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.replace","title":"<code>replace(self, target: AzureBlobPath) -&gt; AzureBlobPath</code>","text":"<p>Rename this path to the target path, overwriting if that path exists.</p> <p>The target path may be absolute or relative. Relative paths are interpreted relative to the current working directory, not the directory of the Path object.</p> <p>Returns the new Path instance pointing to the target path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/azure/azblobpath.py</code> <pre><code>def replace(self, target: \"AzureBlobPath\") -&gt; \"AzureBlobPath\":\n    try:\n        return super().replace(target)\n\n    # we can rename directories on ADLS Gen2\n    except CloudPathIsADirectoryError:\n        if self.client._check_hns():\n            return self.client._move_file(self, target)\n        else:\n            raise\n</code></pre>"},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.stat","title":"<code>stat(self)</code>","text":"<p>Return the result of the stat() system call on this path, like os.stat() does.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/azure/azblobpath.py</code> <pre><code>def stat(self):\n    try:\n        meta = self.client._get_metadata(self)\n    except ResourceNotFoundError:\n        raise NoStatError(\n            f\"No stats available for {self}; it may be a directory or not exist.\"\n        )\n\n    return os.stat_result(\n        (\n            None,  # mode\n            None,  # ino\n            self.cloud_prefix,  # dev,\n            None,  # nlink,\n            None,  # uid,\n            None,  # gid,\n            meta.get(\"size\", 0),  # size,\n            None,  # atime,\n            meta.get(\"last_modified\", 0).timestamp(),  # mtime,\n            None,  # ctime,\n        )\n    )\n</code></pre>"},{"location":"api-reference/azblobpath/#cloudpathlib.azure.azblobpath.AzureBlobPath.touch","title":"<code>touch(self, exist_ok: bool = True)</code>","text":"<p>Create this file with the given access mode, if it doesn't exist.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/azure/azblobpath.py</code> <pre><code>def touch(self, exist_ok: bool = True):\n    if self.exists():\n        if not exist_ok:\n            raise FileExistsError(f\"File exists: {self}\")\n        self.client._move_file(self, self)\n    else:\n        tf = TemporaryDirectory()\n        p = Path(tf.name) / \"empty\"\n        p.touch()\n\n        self.client._upload_file(p, self)\n\n        tf.cleanup()\n</code></pre>"},{"location":"api-reference/client/","title":"cloudpathlib.client","text":""},{"location":"api-reference/client/#cloudpathlib.client.BoundedCloudPath","title":"<code>BoundedCloudPath</code>","text":""},{"location":"api-reference/client/#cloudpathlib.client-classes","title":"Classes","text":""},{"location":"api-reference/client/#cloudpathlib.client.Client","title":"<code> Client            (ABC, Generic)         </code>","text":"Source code in <code>cloudpathlib/client.py</code> <pre><code>class Client(abc.ABC, Generic[BoundedCloudPath]):\n    _cloud_meta: CloudImplementation\n    _default_client = None\n\n    def __init__(\n        self,\n        file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n        local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n        content_type_method: Optional[Callable] = mimetypes.guess_type,\n    ):\n        self.file_cache_mode = None\n        self._cache_tmp_dir = None\n        self._cloud_meta.validate_completeness()\n\n        # convert strings passed to enum\n        if isinstance(file_cache_mode, str):\n            file_cache_mode = FileCacheMode(file_cache_mode)\n\n        # if not explcitly passed to client, get from env var\n        if file_cache_mode is None:\n            file_cache_mode = FileCacheMode.from_environment()\n\n        if local_cache_dir is None:\n            local_cache_dir = os.environ.get(\"CLOUDPATHLIB_LOCAL_CACHE_DIR\", None)\n\n            # treat empty string as None to avoid writing cache in cwd; set to \".\" for cwd\n            if local_cache_dir == \"\":\n                local_cache_dir = None\n\n        # explicitly passing a cache dir, so we set to persistent\n        # unless user explicitly passes a different file cache mode\n        if local_cache_dir and file_cache_mode is None:\n            file_cache_mode = FileCacheMode.persistent\n\n        if file_cache_mode == FileCacheMode.persistent and local_cache_dir is None:\n            raise InvalidConfigurationException(\n                f\"If you use the '{FileCacheMode.persistent}' cache mode, you must pass a `local_cache_dir` when you instantiate the client.\"\n            )\n\n        # if no explicit local dir, setup caching in temporary dir\n        if local_cache_dir is None:\n            self._cache_tmp_dir = TemporaryDirectory()\n            local_cache_dir = self._cache_tmp_dir.name\n\n            if file_cache_mode is None:\n                file_cache_mode = FileCacheMode.tmp_dir\n\n        self._local_cache_dir = Path(local_cache_dir)\n        self.content_type_method = content_type_method\n\n        # Fallback: if not set anywhere, default to tmp_dir (for backwards compatibility)\n        if file_cache_mode is None:\n            file_cache_mode = FileCacheMode.tmp_dir\n\n        self.file_cache_mode = file_cache_mode\n\n    def __del__(self) -&gt; None:\n        # remove containing dir, even if a more aggressive strategy\n        # removed the actual files\n        if getattr(self, \"file_cache_mode\", None) in [\n            FileCacheMode.tmp_dir,\n            FileCacheMode.close_file,\n            FileCacheMode.cloudpath_object,\n        ]:\n            self.clear_cache()\n\n            if self._local_cache_dir.exists():\n                self._local_cache_dir.rmdir()\n\n    @classmethod\n    def get_default_client(cls) -&gt; \"Client\":\n        \"\"\"Get the default client, which the one that is used when instantiating a cloud path\n        instance for this cloud without a client specified.\n        \"\"\"\n        if cls._default_client is None:\n            cls._default_client = cls()\n        return cls._default_client\n\n    def set_as_default_client(self) -&gt; None:\n        \"\"\"Set this client instance as the default one used when instantiating cloud path\n        instances for this cloud without a client specified.\"\"\"\n        self.__class__._default_client = self\n\n    def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n        return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n\n    def clear_cache(self):\n        \"\"\"Clears the contents of the cache folder.\n        Does not remove folder so it can keep being written to.\n        \"\"\"\n        if self._local_cache_dir.exists():\n            for p in self._local_cache_dir.iterdir():\n                if p.is_file():\n                    p.unlink()\n                else:\n                    shutil.rmtree(p)\n\n    @abc.abstractmethod\n    def _download_file(\n        self, cloud_path: BoundedCloudPath, local_path: Union[str, os.PathLike]\n    ) -&gt; Path:\n        pass\n\n    @abc.abstractmethod\n    def _exists(self, cloud_path: BoundedCloudPath) -&gt; bool:\n        pass\n\n    @abc.abstractmethod\n    def _list_dir(\n        self, cloud_path: BoundedCloudPath, recursive: bool\n    ) -&gt; Iterable[Tuple[BoundedCloudPath, bool]]:\n        \"\"\"List all the files and folders in a directory.\n\n        Parameters\n        ----------\n        cloud_path : CloudPath\n            The folder to start from.\n        recursive : bool\n            Whether or not to list recursively.\n\n        Returns\n        -------\n        contents : Iterable[Tuple]\n            Of the form [(CloudPath, is_dir), ...] for every child of the dir.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def _move_file(\n        self, src: BoundedCloudPath, dst: BoundedCloudPath, remove_src: bool = True\n    ) -&gt; BoundedCloudPath:\n        pass\n\n    @abc.abstractmethod\n    def _remove(self, path: BoundedCloudPath, missing_ok: bool = True) -&gt; None:\n        \"\"\"Remove a file or folder from the server.\n\n        Parameters\n        ----------\n        path : CloudPath\n            The file or folder to remove.\n        \"\"\"\n        pass\n\n    @abc.abstractmethod\n    def _upload_file(\n        self, local_path: Union[str, os.PathLike], cloud_path: BoundedCloudPath\n    ) -&gt; BoundedCloudPath:\n        pass\n\n    @abc.abstractmethod\n    def _get_public_url(self, cloud_path: BoundedCloudPath) -&gt; str:\n        pass\n\n    @abc.abstractmethod\n    def _generate_presigned_url(\n        self, cloud_path: BoundedCloudPath, expire_seconds: int = 60 * 60\n    ) -&gt; str:\n        pass\n</code></pre>"},{"location":"api-reference/client/#cloudpathlib.client.Client-methods","title":"Methods","text":""},{"location":"api-reference/client/#cloudpathlib.client.Client.CloudPath","title":"<code>CloudPath(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code>","text":"Source code in <code>cloudpathlib/client.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre>"},{"location":"api-reference/client/#cloudpathlib.client.Client.__init__","title":"<code>__init__(self, file_cache_mode: Union[str, cloudpathlib.enums.FileCacheMode] = None, local_cache_dir: Union[str, os.PathLike] = None, content_type_method: Optional[Callable] = &lt;function guess_type at 0x7f08e5d20790&gt;)</code>  <code>special</code>","text":"Source code in <code>cloudpathlib/client.py</code> <pre><code>def __init__(\n    self,\n    file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n    local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n    content_type_method: Optional[Callable] = mimetypes.guess_type,\n):\n    self.file_cache_mode = None\n    self._cache_tmp_dir = None\n    self._cloud_meta.validate_completeness()\n\n    # convert strings passed to enum\n    if isinstance(file_cache_mode, str):\n        file_cache_mode = FileCacheMode(file_cache_mode)\n\n    # if not explcitly passed to client, get from env var\n    if file_cache_mode is None:\n        file_cache_mode = FileCacheMode.from_environment()\n\n    if local_cache_dir is None:\n        local_cache_dir = os.environ.get(\"CLOUDPATHLIB_LOCAL_CACHE_DIR\", None)\n\n        # treat empty string as None to avoid writing cache in cwd; set to \".\" for cwd\n        if local_cache_dir == \"\":\n            local_cache_dir = None\n\n    # explicitly passing a cache dir, so we set to persistent\n    # unless user explicitly passes a different file cache mode\n    if local_cache_dir and file_cache_mode is None:\n        file_cache_mode = FileCacheMode.persistent\n\n    if file_cache_mode == FileCacheMode.persistent and local_cache_dir is None:\n        raise InvalidConfigurationException(\n            f\"If you use the '{FileCacheMode.persistent}' cache mode, you must pass a `local_cache_dir` when you instantiate the client.\"\n        )\n\n    # if no explicit local dir, setup caching in temporary dir\n    if local_cache_dir is None:\n        self._cache_tmp_dir = TemporaryDirectory()\n        local_cache_dir = self._cache_tmp_dir.name\n\n        if file_cache_mode is None:\n            file_cache_mode = FileCacheMode.tmp_dir\n\n    self._local_cache_dir = Path(local_cache_dir)\n    self.content_type_method = content_type_method\n\n    # Fallback: if not set anywhere, default to tmp_dir (for backwards compatibility)\n    if file_cache_mode is None:\n        file_cache_mode = FileCacheMode.tmp_dir\n\n    self.file_cache_mode = file_cache_mode\n</code></pre>"},{"location":"api-reference/client/#cloudpathlib.client.Client.clear_cache","title":"<code>clear_cache(self)</code>","text":"<p>Clears the contents of the cache folder. Does not remove folder so it can keep being written to.</p> Source code in <code>cloudpathlib/client.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Clears the contents of the cache folder.\n    Does not remove folder so it can keep being written to.\n    \"\"\"\n    if self._local_cache_dir.exists():\n        for p in self._local_cache_dir.iterdir():\n            if p.is_file():\n                p.unlink()\n            else:\n                shutil.rmtree(p)\n</code></pre>"},{"location":"api-reference/client/#cloudpathlib.client.Client.get_default_client","title":"<code>get_default_client() -&gt; Client</code>  <code>classmethod</code>","text":"<p>Get the default client, which the one that is used when instantiating a cloud path instance for this cloud without a client specified.</p> Source code in <code>cloudpathlib/client.py</code> <pre><code>@classmethod\ndef get_default_client(cls) -&gt; \"Client\":\n    \"\"\"Get the default client, which the one that is used when instantiating a cloud path\n    instance for this cloud without a client specified.\n    \"\"\"\n    if cls._default_client is None:\n        cls._default_client = cls()\n    return cls._default_client\n</code></pre>"},{"location":"api-reference/client/#cloudpathlib.client.Client.set_as_default_client","title":"<code>set_as_default_client(self) -&gt; None</code>","text":"<p>Set this client instance as the default one used when instantiating cloud path instances for this cloud without a client specified.</p> Source code in <code>cloudpathlib/client.py</code> <pre><code>def set_as_default_client(self) -&gt; None:\n    \"\"\"Set this client instance as the default one used when instantiating cloud path\n    instances for this cloud without a client specified.\"\"\"\n    self.__class__._default_client = self\n</code></pre>"},{"location":"api-reference/client/#cloudpathlib.client.register_client_class","title":"<code>register_client_class(key: str) -&gt; Callable</code>","text":"Source code in <code>cloudpathlib/client.py</code> <pre><code>def register_client_class(key: str) -&gt; Callable:\n    def decorator(cls: type) -&gt; type:\n        if not issubclass(cls, Client):\n            raise TypeError(\"Only subclasses of Client can be registered.\")\n        implementation_registry[key]._client_class = cls\n        implementation_registry[key].name = key\n        cls._cloud_meta = implementation_registry[key]\n        return cls\n\n    return decorator\n</code></pre>"},{"location":"api-reference/cloudpath/","title":"cloudpathlib.CloudPath","text":"<p>Base class for cloud storage file URIs, in the style of the Python standard library's <code>pathlib</code> module. Instances represent a path in cloud storage with filesystem path semantics, and convenient methods allow for basic operations like joining, reading, writing, iterating over contents, etc. <code>CloudPath</code> almost entirely mimics the <code>pathlib.Path</code> interface, so most familiar properties and methods should be available and behave in the expected way.</p> <p>Analogous to the way <code>pathlib.Path</code> works, instantiating <code>CloudPath</code> will instead create an instance of an appropriate subclass that implements a particular cloud storage service, such as <code>S3Path</code>. This dispatching behavior is based on the URI scheme part of a cloud storage URI (e.g., <code>\"s3://\"</code>).</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>class CloudPath(metaclass=CloudPathMeta):\n    \"\"\"Base class for cloud storage file URIs, in the style of the Python standard library's\n    [`pathlib` module](https://docs.python.org/3/library/pathlib.html). Instances represent a path\n    in cloud storage with filesystem path semantics, and convenient methods allow for basic\n    operations like joining, reading, writing, iterating over contents, etc. `CloudPath` almost\n    entirely mimics the [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)\n    interface, so most familiar properties and methods should be available and behave in the\n    expected way.\n\n    Analogous to the way `pathlib.Path` works, instantiating `CloudPath` will instead create an\n    instance of an appropriate subclass that implements a particular cloud storage service, such as\n    [`S3Path`](../s3path). This dispatching behavior is based on the URI scheme part of a cloud\n    storage URI (e.g., `\"s3://\"`).\n    \"\"\"\n\n    _cloud_meta: CloudImplementation\n    cloud_prefix: str\n\n    def __init__(\n        self,\n        cloud_path: Union[str, Self, \"CloudPath\"],\n        client: Optional[\"Client\"] = None,\n    ) -&gt; None:\n        # handle if local file gets opened. must be set at the top of the method in case any code\n        # below raises an exception, this prevents __del__ from raising an AttributeError\n        self._handle: Optional[IO] = None\n        self._client: Optional[\"Client\"] = None\n\n        self.is_valid_cloudpath(cloud_path, raise_on_error=True)\n        self._cloud_meta.validate_completeness()\n\n        # versions of the raw string that provide useful methods\n        self._str = str(cloud_path)\n        self._url = urlparse(self._str)\n        self._path = PurePosixPath(f\"/{self._no_prefix}\")\n\n        # setup client\n        if client is None:\n            if isinstance(cloud_path, CloudPath):\n                self._client = cloud_path.client\n        else:\n            self._client = client\n\n        if client is not None and not isinstance(client, self._cloud_meta.client_class):\n            raise ClientMismatchError(\n                f\"Client of type [{client.__class__}] is not valid for cloud path of type \"\n                f\"[{self.__class__}]; must be instance of [{self._cloud_meta.client_class}], or \"\n                f\"None to use default client for this cloud path class.\"\n            )\n\n        # track if local has been written to, if so it may need to be uploaded\n        self._dirty = False\n\n    @property\n    def client(self):\n        if getattr(self, \"_client\", None) is None:\n            self._client = self._cloud_meta.client_class.get_default_client()\n\n        return self._client\n\n    def __del__(self) -&gt; None:\n        # make sure that file handle to local path is closed\n        if self._handle is not None and self._local.exists():\n            self._handle.close()\n\n        # ensure file removed from cache when cloudpath object deleted\n        client = getattr(self, \"_client\", None)\n        if getattr(client, \"file_cache_mode\", None) == FileCacheMode.cloudpath_object:\n            self.clear_cache()\n\n    def __getstate__(self) -&gt; Dict[str, Any]:\n        state = self.__dict__.copy()\n\n        # don't pickle client\n        if \"_client\" in state:\n            del state[\"_client\"]\n\n        return state\n\n    def __setstate__(self, state: Dict[str, Any]) -&gt; None:\n        self.__dict__.update(state)\n\n    @property\n    def _no_prefix(self) -&gt; str:\n        return self._str[len(self.cloud_prefix) :]\n\n    @property\n    def _no_prefix_no_drive(self) -&gt; str:\n        return self._str[len(self.cloud_prefix) + len(self.drive) :]\n\n    @overload\n    @classmethod\n    def is_valid_cloudpath(\n        cls, path: \"CloudPath\", raise_on_error: bool = ...\n    ) -&gt; TypeGuard[Self]: ...\n\n    @overload\n    @classmethod\n    def is_valid_cloudpath(cls, path: str, raise_on_error: bool = ...) -&gt; bool: ...\n\n    @classmethod\n    def is_valid_cloudpath(\n        cls, path: Union[str, \"CloudPath\"], raise_on_error: bool = False\n    ) -&gt; Union[bool, TypeGuard[Self]]:\n        valid = str(path).lower().startswith(cls.cloud_prefix.lower())\n\n        if raise_on_error and not valid:\n            raise InvalidPrefixError(\n                f\"'{path}' is not a valid path since it does not start with '{cls.cloud_prefix}'\"\n            )\n\n        return valid\n\n    def __repr__(self) -&gt; str:\n        return f\"{self.__class__.__name__}('{self}')\"\n\n    def __str__(self) -&gt; str:\n        return self._str\n\n    def __hash__(self) -&gt; int:\n        return hash((type(self).__name__, str(self)))\n\n    def __eq__(self, other: Any) -&gt; bool:\n        return isinstance(other, type(self)) and str(self) == str(other)\n\n    def __fspath__(self) -&gt; str:\n        if self.is_file():\n            self._refresh_cache()\n        return str(self._local)\n\n    def __lt__(self, other: Any) -&gt; bool:\n        if not isinstance(other, type(self)):\n            return NotImplemented\n        return self.parts &lt; other.parts\n\n    def __le__(self, other: Any) -&gt; bool:\n        if not isinstance(other, type(self)):\n            return NotImplemented\n        return self.parts &lt;= other.parts\n\n    def __gt__(self, other: Any) -&gt; bool:\n        if not isinstance(other, type(self)):\n            return NotImplemented\n        return self.parts &gt; other.parts\n\n    def __ge__(self, other: Any) -&gt; bool:\n        if not isinstance(other, type(self)):\n            return NotImplemented\n        return self.parts &gt;= other.parts\n\n    # ====================== NOT IMPLEMENTED ======================\n    # as_posix - no cloud equivalent; not needed since we assume url separator\n    # chmod - permission changing should be explicitly done per client with methods\n    #           that make sense for the client permission options\n    # cwd - no cloud equivalent\n    # expanduser - no cloud equivalent\n    # group - should be implemented with client-specific permissions\n    # home - no cloud equivalent\n    # is_block_device - no cloud equivalent\n    # is_char_device - no cloud equivalent\n    # is_fifo - no cloud equivalent\n    # is_mount - no cloud equivalent\n    # is_reserved - no cloud equivalent\n    # is_socket - no cloud equivalent\n    # is_symlink - no cloud equivalent\n    # lchmod - no cloud equivalent\n    # lstat - no cloud equivalent\n    # owner - no cloud equivalent\n    # root - drive already has the bucket and anchor/prefix has the scheme, so nothing to store here\n    # symlink_to - no cloud equivalent\n    # link_to - no cloud equivalent\n    # hardlink_to - no cloud equivalent\n\n    # ====================== REQUIRED, NOT GENERIC ======================\n    # Methods that must be implemented, but have no generic application\n    @property\n    @abc.abstractmethod\n    def drive(self) -&gt; str:\n        \"\"\"For example \"bucket\" on S3 or \"container\" on Azure; needs to be defined for each class\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def is_dir(self) -&gt; bool:\n        \"\"\"Should be implemented without requiring a dir is downloaded\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def is_file(self) -&gt; bool:\n        \"\"\"Should be implemented without requiring that the file is downloaded\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def mkdir(self, parents: bool = False, exist_ok: bool = False) -&gt; None:\n        \"\"\"Should be implemented using the client API without requiring a dir is downloaded\"\"\"\n        pass\n\n    @abc.abstractmethod\n    def touch(self, exist_ok: bool = True) -&gt; None:\n        \"\"\"Should be implemented using the client API to create and update modified time\"\"\"\n        pass\n\n    def as_url(self, presign: bool = False, expire_seconds: int = 60 * 60) -&gt; str:\n        if presign:\n            url = self.client._generate_presigned_url(self, expire_seconds=expire_seconds)\n        else:\n            url = self.client._get_public_url(self)\n        return url\n\n    # ====================== IMPLEMENTED FROM SCRATCH ======================\n    # Methods with their own implementations that work generically\n    def __rtruediv__(self, other: Any) -&gt; None:\n        raise ValueError(\n            \"Cannot change a cloud path's root since all paths are absolute; create a new path instead.\"\n        )\n\n    @property\n    def anchor(self) -&gt; str:\n        return self.cloud_prefix\n\n    def as_uri(self) -&gt; str:\n        return str(self)\n\n    def exists(self) -&gt; bool:\n        return self.client._exists(self)\n\n    @property\n    def fspath(self) -&gt; str:\n        return self.__fspath__()\n\n    def _glob_checks(self, pattern: str) -&gt; None:\n        if \"..\" in pattern:\n            raise CloudPathNotImplementedError(\n                \"Relative paths with '..' not supported in glob patterns.\"\n            )\n\n        if pattern.startswith(self.cloud_prefix) or pattern.startswith(\"/\"):\n            raise CloudPathNotImplementedError(\"Non-relative patterns are unsupported\")\n\n        if self.drive == \"\":\n            raise CloudPathNotImplementedError(\n                \".glob is only supported within a bucket or container; you can use `.iterdir` to list buckets; for example, CloudPath('s3://').iterdir()\"\n            )\n\n    def _build_subtree(self, recursive):\n        # build a tree structure for all files out of default dicts\n        Tree: Callable = lambda: defaultdict(Tree)\n\n        def _build_tree(trunk, branch, nodes, is_dir):\n            \"\"\"Utility to build a tree from nested defaultdicts with a generator\n            of nodes (parts) of a path.\"\"\"\n            next_branch = next(nodes, None)\n\n            if next_branch is None:\n                trunk[branch] = Tree() if is_dir else None  # leaf node\n\n            else:\n                _build_tree(trunk[branch], next_branch, nodes, is_dir)\n\n        file_tree = Tree()\n\n        for f, is_dir in self.client._list_dir(self, recursive=recursive):\n            parts = str(f.relative_to(self)).split(\"/\")\n\n            # skip self\n            if len(parts) == 1 and parts[0] == \".\":\n                continue\n\n            nodes = (p for p in parts)\n            _build_tree(file_tree, next(nodes, None), nodes, is_dir)\n\n        return dict(file_tree)  # freeze as normal dict before passing in\n\n    def _glob(self, selector, recursive: bool) -&gt; Generator[Self, None, None]:\n        file_tree = self._build_subtree(recursive)\n\n        root = _CloudPathSelectable(\n            self.name,\n            [],  # nothing above self will be returned, so initial parents is empty\n            file_tree,\n        )\n\n        for p in selector.select_from(root):\n            # select_from returns self.name/... so strip before joining\n            yield (self / str(p)[len(self.name) + 1 :])\n\n    def glob(\n        self, pattern: str, case_sensitive: Optional[bool] = None\n    ) -&gt; Generator[Self, None, None]:\n        self._glob_checks(pattern)\n\n        pattern_parts = PurePosixPath(pattern).parts\n        selector = _make_selector(\n            tuple(pattern_parts), _posix_flavour, case_sensitive=case_sensitive\n        )\n\n        yield from self._glob(\n            selector,\n            \"/\" in pattern\n            or \"**\"\n            in pattern,  # recursive listing needed if explicit ** or any sub folder in pattern\n        )\n\n    def rglob(\n        self, pattern: str, case_sensitive: Optional[bool] = None\n    ) -&gt; Generator[Self, None, None]:\n        self._glob_checks(pattern)\n\n        pattern_parts = PurePosixPath(pattern).parts\n        selector = _make_selector(\n            (\"**\",) + tuple(pattern_parts), _posix_flavour, case_sensitive=case_sensitive\n        )\n\n        yield from self._glob(selector, True)\n\n    def iterdir(self) -&gt; Generator[Self, None, None]:\n        for f, _ in self.client._list_dir(self, recursive=False):\n            if f != self:  # iterdir does not include itself in pathlib\n                yield f\n\n    @staticmethod\n    def _walk_results_from_tree(root, tree, top_down=True):\n        \"\"\"Utility to yield tuples in the form expected by `.walk` from the file\n        tree constructed by `_build_substree`.\n        \"\"\"\n        dirs = []\n        files = []\n        for item, branch in tree.items():\n            files.append(item) if branch is None else dirs.append(item)\n\n        if top_down:\n            yield root, dirs, files\n\n        for dir in dirs:\n            yield from CloudPath._walk_results_from_tree(root / dir, tree[dir], top_down=top_down)\n\n        if not top_down:\n            yield root, dirs, files\n\n    def walk(\n        self,\n        top_down: bool = True,\n        on_error: Optional[Callable] = None,\n        follow_symlinks: bool = False,\n    ) -&gt; Generator[Tuple[Self, List[str], List[str]], None, None]:\n        try:\n            file_tree = self._build_subtree(recursive=True)  # walking is always recursive\n            yield from self._walk_results_from_tree(self, file_tree, top_down=top_down)\n\n        except Exception as e:\n            if on_error is not None:\n                on_error(e)\n            else:\n                raise\n\n    def open(\n        self,\n        mode: str = \"r\",\n        buffering: int = -1,\n        encoding: Optional[str] = None,\n        errors: Optional[str] = None,\n        newline: Optional[str] = None,\n        force_overwrite_from_cloud: Optional[bool] = None,  # extra kwarg not in pathlib\n        force_overwrite_to_cloud: Optional[bool] = None,  # extra kwarg not in pathlib\n    ) -&gt; IO[Any]:\n        # if trying to call open on a directory that exists\n        if self.exists() and not self.is_file():\n            raise CloudPathIsADirectoryError(\n                f\"Cannot open directory, only files. Tried to open ({self})\"\n            )\n\n        if mode == \"x\" and self.exists():\n            raise CloudPathFileExistsError(f\"Cannot open existing file ({self}) for creation.\")\n\n        # TODO: consider streaming from client rather than DLing entire file to cache\n        self._refresh_cache(force_overwrite_from_cloud=force_overwrite_from_cloud)\n\n        # create any directories that may be needed if the file is new\n        if not self._local.exists():\n            self._local.parent.mkdir(parents=True, exist_ok=True)\n            original_mtime = 0.0\n        else:\n            original_mtime = self._local.stat().st_mtime\n\n        buffer = self._local.open(\n            mode=mode,\n            buffering=buffering,\n            encoding=encoding,\n            errors=errors,\n            newline=newline,\n        )\n\n        # write modes need special on closing the buffer\n        if any(m in mode for m in (\"w\", \"+\", \"x\", \"a\")):\n            # dirty, handle, patch close\n            wrapped_close = buffer.close\n\n            # since we are pretending this is a cloud file, upload it to the cloud\n            # when the buffer is closed\n            def _patched_close_upload(*args, **kwargs) -&gt; None:\n                wrapped_close(*args, **kwargs)\n\n                # we should be idempotent and not upload again if\n                # we already ran our close method patch\n                if not self._dirty:\n                    return\n\n                # original mtime should match what was in the cloud; because of system clocks or rounding\n                # by the cloud provider, the new version in our cache is \"older\" than the original version;\n                # explicitly set the new modified time to be after the original modified time.\n                if self._local.stat().st_mtime &lt; original_mtime:\n                    new_mtime = original_mtime + 1\n                    os.utime(self._local, times=(new_mtime, new_mtime))\n\n                self._upload_local_to_cloud(force_overwrite_to_cloud=force_overwrite_to_cloud)\n                self._dirty = False\n\n            buffer.close = _patched_close_upload  # type: ignore\n\n            # keep reference in case we need to close when __del__ is called on this object\n            self._handle = buffer\n\n            # opened for write, so mark dirty\n            self._dirty = True\n\n        # if we don't want any cache around, remove the cache\n        # as soon as the file is closed\n        if self.client.file_cache_mode == FileCacheMode.close_file:\n            # this may be _patched_close_upload, in which case we need to\n            # make sure to call that first so the file gets uploaded\n            wrapped_close_for_cache = buffer.close\n\n            def _patched_close_empty_cache(*args, **kwargs):\n                wrapped_close_for_cache(*args, **kwargs)\n\n                # remove local file as last step on closing\n                self.clear_cache()\n\n            buffer.close = _patched_close_empty_cache  # type: ignore\n\n        return buffer\n\n    def replace(self, target: Self) -&gt; Self:\n        if type(self) is not type(target):\n            raise TypeError(\n                f\"The target based to rename must be an instantiated class of type: {type(self)}\"\n            )\n\n        if self.is_dir():\n            raise CloudPathIsADirectoryError(\n                f\"Path {self} is a directory; rename/replace the files recursively.\"\n            )\n\n        if target == self:\n            # Request is to replace/rename this with the same path - nothing to do\n            return self\n\n        if target.exists():\n            target.unlink()\n\n        self.client._move_file(self, target)\n        return target\n\n    def rename(self, target: Self) -&gt; Self:\n        # for cloud services replace == rename since we don't just rename,\n        # we actually move files\n        return self.replace(target)\n\n    def rmdir(self) -&gt; None:\n        if self.is_file():\n            raise CloudPathNotADirectoryError(\n                f\"Path {self} is a file; call unlink instead of rmdir.\"\n            )\n        try:\n            next(self.iterdir())\n            raise DirectoryNotEmptyError(\n                f\"Directory not empty: '{self}'. Use rmtree to delete recursively.\"\n            )\n        except StopIteration:\n            pass\n        self.client._remove(self)\n\n    def samefile(self, other_path: Union[str, os.PathLike]) -&gt; bool:\n        # all cloud paths are absolute and the paths are used for hash\n        return self == other_path\n\n    def unlink(self, missing_ok: bool = True) -&gt; None:\n        # Note: missing_ok defaults to False in pathlib, but changing the default now would be a breaking change.\n        if self.is_dir():\n            raise CloudPathIsADirectoryError(\n                f\"Path {self} is a directory; call rmdir instead of unlink.\"\n            )\n        self.client._remove(self, missing_ok)\n\n    def write_bytes(self, data: bytes) -&gt; int:\n        \"\"\"Open the file in bytes mode, write to it, and close the file.\n\n        NOTE: vendored from pathlib since we override open\n        https://github.com/python/cpython/blob/3.8/Lib/pathlib.py#L1235-L1242\n        \"\"\"\n        # type-check for the buffer interface before truncating the file\n        view = memoryview(data)\n        with self.open(mode=\"wb\") as f:\n            return f.write(view)\n\n    def write_text(\n        self,\n        data: str,\n        encoding: Optional[str] = None,\n        errors: Optional[str] = None,\n        newline: Optional[str] = None,\n    ) -&gt; int:\n        \"\"\"Open the file in text mode, write to it, and close the file.\n\n        NOTE: vendored from pathlib since we override open\n        https://github.com/python/cpython/blob/3.10/Lib/pathlib.py#L1146-L1155\n        \"\"\"\n        if not isinstance(data, str):\n            raise TypeError(\"data must be str, not %s\" % data.__class__.__name__)\n\n        with self.open(mode=\"w\", encoding=encoding, errors=errors, newline=newline) as f:\n            return f.write(data)\n\n    def read_bytes(self) -&gt; bytes:\n        with self.open(mode=\"rb\") as f:\n            return f.read()\n\n    def read_text(self, encoding: Optional[str] = None, errors: Optional[str] = None) -&gt; str:\n        with self.open(mode=\"r\", encoding=encoding, errors=errors) as f:\n            return f.read()\n\n    def is_junction(self):\n        return False  # only windows paths can be junctions, not cloudpaths\n\n    # ====================== DISPATCHED TO POSIXPATH FOR PURE PATHS ======================\n    # Methods that are dispatched to exactly how pathlib.PurePosixPath would calculate it on\n    # self._path for pure paths (does not matter if file exists);\n    # see the next session for ones that require a real file to exist\n    def _dispatch_to_path(self, func: str, *args, **kwargs) -&gt; Any:\n        \"\"\"Some functions we can just dispatch to the pathlib version\n        We want to do this explicitly so we don't have to support all\n        of pathlib and subclasses can override individually if necessary.\n        \"\"\"\n        path_version = self._path.__getattribute__(func)\n\n        # Path functions should be called so the results are calculated\n        if callable(path_version):\n            path_version = path_version(*args, **kwargs)\n\n        # Paths should always be resolved and then converted to the same client + class as this one\n        if isinstance(path_version, PurePosixPath):\n            # always resolve since cloud paths must be absolute\n            path_version = _resolve(path_version)\n            return self._new_cloudpath(path_version)\n\n        # When sequence of PurePosixPath, we want to convert to sequence of CloudPaths\n        if (\n            isinstance(path_version, collections.abc.Sequence)\n            and len(path_version) &gt; 0\n            and isinstance(path_version[0], PurePosixPath)\n        ):\n            sequence_class = (\n                type(path_version) if not isinstance(path_version, _PathParents) else tuple\n            )\n            return sequence_class(  # type: ignore\n                self._new_cloudpath(_resolve(p)) for p in path_version if _resolve(p) != p.root\n            )\n\n        # when pathlib returns something else, we probably just want that thing\n        # cases this should include: str, empty sequence, sequence of str, ...\n        else:\n            return path_version\n\n    def __truediv__(self, other: Union[str, PurePosixPath]) -&gt; Self:\n        if not isinstance(other, (str, PurePosixPath)):\n            raise TypeError(f\"Can only join path {repr(self)} with strings or posix paths.\")\n\n        return self._dispatch_to_path(\"__truediv__\", other)\n\n    def joinpath(self, *pathsegments: Union[str, os.PathLike]) -&gt; Self:\n        return self._dispatch_to_path(\"joinpath\", *pathsegments)\n\n    def absolute(self) -&gt; Self:\n        return self\n\n    def is_absolute(self) -&gt; bool:\n        return True\n\n    def resolve(self, strict: bool = False) -&gt; Self:\n        return self\n\n    def relative_to(self, other: Self, walk_up: bool = False) -&gt; PurePosixPath:\n        # We don't dispatch regularly since this never returns a cloud path (since it is relative, and cloud paths are\n        # absolute)\n        if not isinstance(other, CloudPath):\n            raise ValueError(f\"{self} is a cloud path, but {other} is not\")\n        if self.cloud_prefix != other.cloud_prefix:\n            raise ValueError(\n                f\"{self} is a {self.cloud_prefix} path, but {other} is a {other.cloud_prefix} path\"\n            )\n\n        kwargs = dict(walk_up=walk_up)\n\n        if sys.version_info &lt; (3, 12):\n            kwargs.pop(\"walk_up\")\n\n        return self._path.relative_to(other._path, **kwargs)  # type: ignore[call-arg]\n\n    def is_relative_to(self, other: Self) -&gt; bool:\n        try:\n            self.relative_to(other)\n            return True\n        except ValueError:\n            return False\n\n    @property\n    def name(self) -&gt; str:\n        return self._dispatch_to_path(\"name\")\n\n    def match(self, path_pattern: str, case_sensitive: Optional[bool] = None) -&gt; bool:\n        # strip scheme from start of pattern before testing\n        if path_pattern.startswith(self.anchor + self.drive + \"/\"):\n            path_pattern = path_pattern[len(self.anchor + self.drive + \"/\") :]\n\n        kwargs = dict(case_sensitive=case_sensitive)\n\n        if sys.version_info &lt; (3, 12):\n            kwargs.pop(\"case_sensitive\")\n\n        return self._dispatch_to_path(\"match\", path_pattern, **kwargs)\n\n    @property\n    def parent(self) -&gt; Self:\n        return self._dispatch_to_path(\"parent\")\n\n    @property\n    def parents(self) -&gt; Sequence[Self]:\n        return self._dispatch_to_path(\"parents\")\n\n    @property\n    def parts(self) -&gt; Tuple[str, ...]:\n        parts = self._dispatch_to_path(\"parts\")\n        if parts[0] == \"/\":\n            parts = parts[1:]\n\n        return (self.anchor, *parts)\n\n    @property\n    def stem(self) -&gt; str:\n        return self._dispatch_to_path(\"stem\")\n\n    @property\n    def suffix(self) -&gt; str:\n        return self._dispatch_to_path(\"suffix\")\n\n    @property\n    def suffixes(self) -&gt; List[str]:\n        return self._dispatch_to_path(\"suffixes\")\n\n    def with_stem(self, stem: str) -&gt; Self:\n        try:\n            return self._dispatch_to_path(\"with_stem\", stem)\n        except AttributeError:\n            # with_stem was only added in python 3.9, so we fallback for compatibility\n            return self.with_name(stem + self.suffix)\n\n    def with_name(self, name: str) -&gt; Self:\n        return self._dispatch_to_path(\"with_name\", name)\n\n    def with_segments(self, *pathsegments) -&gt; Self:\n        \"\"\"Create a new CloudPath with the same client out of the given segments.\n        The first segment will be interpreted as the bucket/container name.\n        \"\"\"\n        return self._new_cloudpath(\"/\".join(pathsegments))\n\n    def with_suffix(self, suffix: str) -&gt; Self:\n        return self._dispatch_to_path(\"with_suffix\", suffix)\n\n    # ====================== DISPATCHED TO LOCAL CACHE FOR CONCRETE PATHS ======================\n    # Items that can be executed on the cached file on the local filesystem\n    def _dispatch_to_local_cache_path(self, func: str, *args, **kwargs) -&gt; Any:\n        self._refresh_cache()\n\n        path_version = self._local.__getattribute__(func)\n\n        # Path functions should be called so the results are calculated\n        if callable(path_version):\n            path_version = path_version(*args, **kwargs)\n\n        # Paths should always be resolved and then converted to the same client + class as this one\n        if isinstance(path_version, (PosixPath, WindowsPath)):\n            # always resolve since cloud paths must be absolute\n            path_version = path_version.resolve()\n            return self._new_cloudpath(path_version)\n\n        # when pathlib returns a string, etc. we probably just want that thing\n        else:\n            return path_version\n\n    def stat(self, follow_symlinks: bool = True) -&gt; os.stat_result:\n        \"\"\"Note: for many clients, we may want to override so we don't incur\n        network costs since many of these properties are available as\n        API calls.\n        \"\"\"\n        warn(\n            f\"stat not implemented as API call for {self.__class__} so file must be downloaded to \"\n            f\"calculate stats; this may take a long time depending on filesize\"\n        )\n        return self._dispatch_to_local_cache_path(\"stat\", follow_symlinks=follow_symlinks)\n\n    # ===========  public cloud methods, not in pathlib ===============\n    def download_to(self, destination: Union[str, os.PathLike]) -&gt; Path:\n        destination = Path(destination)\n\n        if not self.exists():\n            raise CloudPathNotExistsError(f\"Cannot download because path does not exist: {self}\")\n\n        if self.is_file():\n            if destination.is_dir():\n                destination = destination / self.name\n            return self.client._download_file(self, destination)\n        else:\n            destination.mkdir(exist_ok=True)\n            for f in self.iterdir():\n                rel = str(self)\n                if not rel.endswith(\"/\"):\n                    rel = rel + \"/\"\n\n                rel_dest = str(f)[len(rel) :]\n                f.download_to(destination / rel_dest)\n\n            return destination\n\n    def rmtree(self) -&gt; None:\n        \"\"\"Delete an entire directory tree.\"\"\"\n        if self.is_file():\n            raise CloudPathNotADirectoryError(\n                f\"Path {self} is a file; call unlink instead of rmtree.\"\n            )\n        self.client._remove(self)\n\n    def upload_from(\n        self,\n        source: Union[str, os.PathLike],\n        force_overwrite_to_cloud: Optional[bool] = None,\n    ) -&gt; Self:\n        \"\"\"Upload a file or directory to the cloud path.\"\"\"\n        source = Path(source)\n\n        if source.is_dir():\n            for p in source.iterdir():\n                (self / p.name).upload_from(p, force_overwrite_to_cloud=force_overwrite_to_cloud)\n\n            return self\n\n        else:\n            if self.exists() and self.is_dir():\n                dst = self / source.name\n            else:\n                dst = self\n\n            dst._upload_file_to_cloud(source, force_overwrite_to_cloud=force_overwrite_to_cloud)\n\n            return dst\n\n    @overload\n    def copy(\n        self,\n        destination: Self,\n        force_overwrite_to_cloud: Optional[bool] = None,\n    ) -&gt; Self: ...\n\n    @overload\n    def copy(\n        self,\n        destination: Path,\n        force_overwrite_to_cloud: Optional[bool] = None,\n    ) -&gt; Path: ...\n\n    @overload\n    def copy(\n        self,\n        destination: str,\n        force_overwrite_to_cloud: Optional[bool] = None,\n    ) -&gt; Union[Path, \"CloudPath\"]: ...\n\n    def copy(self, destination, force_overwrite_to_cloud=None):\n        \"\"\"Copy self to destination folder of file, if self is a file.\"\"\"\n        if not self.exists() or not self.is_file():\n            raise ValueError(\n                f\"Path {self} should be a file. To copy a directory tree use the method copytree.\"\n            )\n\n        # handle string version of cloud paths + local paths\n        if isinstance(destination, (str, os.PathLike)):\n            destination = anypath.to_anypath(destination)\n\n        if not isinstance(destination, CloudPath):\n            return self.download_to(destination)\n\n        # if same client, use cloud-native _move_file on client to avoid downloading\n        if self.client is destination.client:\n            if destination.exists() and destination.is_dir():\n                destination = destination / self.name\n\n            if force_overwrite_to_cloud is None:\n                force_overwrite_to_cloud = os.environ.get(\n                    \"CLOUDPATHLIB_FORCE_OVERWRITE_TO_CLOUD\", \"False\"\n                ).lower() in [\"1\", \"true\"]\n\n            if (\n                not force_overwrite_to_cloud\n                and destination.exists()\n                and destination.stat().st_mtime &gt;= self.stat().st_mtime\n            ):\n                raise OverwriteNewerCloudError(\n                    f\"File ({destination}) is newer than ({self}). \"\n                    f\"To overwrite \"\n                    f\"pass `force_overwrite_to_cloud=True`.\"\n                )\n\n            return self.client._move_file(self, destination, remove_src=False)\n\n        else:\n            if not destination.exists() or destination.is_file():\n                return destination.upload_from(\n                    self.fspath, force_overwrite_to_cloud=force_overwrite_to_cloud\n                )\n            else:\n                return (destination / self.name).upload_from(\n                    self.fspath, force_overwrite_to_cloud=force_overwrite_to_cloud\n                )\n\n    @overload\n    def copytree(\n        self,\n        destination: Self,\n        force_overwrite_to_cloud: Optional[bool] = None,\n        ignore: Optional[Callable[[str, Iterable[str]], Container[str]]] = None,\n    ) -&gt; Self: ...\n\n    @overload\n    def copytree(\n        self,\n        destination: Path,\n        force_overwrite_to_cloud: Optional[bool] = None,\n        ignore: Optional[Callable[[str, Iterable[str]], Container[str]]] = None,\n    ) -&gt; Path: ...\n\n    @overload\n    def copytree(\n        self,\n        destination: str,\n        force_overwrite_to_cloud: Optional[bool] = None,\n        ignore: Optional[Callable[[str, Iterable[str]], Container[str]]] = None,\n    ) -&gt; Union[Path, \"CloudPath\"]: ...\n\n    def copytree(self, destination, force_overwrite_to_cloud=None, ignore=None):\n        \"\"\"Copy self to a directory, if self is a directory.\"\"\"\n        if not self.is_dir():\n            raise CloudPathNotADirectoryError(\n                f\"Origin path {self} must be a directory. To copy a single file use the method copy.\"\n            )\n\n        # handle string version of cloud paths + local paths\n        if isinstance(destination, (str, os.PathLike)):\n            destination = anypath.to_anypath(destination)\n\n        if destination.exists() and destination.is_file():\n            raise CloudPathFileExistsError(\n                f\"Destination path {destination} of copytree must be a directory.\"\n            )\n\n        contents = list(self.iterdir())\n\n        if ignore is not None:\n            ignored_names = ignore(self._no_prefix_no_drive, [x.name for x in contents])\n        else:\n            ignored_names = set()\n\n        destination.mkdir(parents=True, exist_ok=True)\n\n        for subpath in contents:\n            if subpath.name in ignored_names:\n                continue\n            if subpath.is_file():\n                subpath.copy(\n                    destination / subpath.name, force_overwrite_to_cloud=force_overwrite_to_cloud\n                )\n            elif subpath.is_dir():\n                subpath.copytree(\n                    destination / subpath.name,\n                    force_overwrite_to_cloud=force_overwrite_to_cloud,\n                    ignore=ignore,\n                )\n\n        return destination\n\n    def clear_cache(self):\n        \"\"\"Removes cache if it exists\"\"\"\n        if self._local.exists():\n            if self._local.is_file():\n                self._local.unlink()\n            else:\n                shutil.rmtree(self._local)\n\n    # ===========  private cloud methods ===============\n    @property\n    def _local(self) -&gt; Path:\n        \"\"\"Cached local version of the file.\"\"\"\n        return self.client._local_cache_dir / self._no_prefix\n\n    def _new_cloudpath(self, path: Union[str, os.PathLike]) -&gt; Self:\n        \"\"\"Use the scheme, client, cache dir of this cloudpath to instantiate\n        a new cloudpath of the same type with the path passed.\n\n        Used to make results of iterdir and joins have a unified client + cache.\n        \"\"\"\n        path = str(path)\n\n        # strip initial \"/\" if path has one\n        if path.startswith(\"/\"):\n            path = path[1:]\n\n        # add prefix/anchor if it is not already\n        if not path.startswith(self.cloud_prefix):\n            path = f\"{self.cloud_prefix}{path}\"\n\n        return self.client.CloudPath(path)\n\n    def _refresh_cache(self, force_overwrite_from_cloud: Optional[bool] = None) -&gt; None:\n        try:\n            stats = self.stat()\n        except NoStatError:\n            # nothing to cache if the file does not exist; happens when creating\n            # new files that will be uploaded\n            return\n\n        if force_overwrite_from_cloud is None:\n            force_overwrite_from_cloud = os.environ.get(\n                \"CLOUDPATHLIB_FORCE_OVERWRITE_FROM_CLOUD\", \"False\"\n            ).lower() in [\"1\", \"true\"]\n\n        # if not exist or cloud newer\n        if (\n            force_overwrite_from_cloud\n            or not self._local.exists()\n            or (self._local.stat().st_mtime &lt; stats.st_mtime)\n        ):\n            # ensure there is a home for the file\n            self._local.parent.mkdir(parents=True, exist_ok=True)\n            self.download_to(self._local)\n\n            # force cache time to match cloud times\n            os.utime(self._local, times=(stats.st_mtime, stats.st_mtime))\n\n        if self._dirty:\n            raise OverwriteDirtyFileError(\n                f\"Local file ({self._local}) for cloud path ({self}) has been changed by your code, but \"\n                f\"is being requested for download from cloud. Either (1) push your changes to the cloud, \"\n                f\"(2) remove the local file, or (3) pass `force_overwrite_from_cloud=True` to \"\n                f\"overwrite; or set env var CLOUDPATHLIB_FORCE_OVERWRITE_FROM_CLOUD=1.\"\n            )\n\n        # if local newer but not dirty, it was updated\n        # by a separate process; do not overwrite unless forced to\n        if self._local.stat().st_mtime &gt; stats.st_mtime:\n            raise OverwriteNewerLocalError(\n                f\"Local file ({self._local}) for cloud path ({self}) is newer on disk, but \"\n                f\"is being requested for download from cloud. Either (1) push your changes to the cloud, \"\n                f\"(2) remove the local file, or (3) pass `force_overwrite_from_cloud=True` to \"\n                f\"overwrite; or set env var CLOUDPATHLIB_FORCE_OVERWRITE_FROM_CLOUD=1.\"\n            )\n\n    def _upload_local_to_cloud(\n        self,\n        force_overwrite_to_cloud: Optional[bool] = None,\n    ) -&gt; Self:\n        \"\"\"Uploads cache file at self._local to the cloud\"\"\"\n        # We should never try to be syncing entire directories; we should only\n        # cache and upload individual files.\n        if self._local.is_dir():\n            raise ValueError(\"Only individual files can be uploaded to the cloud\")\n\n        uploaded = self._upload_file_to_cloud(\n            self._local, force_overwrite_to_cloud=force_overwrite_to_cloud\n        )\n\n        # force cache time to match cloud times\n        stats = self.stat()\n        os.utime(self._local, times=(stats.st_mtime, stats.st_mtime))\n\n        # reset dirty and handle now that this is uploaded\n        self._dirty = False\n        self._handle = None\n\n        return uploaded\n\n    def _upload_file_to_cloud(\n        self,\n        local_path: Path,\n        force_overwrite_to_cloud: Optional[bool] = None,\n    ) -&gt; Self:\n        \"\"\"Uploads file at `local_path` to the cloud if there is not a newer file\n        already there.\n        \"\"\"\n        if force_overwrite_to_cloud is None:\n            force_overwrite_to_cloud = os.environ.get(\n                \"CLOUDPATHLIB_FORCE_OVERWRITE_TO_CLOUD\", \"False\"\n            ).lower() in [\"1\", \"true\"]\n\n        if force_overwrite_to_cloud:\n            # If we are overwriting no need to perform any checks, so we can save time\n            self.client._upload_file(\n                local_path,\n                self,\n            )\n            return self\n\n        try:\n            stats = self.stat()\n        except NoStatError:\n            stats = None\n\n        # if cloud does not exist or local is newer, do the upload\n        if not stats or (local_path.stat().st_mtime &gt; stats.st_mtime):\n            self.client._upload_file(\n                local_path,\n                self,\n            )\n\n            return self\n\n        # cloud is newer and we are not overwriting\n        raise OverwriteNewerCloudError(\n            f\"Local file ({self._local}) for cloud path ({self}) is newer in the cloud disk, but \"\n            f\"is being requested to be uploaded to the cloud. Either (1) redownload changes from the cloud or \"\n            f\"(2) pass `force_overwrite_to_cloud=True` to \"\n            f\"overwrite; or set env var CLOUDPATHLIB_FORCE_OVERWRITE_TO_CLOUD=1.\"\n        )\n\n    # ===========  pydantic integration special methods ===============\n    @classmethod\n    def __get_pydantic_core_schema__(cls, _source_type: Any, _handler):\n        \"\"\"Pydantic special method. See\n        https://docs.pydantic.dev/2.0/usage/types/custom/\"\"\"\n        try:\n            from pydantic_core import core_schema\n\n            return core_schema.no_info_after_validator_function(\n                cls.validate,\n                core_schema.any_schema(),\n            )\n        except ImportError:\n            return None\n\n    @classmethod\n    def validate(cls, v: str) -&gt; Self:\n        \"\"\"Used as a Pydantic validator. See\n        https://docs.pydantic.dev/2.0/usage/types/custom/\"\"\"\n        return cls(v)\n\n    @classmethod\n    def __get_validators__(cls) -&gt; Generator[Callable[[Any], Self], None, None]:\n        \"\"\"Pydantic special method. See\n        https://pydantic-docs.helpmanual.io/usage/types/#custom-data-types\"\"\"\n        yield cls._validate\n\n    @classmethod\n    def _validate(cls, value: Any) -&gt; Self:\n        \"\"\"Used as a Pydantic validator. See\n        https://pydantic-docs.helpmanual.io/usage/types/#custom-data-types\"\"\"\n        return cls(value)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath-attributes","title":"Attributes","text":""},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.anchor","title":"<code>anchor: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The concatenation of the drive and root, or ''. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.client","title":"<code>client</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.drive","title":"<code>drive: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.fspath","title":"<code>fspath: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.name","title":"<code>name: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The final path component, if any. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.parent","title":"<code>parent: typing_extensions.Self</code>  <code>property</code> <code>readonly</code>","text":"<p>The logical parent of the path. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.parents","title":"<code>parents: Sequence[typing_extensions.Self]</code>  <code>property</code> <code>readonly</code>","text":"<p>A sequence of this path's logical parents. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.parts","title":"<code>parts: Tuple[str, ...]</code>  <code>property</code> <code>readonly</code>","text":"<p>An object providing sequence-like access to the components in the filesystem path. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.stem","title":"<code>stem: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The final path component, minus its last suffix. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.suffix","title":"<code>suffix: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The final component's last suffix, if any.</p> <p>This includes the leading period. For example: '.txt'  (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.suffixes","title":"<code>suffixes: List[str]</code>  <code>property</code> <code>readonly</code>","text":"<p>A list of the final component's suffixes, if any.</p> <p>These include the leading periods. For example: ['.tar', '.gz']  (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath-methods","title":"Methods","text":""},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.__init__","title":"<code>__init__(self, cloud_path: Union[str, typing_extensions.Self, CloudPath], client: Optional[Client] = None) -&gt; None</code>  <code>special</code>","text":"Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def __init__(\n    self,\n    cloud_path: Union[str, Self, \"CloudPath\"],\n    client: Optional[\"Client\"] = None,\n) -&gt; None:\n    # handle if local file gets opened. must be set at the top of the method in case any code\n    # below raises an exception, this prevents __del__ from raising an AttributeError\n    self._handle: Optional[IO] = None\n    self._client: Optional[\"Client\"] = None\n\n    self.is_valid_cloudpath(cloud_path, raise_on_error=True)\n    self._cloud_meta.validate_completeness()\n\n    # versions of the raw string that provide useful methods\n    self._str = str(cloud_path)\n    self._url = urlparse(self._str)\n    self._path = PurePosixPath(f\"/{self._no_prefix}\")\n\n    # setup client\n    if client is None:\n        if isinstance(cloud_path, CloudPath):\n            self._client = cloud_path.client\n    else:\n        self._client = client\n\n    if client is not None and not isinstance(client, self._cloud_meta.client_class):\n        raise ClientMismatchError(\n            f\"Client of type [{client.__class__}] is not valid for cloud path of type \"\n            f\"[{self.__class__}]; must be instance of [{self._cloud_meta.client_class}], or \"\n            f\"None to use default client for this cloud path class.\"\n        )\n\n    # track if local has been written to, if so it may need to be uploaded\n    self._dirty = False\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.absolute","title":"<code>absolute(self) -&gt; typing_extensions.Self</code>","text":"<p>Return an absolute version of this path.  This function works even if the path doesn't point to anything.</p> <p>No normalization is done, i.e. all '.' and '..' will be kept along. Use resolve() to get the canonical path to a file.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def absolute(self) -&gt; Self:\n    return self\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.as_uri","title":"<code>as_uri(self) -&gt; str</code>","text":"<p>Return the path as a 'file' URI. (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def as_uri(self) -&gt; str:\n    return str(self)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.as_url","title":"<code>as_url(self, presign: bool = False, expire_seconds: int = 3600) -&gt; str</code>","text":"Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def as_url(self, presign: bool = False, expire_seconds: int = 60 * 60) -&gt; str:\n    if presign:\n        url = self.client._generate_presigned_url(self, expire_seconds=expire_seconds)\n    else:\n        url = self.client._get_public_url(self)\n    return url\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.clear_cache","title":"<code>clear_cache(self)</code>","text":"<p>Removes cache if it exists</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def clear_cache(self):\n    \"\"\"Removes cache if it exists\"\"\"\n    if self._local.exists():\n        if self._local.is_file():\n            self._local.unlink()\n        else:\n            shutil.rmtree(self._local)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.copy","title":"<code>copy(self, destination, force_overwrite_to_cloud = None)</code>","text":"<p>Copy self to destination folder of file, if self is a file.</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def copy(self, destination, force_overwrite_to_cloud=None):\n    \"\"\"Copy self to destination folder of file, if self is a file.\"\"\"\n    if not self.exists() or not self.is_file():\n        raise ValueError(\n            f\"Path {self} should be a file. To copy a directory tree use the method copytree.\"\n        )\n\n    # handle string version of cloud paths + local paths\n    if isinstance(destination, (str, os.PathLike)):\n        destination = anypath.to_anypath(destination)\n\n    if not isinstance(destination, CloudPath):\n        return self.download_to(destination)\n\n    # if same client, use cloud-native _move_file on client to avoid downloading\n    if self.client is destination.client:\n        if destination.exists() and destination.is_dir():\n            destination = destination / self.name\n\n        if force_overwrite_to_cloud is None:\n            force_overwrite_to_cloud = os.environ.get(\n                \"CLOUDPATHLIB_FORCE_OVERWRITE_TO_CLOUD\", \"False\"\n            ).lower() in [\"1\", \"true\"]\n\n        if (\n            not force_overwrite_to_cloud\n            and destination.exists()\n            and destination.stat().st_mtime &gt;= self.stat().st_mtime\n        ):\n            raise OverwriteNewerCloudError(\n                f\"File ({destination}) is newer than ({self}). \"\n                f\"To overwrite \"\n                f\"pass `force_overwrite_to_cloud=True`.\"\n            )\n\n        return self.client._move_file(self, destination, remove_src=False)\n\n    else:\n        if not destination.exists() or destination.is_file():\n            return destination.upload_from(\n                self.fspath, force_overwrite_to_cloud=force_overwrite_to_cloud\n            )\n        else:\n            return (destination / self.name).upload_from(\n                self.fspath, force_overwrite_to_cloud=force_overwrite_to_cloud\n            )\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.copytree","title":"<code>copytree(self, destination, force_overwrite_to_cloud = None, ignore = None)</code>","text":"<p>Copy self to a directory, if self is a directory.</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def copytree(self, destination, force_overwrite_to_cloud=None, ignore=None):\n    \"\"\"Copy self to a directory, if self is a directory.\"\"\"\n    if not self.is_dir():\n        raise CloudPathNotADirectoryError(\n            f\"Origin path {self} must be a directory. To copy a single file use the method copy.\"\n        )\n\n    # handle string version of cloud paths + local paths\n    if isinstance(destination, (str, os.PathLike)):\n        destination = anypath.to_anypath(destination)\n\n    if destination.exists() and destination.is_file():\n        raise CloudPathFileExistsError(\n            f\"Destination path {destination} of copytree must be a directory.\"\n        )\n\n    contents = list(self.iterdir())\n\n    if ignore is not None:\n        ignored_names = ignore(self._no_prefix_no_drive, [x.name for x in contents])\n    else:\n        ignored_names = set()\n\n    destination.mkdir(parents=True, exist_ok=True)\n\n    for subpath in contents:\n        if subpath.name in ignored_names:\n            continue\n        if subpath.is_file():\n            subpath.copy(\n                destination / subpath.name, force_overwrite_to_cloud=force_overwrite_to_cloud\n            )\n        elif subpath.is_dir():\n            subpath.copytree(\n                destination / subpath.name,\n                force_overwrite_to_cloud=force_overwrite_to_cloud,\n                ignore=ignore,\n            )\n\n    return destination\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.download_to","title":"<code>download_to(self, destination: Union[str, os.PathLike]) -&gt; Path</code>","text":"Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def download_to(self, destination: Union[str, os.PathLike]) -&gt; Path:\n    destination = Path(destination)\n\n    if not self.exists():\n        raise CloudPathNotExistsError(f\"Cannot download because path does not exist: {self}\")\n\n    if self.is_file():\n        if destination.is_dir():\n            destination = destination / self.name\n        return self.client._download_file(self, destination)\n    else:\n        destination.mkdir(exist_ok=True)\n        for f in self.iterdir():\n            rel = str(self)\n            if not rel.endswith(\"/\"):\n                rel = rel + \"/\"\n\n            rel_dest = str(f)[len(rel) :]\n            f.download_to(destination / rel_dest)\n\n        return destination\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.exists","title":"<code>exists(self) -&gt; bool</code>","text":"<p>Whether this path exists.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def exists(self) -&gt; bool:\n    return self.client._exists(self)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.glob","title":"<code>glob(self, pattern: str, case_sensitive: Optional[bool] = None) -&gt; Generator[typing_extensions.Self, NoneType, NoneType]</code>","text":"<p>Iterate over this subtree and yield all existing files (of any kind, including directories) matching the given relative pattern.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def glob(\n    self, pattern: str, case_sensitive: Optional[bool] = None\n) -&gt; Generator[Self, None, None]:\n    self._glob_checks(pattern)\n\n    pattern_parts = PurePosixPath(pattern).parts\n    selector = _make_selector(\n        tuple(pattern_parts), _posix_flavour, case_sensitive=case_sensitive\n    )\n\n    yield from self._glob(\n        selector,\n        \"/\" in pattern\n        or \"**\"\n        in pattern,  # recursive listing needed if explicit ** or any sub folder in pattern\n    )\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.is_absolute","title":"<code>is_absolute(self) -&gt; bool</code>","text":"<p>True if the path is absolute (has both a root and, if applicable, a drive). (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def is_absolute(self) -&gt; bool:\n    return True\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.is_dir","title":"<code>is_dir(self) -&gt; bool</code>","text":"<p>Whether this path is a directory.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>@abc.abstractmethod\ndef is_dir(self) -&gt; bool:\n    \"\"\"Should be implemented without requiring a dir is downloaded\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.is_file","title":"<code>is_file(self) -&gt; bool</code>","text":"<p>Whether this path is a regular file (also True for symlinks pointing to regular files).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>@abc.abstractmethod\ndef is_file(self) -&gt; bool:\n    \"\"\"Should be implemented without requiring that the file is downloaded\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.is_junction","title":"<code>is_junction(self)</code>","text":"Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def is_junction(self):\n    return False  # only windows paths can be junctions, not cloudpaths\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.is_relative_to","title":"<code>is_relative_to(self, other: typing_extensions.Self) -&gt; bool</code>","text":"Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def is_relative_to(self, other: Self) -&gt; bool:\n    try:\n        self.relative_to(other)\n        return True\n    except ValueError:\n        return False\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.is_valid_cloudpath","title":"<code>is_valid_cloudpath(path: Union[str, CloudPath], raise_on_error: bool = False) -&gt; Union[bool, typing_extensions.TypeGuard[typing_extensions.Self]]</code>  <code>classmethod</code>","text":"Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>@classmethod\ndef is_valid_cloudpath(\n    cls, path: Union[str, \"CloudPath\"], raise_on_error: bool = False\n) -&gt; Union[bool, TypeGuard[Self]]:\n    valid = str(path).lower().startswith(cls.cloud_prefix.lower())\n\n    if raise_on_error and not valid:\n        raise InvalidPrefixError(\n            f\"'{path}' is not a valid path since it does not start with '{cls.cloud_prefix}'\"\n        )\n\n    return valid\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.iterdir","title":"<code>iterdir(self) -&gt; Generator[typing_extensions.Self, NoneType, NoneType]</code>","text":"<p>Iterate over the files in this directory.  Does not yield any result for the special paths '.' and '..'.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def iterdir(self) -&gt; Generator[Self, None, None]:\n    for f, _ in self.client._list_dir(self, recursive=False):\n        if f != self:  # iterdir does not include itself in pathlib\n            yield f\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.joinpath","title":"<code>joinpath(self, *pathsegments: Union[str, os.PathLike]) -&gt; typing_extensions.Self</code>","text":"<p>Combine this path with one or several arguments, and return a new path representing either a subpath (if all arguments are relative paths) or a totally different path (if one of the arguments is anchored).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def joinpath(self, *pathsegments: Union[str, os.PathLike]) -&gt; Self:\n    return self._dispatch_to_path(\"joinpath\", *pathsegments)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.match","title":"<code>match(self, path_pattern: str, case_sensitive: Optional[bool] = None) -&gt; bool</code>","text":"<p>Return True if this path matches the given pattern.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def match(self, path_pattern: str, case_sensitive: Optional[bool] = None) -&gt; bool:\n    # strip scheme from start of pattern before testing\n    if path_pattern.startswith(self.anchor + self.drive + \"/\"):\n        path_pattern = path_pattern[len(self.anchor + self.drive + \"/\") :]\n\n    kwargs = dict(case_sensitive=case_sensitive)\n\n    if sys.version_info &lt; (3, 12):\n        kwargs.pop(\"case_sensitive\")\n\n    return self._dispatch_to_path(\"match\", path_pattern, **kwargs)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.mkdir","title":"<code>mkdir(self, parents: bool = False, exist_ok: bool = False) -&gt; None</code>","text":"<p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>@abc.abstractmethod\ndef mkdir(self, parents: bool = False, exist_ok: bool = False) -&gt; None:\n    \"\"\"Should be implemented using the client API without requiring a dir is downloaded\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.open","title":"<code>open(self, mode: str = 'r', buffering: int = -1, encoding: Optional[str] = None, errors: Optional[str] = None, newline: Optional[str] = None, force_overwrite_from_cloud: Optional[bool] = None, force_overwrite_to_cloud: Optional[bool] = None) -&gt; IO[Any]</code>","text":"<p>Open the file pointed by this path and return a file object, as the built-in open() function does.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def open(\n    self,\n    mode: str = \"r\",\n    buffering: int = -1,\n    encoding: Optional[str] = None,\n    errors: Optional[str] = None,\n    newline: Optional[str] = None,\n    force_overwrite_from_cloud: Optional[bool] = None,  # extra kwarg not in pathlib\n    force_overwrite_to_cloud: Optional[bool] = None,  # extra kwarg not in pathlib\n) -&gt; IO[Any]:\n    # if trying to call open on a directory that exists\n    if self.exists() and not self.is_file():\n        raise CloudPathIsADirectoryError(\n            f\"Cannot open directory, only files. Tried to open ({self})\"\n        )\n\n    if mode == \"x\" and self.exists():\n        raise CloudPathFileExistsError(f\"Cannot open existing file ({self}) for creation.\")\n\n    # TODO: consider streaming from client rather than DLing entire file to cache\n    self._refresh_cache(force_overwrite_from_cloud=force_overwrite_from_cloud)\n\n    # create any directories that may be needed if the file is new\n    if not self._local.exists():\n        self._local.parent.mkdir(parents=True, exist_ok=True)\n        original_mtime = 0.0\n    else:\n        original_mtime = self._local.stat().st_mtime\n\n    buffer = self._local.open(\n        mode=mode,\n        buffering=buffering,\n        encoding=encoding,\n        errors=errors,\n        newline=newline,\n    )\n\n    # write modes need special on closing the buffer\n    if any(m in mode for m in (\"w\", \"+\", \"x\", \"a\")):\n        # dirty, handle, patch close\n        wrapped_close = buffer.close\n\n        # since we are pretending this is a cloud file, upload it to the cloud\n        # when the buffer is closed\n        def _patched_close_upload(*args, **kwargs) -&gt; None:\n            wrapped_close(*args, **kwargs)\n\n            # we should be idempotent and not upload again if\n            # we already ran our close method patch\n            if not self._dirty:\n                return\n\n            # original mtime should match what was in the cloud; because of system clocks or rounding\n            # by the cloud provider, the new version in our cache is \"older\" than the original version;\n            # explicitly set the new modified time to be after the original modified time.\n            if self._local.stat().st_mtime &lt; original_mtime:\n                new_mtime = original_mtime + 1\n                os.utime(self._local, times=(new_mtime, new_mtime))\n\n            self._upload_local_to_cloud(force_overwrite_to_cloud=force_overwrite_to_cloud)\n            self._dirty = False\n\n        buffer.close = _patched_close_upload  # type: ignore\n\n        # keep reference in case we need to close when __del__ is called on this object\n        self._handle = buffer\n\n        # opened for write, so mark dirty\n        self._dirty = True\n\n    # if we don't want any cache around, remove the cache\n    # as soon as the file is closed\n    if self.client.file_cache_mode == FileCacheMode.close_file:\n        # this may be _patched_close_upload, in which case we need to\n        # make sure to call that first so the file gets uploaded\n        wrapped_close_for_cache = buffer.close\n\n        def _patched_close_empty_cache(*args, **kwargs):\n            wrapped_close_for_cache(*args, **kwargs)\n\n            # remove local file as last step on closing\n            self.clear_cache()\n\n        buffer.close = _patched_close_empty_cache  # type: ignore\n\n    return buffer\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.read_bytes","title":"<code>read_bytes(self) -&gt; bytes</code>","text":"<p>Open the file in bytes mode, read it, and close the file.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def read_bytes(self) -&gt; bytes:\n    with self.open(mode=\"rb\") as f:\n        return f.read()\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.read_text","title":"<code>read_text(self, encoding: Optional[str] = None, errors: Optional[str] = None) -&gt; str</code>","text":"<p>Open the file in text mode, read it, and close the file.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def read_text(self, encoding: Optional[str] = None, errors: Optional[str] = None) -&gt; str:\n    with self.open(mode=\"r\", encoding=encoding, errors=errors) as f:\n        return f.read()\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.relative_to","title":"<code>relative_to(self, other: typing_extensions.Self, walk_up: bool = False) -&gt; PurePosixPath</code>","text":"<p>Return the relative path to another path identified by the passed arguments.  If the operation is not possible (because this is not a subpath of the other path), raise ValueError.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def relative_to(self, other: Self, walk_up: bool = False) -&gt; PurePosixPath:\n    # We don't dispatch regularly since this never returns a cloud path (since it is relative, and cloud paths are\n    # absolute)\n    if not isinstance(other, CloudPath):\n        raise ValueError(f\"{self} is a cloud path, but {other} is not\")\n    if self.cloud_prefix != other.cloud_prefix:\n        raise ValueError(\n            f\"{self} is a {self.cloud_prefix} path, but {other} is a {other.cloud_prefix} path\"\n        )\n\n    kwargs = dict(walk_up=walk_up)\n\n    if sys.version_info &lt; (3, 12):\n        kwargs.pop(\"walk_up\")\n\n    return self._path.relative_to(other._path, **kwargs)  # type: ignore[call-arg]\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.rename","title":"<code>rename(self, target: typing_extensions.Self) -&gt; typing_extensions.Self</code>","text":"<p>Rename this path to the target path.</p> <p>The target path may be absolute or relative. Relative paths are interpreted relative to the current working directory, not the directory of the Path object.</p> <p>Returns the new Path instance pointing to the target path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def rename(self, target: Self) -&gt; Self:\n    # for cloud services replace == rename since we don't just rename,\n    # we actually move files\n    return self.replace(target)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.replace","title":"<code>replace(self, target: typing_extensions.Self) -&gt; typing_extensions.Self</code>","text":"<p>Rename this path to the target path, overwriting if that path exists.</p> <p>The target path may be absolute or relative. Relative paths are interpreted relative to the current working directory, not the directory of the Path object.</p> <p>Returns the new Path instance pointing to the target path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def replace(self, target: Self) -&gt; Self:\n    if type(self) is not type(target):\n        raise TypeError(\n            f\"The target based to rename must be an instantiated class of type: {type(self)}\"\n        )\n\n    if self.is_dir():\n        raise CloudPathIsADirectoryError(\n            f\"Path {self} is a directory; rename/replace the files recursively.\"\n        )\n\n    if target == self:\n        # Request is to replace/rename this with the same path - nothing to do\n        return self\n\n    if target.exists():\n        target.unlink()\n\n    self.client._move_file(self, target)\n    return target\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.resolve","title":"<code>resolve(self, strict: bool = False) -&gt; typing_extensions.Self</code>","text":"<p>Make the path absolute, resolving all symlinks on the way and also normalizing it (for example turning slashes into backslashes under Windows).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def resolve(self, strict: bool = False) -&gt; Self:\n    return self\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.rglob","title":"<code>rglob(self, pattern: str, case_sensitive: Optional[bool] = None) -&gt; Generator[typing_extensions.Self, NoneType, NoneType]</code>","text":"<p>Recursively yield all existing files (of any kind, including directories) matching the given relative pattern, anywhere in this subtree.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def rglob(\n    self, pattern: str, case_sensitive: Optional[bool] = None\n) -&gt; Generator[Self, None, None]:\n    self._glob_checks(pattern)\n\n    pattern_parts = PurePosixPath(pattern).parts\n    selector = _make_selector(\n        (\"**\",) + tuple(pattern_parts), _posix_flavour, case_sensitive=case_sensitive\n    )\n\n    yield from self._glob(selector, True)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.rmdir","title":"<code>rmdir(self) -&gt; None</code>","text":"<p>Remove this directory.  The directory must be empty.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def rmdir(self) -&gt; None:\n    if self.is_file():\n        raise CloudPathNotADirectoryError(\n            f\"Path {self} is a file; call unlink instead of rmdir.\"\n        )\n    try:\n        next(self.iterdir())\n        raise DirectoryNotEmptyError(\n            f\"Directory not empty: '{self}'. Use rmtree to delete recursively.\"\n        )\n    except StopIteration:\n        pass\n    self.client._remove(self)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.rmtree","title":"<code>rmtree(self) -&gt; None</code>","text":"<p>Delete an entire directory tree.</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def rmtree(self) -&gt; None:\n    \"\"\"Delete an entire directory tree.\"\"\"\n    if self.is_file():\n        raise CloudPathNotADirectoryError(\n            f\"Path {self} is a file; call unlink instead of rmtree.\"\n        )\n    self.client._remove(self)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.samefile","title":"<code>samefile(self, other_path: Union[str, os.PathLike]) -&gt; bool</code>","text":"<p>Return whether other_path is the same or not as this file (as returned by os.path.samefile()).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def samefile(self, other_path: Union[str, os.PathLike]) -&gt; bool:\n    # all cloud paths are absolute and the paths are used for hash\n    return self == other_path\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.stat","title":"<code>stat(self, follow_symlinks: bool = True) -&gt; stat_result</code>","text":"<p>Return the result of the stat() system call on this path, like os.stat() does.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def stat(self, follow_symlinks: bool = True) -&gt; os.stat_result:\n    \"\"\"Note: for many clients, we may want to override so we don't incur\n    network costs since many of these properties are available as\n    API calls.\n    \"\"\"\n    warn(\n        f\"stat not implemented as API call for {self.__class__} so file must be downloaded to \"\n        f\"calculate stats; this may take a long time depending on filesize\"\n    )\n    return self._dispatch_to_local_cache_path(\"stat\", follow_symlinks=follow_symlinks)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.touch","title":"<code>touch(self, exist_ok: bool = True) -&gt; None</code>","text":"<p>Create this file with the given access mode, if it doesn't exist.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>@abc.abstractmethod\ndef touch(self, exist_ok: bool = True) -&gt; None:\n    \"\"\"Should be implemented using the client API to create and update modified time\"\"\"\n    pass\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.unlink","title":"<code>unlink(self, missing_ok: bool = True) -&gt; None</code>","text":"<p>Remove this file or link. If the path is a directory, use rmdir() instead.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def unlink(self, missing_ok: bool = True) -&gt; None:\n    # Note: missing_ok defaults to False in pathlib, but changing the default now would be a breaking change.\n    if self.is_dir():\n        raise CloudPathIsADirectoryError(\n            f\"Path {self} is a directory; call rmdir instead of unlink.\"\n        )\n    self.client._remove(self, missing_ok)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.upload_from","title":"<code>upload_from(self, source: Union[str, os.PathLike], force_overwrite_to_cloud: Optional[bool] = None) -&gt; typing_extensions.Self</code>","text":"<p>Upload a file or directory to the cloud path.</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def upload_from(\n    self,\n    source: Union[str, os.PathLike],\n    force_overwrite_to_cloud: Optional[bool] = None,\n) -&gt; Self:\n    \"\"\"Upload a file or directory to the cloud path.\"\"\"\n    source = Path(source)\n\n    if source.is_dir():\n        for p in source.iterdir():\n            (self / p.name).upload_from(p, force_overwrite_to_cloud=force_overwrite_to_cloud)\n\n        return self\n\n    else:\n        if self.exists() and self.is_dir():\n            dst = self / source.name\n        else:\n            dst = self\n\n        dst._upload_file_to_cloud(source, force_overwrite_to_cloud=force_overwrite_to_cloud)\n\n        return dst\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.validate","title":"<code>validate(v: str) -&gt; typing_extensions.Self</code>  <code>classmethod</code>","text":"<p>Used as a Pydantic validator. See https://docs.pydantic.dev/2.0/usage/types/custom/</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>@classmethod\ndef validate(cls, v: str) -&gt; Self:\n    \"\"\"Used as a Pydantic validator. See\n    https://docs.pydantic.dev/2.0/usage/types/custom/\"\"\"\n    return cls(v)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.walk","title":"<code>walk(self, top_down: bool = True, on_error: Optional[Callable] = None, follow_symlinks: bool = False) -&gt; Generator[Tuple[typing_extensions.Self, List[str], List[str]], NoneType, NoneType]</code>","text":"Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def walk(\n    self,\n    top_down: bool = True,\n    on_error: Optional[Callable] = None,\n    follow_symlinks: bool = False,\n) -&gt; Generator[Tuple[Self, List[str], List[str]], None, None]:\n    try:\n        file_tree = self._build_subtree(recursive=True)  # walking is always recursive\n        yield from self._walk_results_from_tree(self, file_tree, top_down=top_down)\n\n    except Exception as e:\n        if on_error is not None:\n            on_error(e)\n        else:\n            raise\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.with_name","title":"<code>with_name(self, name: str) -&gt; typing_extensions.Self</code>","text":"<p>Return a new path with the file name changed. (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def with_name(self, name: str) -&gt; Self:\n    return self._dispatch_to_path(\"with_name\", name)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.with_segments","title":"<code>with_segments(self, *pathsegments) -&gt; typing_extensions.Self</code>","text":"<p>Create a new CloudPath with the same client out of the given segments. The first segment will be interpreted as the bucket/container name.</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def with_segments(self, *pathsegments) -&gt; Self:\n    \"\"\"Create a new CloudPath with the same client out of the given segments.\n    The first segment will be interpreted as the bucket/container name.\n    \"\"\"\n    return self._new_cloudpath(\"/\".join(pathsegments))\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.with_stem","title":"<code>with_stem(self, stem: str) -&gt; typing_extensions.Self</code>","text":"Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def with_stem(self, stem: str) -&gt; Self:\n    try:\n        return self._dispatch_to_path(\"with_stem\", stem)\n    except AttributeError:\n        # with_stem was only added in python 3.9, so we fallback for compatibility\n        return self.with_name(stem + self.suffix)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.with_suffix","title":"<code>with_suffix(self, suffix: str) -&gt; typing_extensions.Self</code>","text":"<p>Return a new path with the file suffix changed.  If the path has no suffix, add given suffix.  If the given suffix is an empty string, remove the suffix from the path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def with_suffix(self, suffix: str) -&gt; Self:\n    return self._dispatch_to_path(\"with_suffix\", suffix)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.write_bytes","title":"<code>write_bytes(self, data: bytes) -&gt; int</code>","text":"<p>Open the file in bytes mode, write to it, and close the file.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def write_bytes(self, data: bytes) -&gt; int:\n    \"\"\"Open the file in bytes mode, write to it, and close the file.\n\n    NOTE: vendored from pathlib since we override open\n    https://github.com/python/cpython/blob/3.8/Lib/pathlib.py#L1235-L1242\n    \"\"\"\n    # type-check for the buffer interface before truncating the file\n    view = memoryview(data)\n    with self.open(mode=\"wb\") as f:\n        return f.write(view)\n</code></pre>"},{"location":"api-reference/cloudpath/#cloudpathlib.cloudpath.CloudPath.write_text","title":"<code>write_text(self, data: str, encoding: Optional[str] = None, errors: Optional[str] = None, newline: Optional[str] = None) -&gt; int</code>","text":"<p>Open the file in text mode, write to it, and close the file.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/cloudpath.py</code> <pre><code>def write_text(\n    self,\n    data: str,\n    encoding: Optional[str] = None,\n    errors: Optional[str] = None,\n    newline: Optional[str] = None,\n) -&gt; int:\n    \"\"\"Open the file in text mode, write to it, and close the file.\n\n    NOTE: vendored from pathlib since we override open\n    https://github.com/python/cpython/blob/3.10/Lib/pathlib.py#L1146-L1155\n    \"\"\"\n    if not isinstance(data, str):\n        raise TypeError(\"data must be str, not %s\" % data.__class__.__name__)\n\n    with self.open(mode=\"w\", encoding=encoding, errors=errors, newline=newline) as f:\n        return f.write(data)\n</code></pre>"},{"location":"api-reference/enums/","title":"cloudpathlib.enums","text":""},{"location":"api-reference/enums/#cloudpathlib.enums-classes","title":"Classes","text":""},{"location":"api-reference/enums/#cloudpathlib.enums.FileCacheMode","title":"<code> FileCacheMode            (str, Enum)         </code>","text":"<p>Enumeration of the modes available for for the cloudpathlib file cache.</p> <p>Attributes:</p> Name Type Description <code>persistent</code> <code>str</code> <p>Cache is not removed by <code>cloudpathlib</code>.</p> <code>tmp_dir</code> <code>str</code> <p>Cache is stored in a <code>TemporaryDirectory</code> which is removed when the Client object is garbage collected (or by the OS at some point if not).</p> <code>cloudpath_object</code> <code>str</code> <p>Cache for a <code>CloudPath</code> object is removed when <code>__del__</code> for that object is called by Python garbage collection.</p> <code>close_file</code> <code>str</code> <p>Cache for a <code>CloudPath</code> file is removed as soon as the file is closed. Note: you must use <code>CloudPath.open</code> whenever opening the file for this method to function.</p> <p>Modes can be set by passing them to the Client or by setting the <code>CLOUDPATHLIB_FILE_CACHE_MODE</code> environment variable.</p> <p>For more detail, see the caching documentation page.</p> Source code in <code>cloudpathlib/enums.py</code> <pre><code>class FileCacheMode(str, Enum):\n    \"\"\"Enumeration of the modes available for for the cloudpathlib file cache.\n\n    Attributes:\n        persistent (str): Cache is not removed by `cloudpathlib`.\n        tmp_dir (str): Cache is stored in a\n            [`TemporaryDirectory`](https://docs.python.org/3/library/tempfile.html#tempfile.TemporaryDirectory)\n            which is removed when the Client object is garbage collected (or by the OS at some point if not).\n        cloudpath_object (str): Cache for a `CloudPath` object is removed when `__del__` for that object is\n            called by Python garbage collection.\n        close_file (str): Cache for a `CloudPath` file is removed as soon as the file is closed. Note: you must\n            use `CloudPath.open` whenever opening the file for this method to function.\n\n    Modes can be set by passing them to the Client or by setting the `CLOUDPATHLIB_FILE_CACHE_MODE`\n    environment variable.\n\n    For more detail, see the [caching documentation page](../../caching).\n    \"\"\"\n\n    persistent = \"persistent\"  # cache stays as long as dir on OS does\n    tmp_dir = \"tmp_dir\"  # DEFAULT: handled by deleting client, Python, or OS (usually on machine restart)\n    cloudpath_object = \"cloudpath_object\"  # __del__ called on the CloudPath object\n    close_file = \"close_file\"  # cache is cleared when file is closed\n\n    @classmethod\n    def from_environment(cls) -&gt; Optional[\"FileCacheMode\"]:\n        \"\"\"Parses the environment variable `CLOUDPATHLIB_FILE_CACHE_MODE` into\n        an instance of this Enum.\n\n        Returns:\n            FileCacheMode enum value if the env var is defined, else None.\n        \"\"\"\n\n        env_string = os.environ.get(\"CLOUDPATHLIB_FILE_CACHE_MODE\", \"\").lower()\n        env_string_typo = os.environ.get(\"CLOUPATHLIB_FILE_CACHE_MODE\", \"\").lower()\n\n        if env_string_typo:\n            warnings.warn(\n                \"envvar CLOUPATHLIB_FILE_CACHE_MODE has been renamed to \"\n                \"CLOUDPATHLIB_FILE_CACHE_MODE. Reading from the old value \"\n                \"will become deprecated in version 0.20.0\",\n                DeprecationWarning,\n            )\n\n        if env_string and env_string_typo and env_string != env_string_typo:\n            warnings.warn(\n                \"CLOUDPATHLIB_FILE_CACHE_MODE and CLOUPATHLIB_FILE_CACHE_MODE \"\n                \"envvars set to different values. Disregarding old value and \"\n                f\"using CLOUDPATHLIB_FILE_CACHE_MODE = {env_string}\",\n                RuntimeWarning,\n            )\n\n        env_string = env_string or env_string_typo\n        if not env_string:\n            return None\n        else:\n            return cls(env_string)\n</code></pre>"},{"location":"api-reference/enums/#cloudpathlib.enums.FileCacheMode.close_file","title":"<code>close_file</code>","text":""},{"location":"api-reference/enums/#cloudpathlib.enums.FileCacheMode.cloudpath_object","title":"<code>cloudpath_object</code>","text":""},{"location":"api-reference/enums/#cloudpathlib.enums.FileCacheMode.persistent","title":"<code>persistent</code>","text":""},{"location":"api-reference/enums/#cloudpathlib.enums.FileCacheMode.tmp_dir","title":"<code>tmp_dir</code>","text":""},{"location":"api-reference/exceptions/","title":"cloudpathlib.exceptions","text":"<p>This module contains all custom exceptions in the <code>cloudpathlib</code> library. All exceptions subclass the <code>CloudPathException</code> base exception to facilitate catching any exception from this library.</p>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions-classes","title":"Classes","text":""},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.AnyPathTypeError","title":"<code> AnyPathTypeError            (CloudPathException, TypeError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class AnyPathTypeError(CloudPathException, TypeError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.ClientMismatchError","title":"<code> ClientMismatchError            (CloudPathException, ValueError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class ClientMismatchError(CloudPathException, ValueError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.CloudPathException","title":"<code> CloudPathException            (Exception)         </code>","text":"<p>Base exception for all cloudpathlib custom exceptions.</p> Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class CloudPathException(Exception):\n    \"\"\"Base exception for all cloudpathlib custom exceptions.\"\"\"\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.CloudPathFileExistsError","title":"<code> CloudPathFileExistsError            (CloudPathException, FileExistsError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class CloudPathFileExistsError(CloudPathException, FileExistsError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.CloudPathIsADirectoryError","title":"<code> CloudPathIsADirectoryError            (CloudPathException, IsADirectoryError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class CloudPathIsADirectoryError(CloudPathException, IsADirectoryError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.CloudPathNotADirectoryError","title":"<code> CloudPathNotADirectoryError            (CloudPathException, NotADirectoryError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class CloudPathNotADirectoryError(CloudPathException, NotADirectoryError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.CloudPathNotExistsError","title":"<code> CloudPathNotExistsError            (CloudPathException)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class CloudPathNotExistsError(CloudPathException):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.CloudPathNotImplementedError","title":"<code> CloudPathNotImplementedError            (CloudPathException, NotImplementedError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class CloudPathNotImplementedError(CloudPathException, NotImplementedError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.DirectoryNotEmptyError","title":"<code> DirectoryNotEmptyError            (CloudPathException)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class DirectoryNotEmptyError(CloudPathException):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.IncompleteImplementationError","title":"<code> IncompleteImplementationError            (CloudPathException, NotImplementedError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class IncompleteImplementationError(CloudPathException, NotImplementedError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.InvalidConfigurationException","title":"<code> InvalidConfigurationException            (CloudPathException, ValueError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class InvalidConfigurationException(CloudPathException, ValueError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.InvalidPrefixError","title":"<code> InvalidPrefixError            (CloudPathException, ValueError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class InvalidPrefixError(CloudPathException, ValueError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.MissingCredentialsError","title":"<code> MissingCredentialsError            (CloudPathException)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class MissingCredentialsError(CloudPathException):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.MissingDependenciesError","title":"<code> MissingDependenciesError            (CloudPathException, ModuleNotFoundError)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class MissingDependenciesError(CloudPathException, ModuleNotFoundError):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.NoStatError","title":"<code> NoStatError            (CloudPathException)         </code>","text":"<p>Used if stats cannot be retrieved; e.g., file does not exist or for some backends path is a directory (which doesn't have stats available).</p> Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class NoStatError(CloudPathException):\n    \"\"\"Used if stats cannot be retrieved; e.g., file does not exist\n    or for some backends path is a directory (which doesn't have\n    stats available).\n    \"\"\"\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.OverwriteDirtyFileError","title":"<code> OverwriteDirtyFileError            (CloudPathException)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class OverwriteDirtyFileError(CloudPathException):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.OverwriteNewerCloudError","title":"<code> OverwriteNewerCloudError            (CloudPathException)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class OverwriteNewerCloudError(CloudPathException):\n    pass\n</code></pre>"},{"location":"api-reference/exceptions/#cloudpathlib.exceptions.OverwriteNewerLocalError","title":"<code> OverwriteNewerLocalError            (CloudPathException)         </code>","text":"Source code in <code>cloudpathlib/exceptions.py</code> <pre><code>class OverwriteNewerLocalError(CloudPathException):\n    pass\n</code></pre>"},{"location":"api-reference/gsclient/","title":"cloudpathlib.GSClient","text":"<p>Client class for Google Cloud Storage which handles authentication with GCP for <code>GSPath</code> instances. See documentation for the <code>__init__</code> method for detailed authentication options.</p> Source code in <code>cloudpathlib/gs/gsclient.py</code> <pre><code>class GSClient(Client):\n    \"\"\"Client class for Google Cloud Storage which handles authentication with GCP for\n    [`GSPath`](../gspath/) instances. See documentation for the\n    [`__init__` method][cloudpathlib.gs.gsclient.GSClient.__init__] for detailed authentication\n    options.\n    \"\"\"\n\n    def __init__(\n        self,\n        application_credentials: Optional[Union[str, os.PathLike]] = None,\n        credentials: Optional[\"Credentials\"] = None,\n        project: Optional[str] = None,\n        storage_client: Optional[\"StorageClient\"] = None,\n        file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n        local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n        content_type_method: Optional[Callable] = mimetypes.guess_type,\n        download_chunks_concurrently_kwargs: Optional[Dict[str, Any]] = None,\n    ):\n        \"\"\"Class constructor. Sets up a [`Storage\n        Client`](https://googleapis.dev/python/storage/latest/client.html).\n        Supports the following authentication methods of `Storage Client`.\n\n        - Environment variable `\"GOOGLE_APPLICATION_CREDENTIALS\"` containing a\n          path to a JSON credentials file for a Google service account. See\n          [Authenticating as a Service\n          Account](https://cloud.google.com/docs/authentication/production).\n        - File path to a JSON credentials file for a Google service account.\n        - OAuth2 Credentials object and a project name.\n        - Instantiated and already authenticated `Storage Client`.\n\n        If multiple methods are used, priority order is reverse of list above\n        (later in list takes priority). If no authentication methods are used,\n        then the client will be instantiated as anonymous, which will only have\n        access to public buckets.\n\n        Args:\n            application_credentials (Optional[Union[str, os.PathLike]]): Path to Google service\n                account credentials file.\n            credentials (Optional[Credentials]): The OAuth2 Credentials to use for this client.\n                See documentation for [`StorageClient`](\n                https://googleapis.dev/python/storage/latest/client.html).\n            project (Optional[str]): The project which the client acts on behalf of. See\n                documentation for [`StorageClient`](\n                https://googleapis.dev/python/storage/latest/client.html).\n            storage_client (Optional[StorageClient]): Instantiated [`StorageClient`](\n                https://googleapis.dev/python/storage/latest/client.html).\n            file_cache_mode (Optional[Union[str, FileCacheMode]]): How often to clear the file cache; see\n                [the caching docs](https://cloudpathlib.drivendata.org/stable/caching/) for more information\n                about the options in cloudpathlib.eums.FileCacheMode.\n            local_cache_dir (Optional[Union[str, os.PathLike]]): Path to directory to use as cache\n                for downloaded files. If None, will use a temporary directory. Default can be set with\n                the `CLOUDPATHLIB_LOCAL_CACHE_DIR` environment variable.\n            content_type_method (Optional[Callable]): Function to call to guess media type (mimetype) when\n                writing a file to the cloud. Defaults to `mimetypes.guess_type`. Must return a tuple (content type, content encoding).\n            download_chunks_concurrently_kwargs (Optional[Dict[str, Any]]): Keyword arguments to pass to\n                [`download_chunks_concurrently`](https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.transfer_manager#google_cloud_storage_transfer_manager_download_chunks_concurrently)\n                for sliced parallel downloads; Only available in `google-cloud-storage` version 2.7.0 or later, otherwise ignored and a warning is emitted.\n        \"\"\"\n        if application_credentials is None:\n            application_credentials = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n\n        if storage_client is not None:\n            self.client = storage_client\n        elif credentials is not None:\n            self.client = StorageClient(credentials=credentials, project=project)\n        elif application_credentials is not None:\n            self.client = StorageClient.from_service_account_json(application_credentials)\n        else:\n            try:\n                self.client = StorageClient()\n            except DefaultCredentialsError:\n                self.client = StorageClient.create_anonymous_client()\n\n        self.download_chunks_concurrently_kwargs = download_chunks_concurrently_kwargs\n\n        super().__init__(\n            local_cache_dir=local_cache_dir,\n            content_type_method=content_type_method,\n            file_cache_mode=file_cache_mode,\n        )\n\n    def _get_metadata(self, cloud_path: GSPath) -&gt; Optional[Dict[str, Any]]:\n        bucket = self.client.bucket(cloud_path.bucket)\n        blob = bucket.get_blob(cloud_path.blob)\n\n        if blob is None:\n            return None\n        else:\n            return {\n                \"etag\": blob.etag,\n                \"size\": blob.size,\n                \"updated\": blob.updated,\n                \"content_type\": blob.content_type,\n            }\n\n    def _download_file(self, cloud_path: GSPath, local_path: Union[str, os.PathLike]) -&gt; Path:\n        bucket = self.client.bucket(cloud_path.bucket)\n        blob = bucket.get_blob(cloud_path.blob)\n\n        local_path = Path(local_path)\n\n        if transfer_manager is not None and self.download_chunks_concurrently_kwargs is not None:\n            transfer_manager.download_chunks_concurrently(\n                blob, local_path, **self.download_chunks_concurrently_kwargs\n            )\n        else:\n            if transfer_manager is None and self.download_chunks_concurrently_kwargs is not None:\n                warnings.warn(\n                    \"Ignoring `download_chunks_concurrently_kwargs` for version of google-cloud-storage that does not support them (&lt;2.7.0).\"\n                )\n\n            blob.download_to_filename(local_path)\n\n        return local_path\n\n    def _is_file_or_dir(self, cloud_path: GSPath) -&gt; Optional[str]:\n        # short-circuit the root-level bucket\n        if not cloud_path.blob:\n            return \"dir\"\n\n        bucket = self.client.bucket(cloud_path.bucket)\n        blob = bucket.get_blob(cloud_path.blob)\n\n        if blob is not None:\n            return \"file\"\n        else:\n            prefix = cloud_path.blob\n            if prefix and not prefix.endswith(\"/\"):\n                prefix += \"/\"\n\n            # not a file, see if it is a directory\n            f = bucket.list_blobs(max_results=1, prefix=prefix)\n\n            # at least one key with the prefix of the directory\n            if bool(list(f)):\n                return \"dir\"\n            else:\n                return None\n\n    def _exists(self, cloud_path: GSPath) -&gt; bool:\n        # short-circuit the root-level bucket\n        if not cloud_path.blob:\n            return self.client.bucket(cloud_path.bucket).exists()\n\n        return self._is_file_or_dir(cloud_path) in [\"file\", \"dir\"]\n\n    def _list_dir(self, cloud_path: GSPath, recursive=False) -&gt; Iterable[Tuple[GSPath, bool]]:\n        # shortcut if listing all available buckets\n        if not cloud_path.bucket:\n            if recursive:\n                raise NotImplementedError(\n                    \"Cannot recursively list all buckets and contents; you can get all the buckets then recursively list each separately.\"\n                )\n\n            yield from (\n                (self.CloudPath(f\"gs://{str(b)}\"), True) for b in self.client.list_buckets()\n            )\n            return\n\n        bucket = self.client.bucket(cloud_path.bucket)\n\n        prefix = cloud_path.blob\n        if prefix and not prefix.endswith(\"/\"):\n            prefix += \"/\"\n        if recursive:\n            yielded_dirs = set()\n            for o in bucket.list_blobs(prefix=prefix):\n                # get directory from this path\n                for parent in PurePosixPath(o.name[len(prefix) :]).parents:\n                    # if we haven't surfaced this directory already\n                    if parent not in yielded_dirs and str(parent) != \".\":\n                        yield (\n                            self.CloudPath(f\"gs://{cloud_path.bucket}/{prefix}{parent}\"),\n                            True,  # is a directory\n                        )\n                        yielded_dirs.add(parent)\n                yield (self.CloudPath(f\"gs://{cloud_path.bucket}/{o.name}\"), False)  # is a file\n        else:\n            iterator = bucket.list_blobs(delimiter=\"/\", prefix=prefix)\n\n            # files must be iterated first for `.prefixes` to be populated:\n            #   see: https://github.com/googleapis/python-storage/issues/863\n            for file in iterator:\n                yield (\n                    self.CloudPath(f\"gs://{cloud_path.bucket}/{file.name}\"),\n                    False,  # is a file\n                )\n\n            for directory in iterator.prefixes:\n                yield (\n                    self.CloudPath(f\"gs://{cloud_path.bucket}/{directory}\"),\n                    True,  # is a directory\n                )\n\n    def _move_file(self, src: GSPath, dst: GSPath, remove_src: bool = True) -&gt; GSPath:\n        # just a touch, so \"REPLACE\" metadata\n        if src == dst:\n            bucket = self.client.bucket(src.bucket)\n            blob = bucket.get_blob(src.blob)\n\n            # See https://github.com/googleapis/google-cloud-python/issues/1185#issuecomment-431537214\n            if blob.metadata is None:\n                blob.metadata = {\"updated\": datetime.utcnow()}\n            else:\n                blob.metadata[\"updated\"] = datetime.utcnow()\n            blob.patch()\n\n        else:\n            src_bucket = self.client.bucket(src.bucket)\n            dst_bucket = self.client.bucket(dst.bucket)\n\n            src_blob = src_bucket.get_blob(src.blob)\n            src_bucket.copy_blob(src_blob, dst_bucket, dst.blob)\n\n            if remove_src:\n                src_blob.delete()\n\n        return dst\n\n    def _remove(self, cloud_path: GSPath, missing_ok: bool = True) -&gt; None:\n        file_or_dir = self._is_file_or_dir(cloud_path)\n        if file_or_dir == \"dir\":\n            blobs = [\n                b.blob for b, is_dir in self._list_dir(cloud_path, recursive=True) if not is_dir\n            ]\n            bucket = self.client.bucket(cloud_path.bucket)\n            for blob in blobs:\n                bucket.get_blob(blob).delete()\n        elif file_or_dir == \"file\":\n            bucket = self.client.bucket(cloud_path.bucket)\n            bucket.get_blob(cloud_path.blob).delete()\n        else:\n            # Does not exist\n            if not missing_ok:\n                raise FileNotFoundError(f\"File does not exist: {cloud_path}\")\n\n    def _upload_file(self, local_path: Union[str, os.PathLike], cloud_path: GSPath) -&gt; GSPath:\n        bucket = self.client.bucket(cloud_path.bucket)\n        blob = bucket.blob(cloud_path.blob)\n\n        extra_args = {}\n        if self.content_type_method is not None:\n            content_type, _ = self.content_type_method(str(local_path))\n            extra_args[\"content_type\"] = content_type\n\n        blob.upload_from_filename(str(local_path), **extra_args)\n        return cloud_path\n\n    def _get_public_url(self, cloud_path: GSPath) -&gt; str:\n        bucket = self.client.get_bucket(cloud_path.bucket)\n        blob = bucket.blob(cloud_path.blob)\n        return blob.public_url\n\n    def _generate_presigned_url(self, cloud_path: GSPath, expire_seconds: int = 60 * 60) -&gt; str:\n        bucket = self.client.get_bucket(cloud_path.bucket)\n        blob = bucket.blob(cloud_path.blob)\n        url = blob.generate_signed_url(\n            version=\"v4\", expiration=timedelta(seconds=expire_seconds), method=\"GET\"\n        )\n        return url\n</code></pre>"},{"location":"api-reference/gsclient/#cloudpathlib.gs.gsclient.GSClient-methods","title":"Methods","text":""},{"location":"api-reference/gsclient/#cloudpathlib.gs.gsclient.GSClient.GSPath","title":"<code>GSPath(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code>","text":"Source code in <code>cloudpathlib/gs/gsclient.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre>"},{"location":"api-reference/gsclient/#cloudpathlib.gs.gsclient.GSClient.__init__","title":"<code>__init__(self, application_credentials: Union[str, os.PathLike] = None, credentials: Optional[Credentials] = None, project: Optional[str] = None, storage_client: Optional[StorageClient] = None, file_cache_mode: Union[str, cloudpathlib.enums.FileCacheMode] = None, local_cache_dir: Union[str, os.PathLike] = None, content_type_method: Optional[Callable] = &lt;function guess_type at 0x7f08e5d20790&gt;, download_chunks_concurrently_kwargs: Optional[Dict[str, Any]] = None)</code>  <code>special</code>","text":"<p>Class constructor. Sets up a <code>Storage Client</code>. Supports the following authentication methods of <code>Storage Client</code>.</p> <ul> <li>Environment variable <code>\"GOOGLE_APPLICATION_CREDENTIALS\"</code> containing a   path to a JSON credentials file for a Google service account. See   Authenticating as a Service   Account.</li> <li>File path to a JSON credentials file for a Google service account.</li> <li>OAuth2 Credentials object and a project name.</li> <li>Instantiated and already authenticated <code>Storage Client</code>.</li> </ul> <p>If multiple methods are used, priority order is reverse of list above (later in list takes priority). If no authentication methods are used, then the client will be instantiated as anonymous, which will only have access to public buckets.</p> <p>Parameters:</p> Name Type Description Default <code>application_credentials</code> <code>Optional[Union[str, os.PathLike]]</code> <p>Path to Google service account credentials file.</p> <code>None</code> <code>credentials</code> <code>Optional[Credentials]</code> <p>The OAuth2 Credentials to use for this client. See documentation for <code>StorageClient</code>.</p> <code>None</code> <code>project</code> <code>Optional[str]</code> <p>The project which the client acts on behalf of. See documentation for <code>StorageClient</code>.</p> <code>None</code> <code>storage_client</code> <code>Optional[StorageClient]</code> <p>Instantiated <code>StorageClient</code>.</p> <code>None</code> <code>file_cache_mode</code> <code>Optional[Union[str, FileCacheMode]]</code> <p>How often to clear the file cache; see the caching docs for more information about the options in cloudpathlib.eums.FileCacheMode.</p> <code>None</code> <code>local_cache_dir</code> <code>Optional[Union[str, os.PathLike]]</code> <p>Path to directory to use as cache for downloaded files. If None, will use a temporary directory. Default can be set with the <code>CLOUDPATHLIB_LOCAL_CACHE_DIR</code> environment variable.</p> <code>None</code> <code>content_type_method</code> <code>Optional[Callable]</code> <p>Function to call to guess media type (mimetype) when writing a file to the cloud. Defaults to <code>mimetypes.guess_type</code>. Must return a tuple (content type, content encoding).</p> <code>&lt;function guess_type at 0x7f08e5d20790&gt;</code> <code>download_chunks_concurrently_kwargs</code> <code>Optional[Dict[str, Any]]</code> <p>Keyword arguments to pass to <code>download_chunks_concurrently</code> for sliced parallel downloads; Only available in <code>google-cloud-storage</code> version 2.7.0 or later, otherwise ignored and a warning is emitted.</p> <code>None</code> Source code in <code>cloudpathlib/gs/gsclient.py</code> <pre><code>def __init__(\n    self,\n    application_credentials: Optional[Union[str, os.PathLike]] = None,\n    credentials: Optional[\"Credentials\"] = None,\n    project: Optional[str] = None,\n    storage_client: Optional[\"StorageClient\"] = None,\n    file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n    local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n    content_type_method: Optional[Callable] = mimetypes.guess_type,\n    download_chunks_concurrently_kwargs: Optional[Dict[str, Any]] = None,\n):\n    \"\"\"Class constructor. Sets up a [`Storage\n    Client`](https://googleapis.dev/python/storage/latest/client.html).\n    Supports the following authentication methods of `Storage Client`.\n\n    - Environment variable `\"GOOGLE_APPLICATION_CREDENTIALS\"` containing a\n      path to a JSON credentials file for a Google service account. See\n      [Authenticating as a Service\n      Account](https://cloud.google.com/docs/authentication/production).\n    - File path to a JSON credentials file for a Google service account.\n    - OAuth2 Credentials object and a project name.\n    - Instantiated and already authenticated `Storage Client`.\n\n    If multiple methods are used, priority order is reverse of list above\n    (later in list takes priority). If no authentication methods are used,\n    then the client will be instantiated as anonymous, which will only have\n    access to public buckets.\n\n    Args:\n        application_credentials (Optional[Union[str, os.PathLike]]): Path to Google service\n            account credentials file.\n        credentials (Optional[Credentials]): The OAuth2 Credentials to use for this client.\n            See documentation for [`StorageClient`](\n            https://googleapis.dev/python/storage/latest/client.html).\n        project (Optional[str]): The project which the client acts on behalf of. See\n            documentation for [`StorageClient`](\n            https://googleapis.dev/python/storage/latest/client.html).\n        storage_client (Optional[StorageClient]): Instantiated [`StorageClient`](\n            https://googleapis.dev/python/storage/latest/client.html).\n        file_cache_mode (Optional[Union[str, FileCacheMode]]): How often to clear the file cache; see\n            [the caching docs](https://cloudpathlib.drivendata.org/stable/caching/) for more information\n            about the options in cloudpathlib.eums.FileCacheMode.\n        local_cache_dir (Optional[Union[str, os.PathLike]]): Path to directory to use as cache\n            for downloaded files. If None, will use a temporary directory. Default can be set with\n            the `CLOUDPATHLIB_LOCAL_CACHE_DIR` environment variable.\n        content_type_method (Optional[Callable]): Function to call to guess media type (mimetype) when\n            writing a file to the cloud. Defaults to `mimetypes.guess_type`. Must return a tuple (content type, content encoding).\n        download_chunks_concurrently_kwargs (Optional[Dict[str, Any]]): Keyword arguments to pass to\n            [`download_chunks_concurrently`](https://cloud.google.com/python/docs/reference/storage/latest/google.cloud.storage.transfer_manager#google_cloud_storage_transfer_manager_download_chunks_concurrently)\n            for sliced parallel downloads; Only available in `google-cloud-storage` version 2.7.0 or later, otherwise ignored and a warning is emitted.\n    \"\"\"\n    if application_credentials is None:\n        application_credentials = os.getenv(\"GOOGLE_APPLICATION_CREDENTIALS\")\n\n    if storage_client is not None:\n        self.client = storage_client\n    elif credentials is not None:\n        self.client = StorageClient(credentials=credentials, project=project)\n    elif application_credentials is not None:\n        self.client = StorageClient.from_service_account_json(application_credentials)\n    else:\n        try:\n            self.client = StorageClient()\n        except DefaultCredentialsError:\n            self.client = StorageClient.create_anonymous_client()\n\n    self.download_chunks_concurrently_kwargs = download_chunks_concurrently_kwargs\n\n    super().__init__(\n        local_cache_dir=local_cache_dir,\n        content_type_method=content_type_method,\n        file_cache_mode=file_cache_mode,\n    )\n</code></pre>"},{"location":"api-reference/gspath/","title":"cloudpathlib.GSPath","text":"<p>Class for representing and operating on Google Cloud Storage URIs, in the style of the Python standard library's <code>pathlib</code> module. Instances represent a path in GS with filesystem path semantics, and convenient methods allow for basic operations like joining, reading, writing, iterating over contents, etc. This class almost entirely mimics the <code>pathlib.Path</code> interface, so most familiar properties and methods should be available and behave in the expected way.</p> <p>The <code>GSClient</code> class handles authentication with GCP. If a client instance is not explicitly specified on <code>GSPath</code> instantiation, a default client is used. See <code>GSClient</code>'s documentation for more details.</p> Source code in <code>cloudpathlib/gs/gspath.py</code> <pre><code>class GSPath(CloudPath):\n    \"\"\"Class for representing and operating on Google Cloud Storage URIs, in the style of the\n    Python standard library's [`pathlib` module](https://docs.python.org/3/library/pathlib.html).\n    Instances represent a path in GS with filesystem path semantics, and convenient methods allow\n    for basic operations like joining, reading, writing, iterating over contents, etc. This class\n    almost entirely mimics the [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)\n    interface, so most familiar properties and methods should be available and behave in the\n    expected way.\n\n    The [`GSClient`](../gsclient/) class handles authentication with GCP. If a client instance is\n    not explicitly specified on `GSPath` instantiation, a default client is used. See `GSClient`'s\n    documentation for more details.\n    \"\"\"\n\n    cloud_prefix: str = \"gs://\"\n    client: \"GSClient\"\n\n    @property\n    def drive(self) -&gt; str:\n        return self.bucket\n\n    def is_dir(self) -&gt; bool:\n        return self.client._is_file_or_dir(self) == \"dir\"\n\n    def is_file(self) -&gt; bool:\n        return self.client._is_file_or_dir(self) == \"file\"\n\n    def mkdir(self, parents=False, exist_ok=False):\n        # not possible to make empty directory on cloud storage\n        pass\n\n    def touch(self, exist_ok: bool = True):\n        if self.exists():\n            if not exist_ok:\n                raise FileExistsError(f\"File exists: {self}\")\n            self.client._move_file(self, self)\n        else:\n            tf = TemporaryDirectory()\n            p = Path(tf.name) / \"empty\"\n            p.touch()\n\n            self.client._upload_file(p, self)\n\n            tf.cleanup()\n\n    def stat(self):\n        meta = self.client._get_metadata(self)\n        if meta is None:\n            raise NoStatError(\n                f\"No stats available for {self}; it may be a directory or not exist.\"\n            )\n\n        try:\n            mtime = meta[\"updated\"].timestamp()\n        except KeyError:\n            mtime = 0\n\n        return os.stat_result(\n            (\n                None,  # mode\n                None,  # ino\n                self.cloud_prefix,  # dev,\n                None,  # nlink,\n                None,  # uid,\n                None,  # gid,\n                meta.get(\"size\", 0),  # size,\n                None,  # atime,\n                mtime,  # mtime,\n                None,  # ctime,\n            )\n        )\n\n    @property\n    def bucket(self) -&gt; str:\n        return self._no_prefix.split(\"/\", 1)[0]\n\n    @property\n    def blob(self) -&gt; str:\n        key = self._no_prefix_no_drive\n\n        # key should never have starting slash for\n        # use with google-cloud-storage, etc.\n        if key.startswith(\"/\"):\n            key = key[1:]\n\n        return key\n\n    @property\n    def etag(self):\n        return self.client._get_metadata(self).get(\"etag\")\n</code></pre>"},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath-attributes","title":"Attributes","text":""},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.blob","title":"<code>blob: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.bucket","title":"<code>bucket: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.cloud_prefix","title":"<code>cloud_prefix: str</code>","text":""},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.drive","title":"<code>drive: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.etag","title":"<code>etag</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath-methods","title":"Methods","text":""},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.is_dir","title":"<code>is_dir(self) -&gt; bool</code>","text":"<p>Whether this path is a directory.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/gs/gspath.py</code> <pre><code>def is_dir(self) -&gt; bool:\n    return self.client._is_file_or_dir(self) == \"dir\"\n</code></pre>"},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.is_file","title":"<code>is_file(self) -&gt; bool</code>","text":"<p>Whether this path is a regular file (also True for symlinks pointing to regular files).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/gs/gspath.py</code> <pre><code>def is_file(self) -&gt; bool:\n    return self.client._is_file_or_dir(self) == \"file\"\n</code></pre>"},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.mkdir","title":"<code>mkdir(self, parents = False, exist_ok = False)</code>","text":"<p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/gs/gspath.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    # not possible to make empty directory on cloud storage\n    pass\n</code></pre>"},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.stat","title":"<code>stat(self)</code>","text":"<p>Return the result of the stat() system call on this path, like os.stat() does.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/gs/gspath.py</code> <pre><code>def stat(self):\n    meta = self.client._get_metadata(self)\n    if meta is None:\n        raise NoStatError(\n            f\"No stats available for {self}; it may be a directory or not exist.\"\n        )\n\n    try:\n        mtime = meta[\"updated\"].timestamp()\n    except KeyError:\n        mtime = 0\n\n    return os.stat_result(\n        (\n            None,  # mode\n            None,  # ino\n            self.cloud_prefix,  # dev,\n            None,  # nlink,\n            None,  # uid,\n            None,  # gid,\n            meta.get(\"size\", 0),  # size,\n            None,  # atime,\n            mtime,  # mtime,\n            None,  # ctime,\n        )\n    )\n</code></pre>"},{"location":"api-reference/gspath/#cloudpathlib.gs.gspath.GSPath.touch","title":"<code>touch(self, exist_ok: bool = True)</code>","text":"<p>Create this file with the given access mode, if it doesn't exist.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/gs/gspath.py</code> <pre><code>def touch(self, exist_ok: bool = True):\n    if self.exists():\n        if not exist_ok:\n            raise FileExistsError(f\"File exists: {self}\")\n        self.client._move_file(self, self)\n    else:\n        tf = TemporaryDirectory()\n        p = Path(tf.name) / \"empty\"\n        p.touch()\n\n        self.client._upload_file(p, self)\n\n        tf.cleanup()\n</code></pre>"},{"location":"api-reference/local/","title":"cloudpathlib.local","text":"<p>This module implements \"Local\" classes that mimic their associated <code>cloudpathlib</code> non-local counterparts but use the local filesystem in place of cloud storage. They can be used as drop-in replacements, with the intent that you can use them as mock or monkepatch substitutes in your tests. See \"Testing code that uses cloudpathlib\" for usage examples.</p>"},{"location":"api-reference/local/#cloudpathlib.local-modules","title":"Modules","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations","title":"<code>implementations</code>  <code>special</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations-modules","title":"Modules","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure","title":"<code>azure</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure-attributes","title":"Attributes","text":"<code>local_azure_blob_implementation</code> \u00b6 <p>Replacement for \"azure\" CloudImplementation meta object in cloudpathlib.implementation_registry</p>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure-classes","title":"Classes","text":"<code> LocalAzureBlobClient            (LocalClient)         </code> \u00b6 <p>Replacement for AzureBlobClient that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Source code in <code>cloudpathlib/local/implementations/azure.py</code> <pre><code>class LocalAzureBlobClient(LocalClient):\n    \"\"\"Replacement for AzureBlobClient that uses the local file system. Intended as a monkeypatch\n    substitute when writing tests.\n    \"\"\"\n\n    _cloud_meta = local_azure_blob_implementation\n\n    def __init__(self, *args, **kwargs):\n        cred_opts = [\n            kwargs.get(\"blob_service_client\", None),\n            kwargs.get(\"connection_string\", None),\n            kwargs.get(\"account_url\", None),\n            os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\", None),\n        ]\n        super().__init__(*args, **kwargs)\n\n        if all(opt is None for opt in cred_opts):\n            raise MissingCredentialsError(\n                \"AzureBlobClient does not support anonymous instantiation. \"\n                \"Credentials are required; see docs for options.\"\n            )\n</code></pre> <code>AzureBlobPath(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code> \u00b6 Source code in <code>cloudpathlib/local/implementations/azure.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre> <code>__init__(self, *args, **kwargs)</code> <code>special</code> \u00b6 Source code in <code>cloudpathlib/local/implementations/azure.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    cred_opts = [\n        kwargs.get(\"blob_service_client\", None),\n        kwargs.get(\"connection_string\", None),\n        kwargs.get(\"account_url\", None),\n        os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\", None),\n    ]\n    super().__init__(*args, **kwargs)\n\n    if all(opt is None for opt in cred_opts):\n        raise MissingCredentialsError(\n            \"AzureBlobClient does not support anonymous instantiation. \"\n            \"Credentials are required; see docs for options.\"\n        )\n</code></pre> <code> LocalAzureBlobPath            (LocalPath)         </code> \u00b6 <p>Replacement for AzureBlobPath that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Attributes\u00b6 <code>blob: str</code> <code>property</code> <code>readonly</code> \u00b6 <code>cloud_prefix: str</code> \u00b6 <code>container: str</code> <code>property</code> <code>readonly</code> \u00b6 <code>drive: str</code> <code>property</code> <code>readonly</code> \u00b6 <p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p> <code>etag</code> <code>property</code> <code>readonly</code> \u00b6 <code>md5: str</code> <code>property</code> <code>readonly</code> \u00b6 Methods\u00b6 <code>mkdir(self, parents = False, exist_ok = False)</code> \u00b6 <p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/implementations/azure.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    # not possible to make empty directory on blob storage\n    pass\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs","title":"<code>gs</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs-attributes","title":"Attributes","text":"<code>local_gs_implementation</code> \u00b6 <p>Replacement for \"gs\" CloudImplementation meta object in cloudpathlib.implementation_registry</p>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs-classes","title":"Classes","text":"<code> LocalGSClient            (LocalClient)         </code> \u00b6 <p>Replacement for GSClient that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Source code in <code>cloudpathlib/local/implementations/gs.py</code> <pre><code>class LocalGSClient(LocalClient):\n    \"\"\"Replacement for GSClient that uses the local file system. Intended as a monkeypatch\n    substitute when writing tests.\n    \"\"\"\n\n    _cloud_meta = local_gs_implementation\n</code></pre> <code>GSPath(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code> \u00b6 Source code in <code>cloudpathlib/local/implementations/gs.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre> <code> LocalGSPath            (LocalPath)         </code> \u00b6 <p>Replacement for GSPath that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Attributes\u00b6 <code>blob: str</code> <code>property</code> <code>readonly</code> \u00b6 <code>bucket: str</code> <code>property</code> <code>readonly</code> \u00b6 <code>cloud_prefix: str</code> \u00b6 <code>drive: str</code> <code>property</code> <code>readonly</code> \u00b6 <p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p> <code>etag</code> <code>property</code> <code>readonly</code> \u00b6 Methods\u00b6 <code>mkdir(self, parents = False, exist_ok = False)</code> \u00b6 <p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/implementations/gs.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    # not possible to make empty directory on gs\n    pass\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3","title":"<code>s3</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3-attributes","title":"Attributes","text":"<code>local_s3_implementation</code> \u00b6 <p>Replacement for \"s3\" CloudImplementation meta object in cloudpathlib.implementation_registry</p>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3-classes","title":"Classes","text":"<code> LocalS3Client            (LocalClient)         </code> \u00b6 <p>Replacement for S3Client that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Source code in <code>cloudpathlib/local/implementations/s3.py</code> <pre><code>class LocalS3Client(LocalClient):\n    \"\"\"Replacement for S3Client that uses the local file system. Intended as a monkeypatch\n    substitute when writing tests.\n    \"\"\"\n\n    _cloud_meta = local_s3_implementation\n</code></pre> <code>S3Path(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code> \u00b6 Source code in <code>cloudpathlib/local/implementations/s3.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre> <code> LocalS3Path            (LocalPath)         </code> \u00b6 <p>Replacement for S3Path that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Attributes\u00b6 <code>bucket: str</code> <code>property</code> <code>readonly</code> \u00b6 <code>cloud_prefix: str</code> \u00b6 <code>drive: str</code> <code>property</code> <code>readonly</code> \u00b6 <p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p> <code>etag</code> <code>property</code> <code>readonly</code> \u00b6 <code>key: str</code> <code>property</code> <code>readonly</code> \u00b6 Methods\u00b6 <code>mkdir(self, parents = False, exist_ok = False)</code> \u00b6 <p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/implementations/s3.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    # not possible to make empty directory on s3\n    pass\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localclient","title":"<code>localclient</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.localclient-classes","title":"Classes","text":""},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient","title":"<code> LocalClient            (Client)         </code>","text":"<p>Abstract client for accessing objects the local filesystem. Subclasses are as a monkeypatch substitutes for normal Client subclasses when writing tests.</p> Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>class LocalClient(Client):\n    \"\"\"Abstract client for accessing objects the local filesystem. Subclasses are as a monkeypatch\n    substitutes for normal Client subclasses when writing tests.\"\"\"\n\n    # Class-level variable to tracks the default storage directory for this client class\n    # that is used if a client is instantiated without a directory being explicitly provided\n    _default_storage_temp_dir: ClassVar[Optional[TemporaryDirectory]] = None\n\n    # Instance-level variable that tracks the local storage directory for this client\n    _local_storage_dir: Optional[Union[str, os.PathLike]]\n\n    def __init__(\n        self,\n        *args,\n        local_storage_dir: Optional[Union[str, os.PathLike]] = None,\n        file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n        local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n        content_type_method: Optional[Callable] = mimetypes.guess_type,\n        **kwargs,\n    ):\n        self._local_storage_dir = local_storage_dir\n\n        super().__init__(\n            local_cache_dir=local_cache_dir,\n            content_type_method=content_type_method,\n            file_cache_mode=file_cache_mode,\n        )\n\n    @classmethod\n    def get_default_storage_dir(cls) -&gt; Path:\n        \"\"\"Return the default storage directory for this client class. This is used if a client\n        is instantiated without a storage directory being explicitly provided. In this usage,\n        \"storage\" refers to the local storage that simulates the cloud.\n        \"\"\"\n        if cls._default_storage_temp_dir is None:\n            cls._default_storage_temp_dir = TemporaryDirectory()\n            _temp_dirs_to_clean.append(cls._default_storage_temp_dir)\n        return Path(cls._default_storage_temp_dir.name)\n\n    @classmethod\n    def reset_default_storage_dir(cls) -&gt; Path:\n        \"\"\"Reset the default storage directly. This tears down and recreates the directory used by\n        default for this client class when instantiating a client without explicitly providing\n        a storage directory. In this usage, \"storage\" refers to the local storage that simulates\n        the cloud.\n        \"\"\"\n        cls._default_storage_temp_dir = None\n        return cls.get_default_storage_dir()\n\n    @property\n    def local_storage_dir(self) -&gt; Path:\n        \"\"\"The local directory where files are stored for this client. This storage directory is\n        the one that simulates the cloud. If no storage directory was provided on instantiating the\n        client, the default storage directory for this client class is used.\n        \"\"\"\n        if self._local_storage_dir is None:\n            # No explicit local storage was provided on instantiating the client.\n            # Use the default storage directory for this class.\n            return self.get_default_storage_dir()\n        return Path(self._local_storage_dir)\n\n    def _cloud_path_to_local(self, cloud_path: \"LocalPath\") -&gt; Path:\n        return self.local_storage_dir / cloud_path._no_prefix\n\n    def _local_to_cloud_path(self, local_path: Union[str, os.PathLike]) -&gt; \"LocalPath\":\n        local_path = Path(local_path)\n        cloud_prefix = self._cloud_meta.path_class.cloud_prefix\n        return self.CloudPath(\n            f\"{cloud_prefix}{PurePosixPath(local_path.relative_to(self.local_storage_dir))}\"\n        )\n\n    def _download_file(self, cloud_path: \"LocalPath\", local_path: Union[str, os.PathLike]) -&gt; Path:\n        local_path = Path(local_path)\n        local_path.parent.mkdir(exist_ok=True, parents=True)\n\n        try:\n            shutil.copyfile(self._cloud_path_to_local(cloud_path), local_path)\n        except FileNotFoundError:\n            # erroneous FileNotFoundError appears in tests sometimes; patiently insist on the parent directory existing\n            sleep(1.0)\n            local_path.parent.mkdir(exist_ok=True, parents=True)\n            sleep(1.0)\n\n            shutil.copyfile(self._cloud_path_to_local(cloud_path), local_path)\n\n        return local_path\n\n    def _exists(self, cloud_path: \"LocalPath\") -&gt; bool:\n        return self._cloud_path_to_local(cloud_path).exists()\n\n    def _is_dir(self, cloud_path: \"LocalPath\") -&gt; bool:\n        return self._cloud_path_to_local(cloud_path).is_dir()\n\n    def _is_file(self, cloud_path: \"LocalPath\") -&gt; bool:\n        return self._cloud_path_to_local(cloud_path).is_file()\n\n    def _list_dir(\n        self, cloud_path: \"LocalPath\", recursive=False\n    ) -&gt; Iterable[Tuple[\"LocalPath\", bool]]:\n        pattern = \"**/*\" if recursive else \"*\"\n        for obj in self._cloud_path_to_local(cloud_path).glob(pattern):\n            yield (self._local_to_cloud_path(obj), obj.is_dir())\n\n    def _md5(self, cloud_path: \"LocalPath\") -&gt; str:\n        return md5(self._cloud_path_to_local(cloud_path).read_bytes()).hexdigest()\n\n    def _move_file(\n        self, src: \"LocalPath\", dst: \"LocalPath\", remove_src: bool = True\n    ) -&gt; \"LocalPath\":\n        self._cloud_path_to_local(dst).parent.mkdir(exist_ok=True, parents=True)\n\n        if remove_src:\n            self._cloud_path_to_local(src).replace(self._cloud_path_to_local(dst))\n        else:\n            shutil.copy(self._cloud_path_to_local(src), self._cloud_path_to_local(dst))\n        return dst\n\n    def _remove(self, cloud_path: \"LocalPath\", missing_ok: bool = True) -&gt; None:\n        local_storage_path = self._cloud_path_to_local(cloud_path)\n        if not missing_ok and not local_storage_path.exists():\n            raise FileNotFoundError(f\"File does not exist: {cloud_path}\")\n\n        if local_storage_path.is_file():\n            local_storage_path.unlink()\n        elif local_storage_path.is_dir():\n            shutil.rmtree(local_storage_path)\n\n    def _stat(self, cloud_path: \"LocalPath\") -&gt; os.stat_result:\n        stat_result = self._cloud_path_to_local(cloud_path).stat()\n\n        return os.stat_result(\n            (  # type: ignore\n                None,  # type: ignore # mode\n                None,  # ino\n                cloud_path.cloud_prefix,  # dev,\n                None,  # nlink,\n                None,  # uid,\n                None,  # gid,\n                stat_result.st_size,  # size,\n                None,  # atime,\n                stat_result.st_mtime,  # mtime,\n                None,  # ctime,\n            )\n        )\n\n    def _touch(self, cloud_path: \"LocalPath\", exist_ok: bool = True) -&gt; None:\n        local_storage_path = self._cloud_path_to_local(cloud_path)\n        if local_storage_path.exists() and not exist_ok:\n            raise FileExistsError(f\"File exists: {cloud_path}\")\n        local_storage_path.parent.mkdir(exist_ok=True, parents=True)\n        local_storage_path.touch()\n\n    def _upload_file(\n        self, local_path: Union[str, os.PathLike], cloud_path: \"LocalPath\"\n    ) -&gt; \"LocalPath\":\n        dst = self._cloud_path_to_local(cloud_path)\n        dst.parent.mkdir(exist_ok=True, parents=True)\n        shutil.copy(local_path, dst)\n        return cloud_path\n\n    def _get_metadata(self, cloud_path: \"LocalPath\") -&gt; Dict:\n        # content_type is the only metadata we test currently\n        if self.content_type_method is None:\n            content_type_method = lambda x: (None, None)\n        else:\n            content_type_method = self.content_type_method\n\n        return {\n            \"content_type\": content_type_method(str(self._cloud_path_to_local(cloud_path)))[0],\n        }\n\n    def _get_public_url(self, cloud_path: \"LocalPath\") -&gt; str:\n        return cloud_path.as_uri()\n\n    def _generate_presigned_url(\n        self, cloud_path: \"LocalPath\", expire_seconds: int = 60 * 60\n    ) -&gt; str:\n        raise NotImplementedError(\"Cannot generate a presigned URL for a local path.\")\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient-attributes","title":"Attributes","text":"<code>local_storage_dir: Path</code> <code>property</code> <code>readonly</code> \u00b6 <p>The local directory where files are stored for this client. This storage directory is the one that simulates the cloud. If no storage directory was provided on instantiating the client, the default storage directory for this client class is used.</p>"},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient-methods","title":"Methods","text":"<code>__init__(self, *args, *, local_storage_dir: Union[str, os.PathLike] = None, file_cache_mode: Union[str, cloudpathlib.enums.FileCacheMode] = None, local_cache_dir: Union[str, os.PathLike] = None, content_type_method: Optional[Callable] = &lt;function guess_type at 0x7f08e5d20790&gt;, **kwargs)</code> <code>special</code> \u00b6 Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    local_storage_dir: Optional[Union[str, os.PathLike]] = None,\n    file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n    local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n    content_type_method: Optional[Callable] = mimetypes.guess_type,\n    **kwargs,\n):\n    self._local_storage_dir = local_storage_dir\n\n    super().__init__(\n        local_cache_dir=local_cache_dir,\n        content_type_method=content_type_method,\n        file_cache_mode=file_cache_mode,\n    )\n</code></pre> <code>get_default_storage_dir() -&gt; Path</code> <code>classmethod</code> \u00b6 <p>Return the default storage directory for this client class. This is used if a client is instantiated without a storage directory being explicitly provided. In this usage, \"storage\" refers to the local storage that simulates the cloud.</p> Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>@classmethod\ndef get_default_storage_dir(cls) -&gt; Path:\n    \"\"\"Return the default storage directory for this client class. This is used if a client\n    is instantiated without a storage directory being explicitly provided. In this usage,\n    \"storage\" refers to the local storage that simulates the cloud.\n    \"\"\"\n    if cls._default_storage_temp_dir is None:\n        cls._default_storage_temp_dir = TemporaryDirectory()\n        _temp_dirs_to_clean.append(cls._default_storage_temp_dir)\n    return Path(cls._default_storage_temp_dir.name)\n</code></pre> <code>reset_default_storage_dir() -&gt; Path</code> <code>classmethod</code> \u00b6 <p>Reset the default storage directly. This tears down and recreates the directory used by default for this client class when instantiating a client without explicitly providing a storage directory. In this usage, \"storage\" refers to the local storage that simulates the cloud.</p> Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>@classmethod\ndef reset_default_storage_dir(cls) -&gt; Path:\n    \"\"\"Reset the default storage directly. This tears down and recreates the directory used by\n    default for this client class when instantiating a client without explicitly providing\n    a storage directory. In this usage, \"storage\" refers to the local storage that simulates\n    the cloud.\n    \"\"\"\n    cls._default_storage_temp_dir = None\n    return cls.get_default_storage_dir()\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localclient.clean_temp_dirs","title":"<code>clean_temp_dirs()</code>","text":"Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>@atexit.register\ndef clean_temp_dirs():\n    for temp_dir in _temp_dirs_to_clean:\n        temp_dir.cleanup()\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localpath","title":"<code>localpath</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.localpath-classes","title":"Classes","text":""},{"location":"api-reference/local/#cloudpathlib.local.localpath.LocalPath","title":"<code> LocalPath            (CloudPath)         </code>","text":"<p>Abstract CloudPath for accessing objects the local filesystem. Subclasses are as a monkeypatch substitutes for normal CloudPath subclasses when writing tests.</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>class LocalPath(CloudPath):\n    \"\"\"Abstract CloudPath for accessing objects the local filesystem. Subclasses are as a\n    monkeypatch substitutes for normal CloudPath subclasses when writing tests.\"\"\"\n\n    client: \"LocalClient\"\n\n    def is_dir(self) -&gt; bool:\n        return self.client._is_dir(self)\n\n    def is_file(self) -&gt; bool:\n        return self.client._is_file(self)\n\n    def stat(self):\n        try:\n            meta = self.client._stat(self)\n        except FileNotFoundError:\n            raise NoStatError(\n                f\"No stats available for {self}; it may be a directory or not exist.\"\n            )\n        return meta\n\n    def touch(self, exist_ok: bool = True):\n        self.client._touch(self, exist_ok)\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localpath.LocalPath-methods","title":"Methods","text":"<code>is_dir(self) -&gt; bool</code> \u00b6 <p>Whether this path is a directory.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>def is_dir(self) -&gt; bool:\n    return self.client._is_dir(self)\n</code></pre> <code>is_file(self) -&gt; bool</code> \u00b6 <p>Whether this path is a regular file (also True for symlinks pointing to regular files).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>def is_file(self) -&gt; bool:\n    return self.client._is_file(self)\n</code></pre> <code>stat(self)</code> \u00b6 <p>Return the result of the stat() system call on this path, like os.stat() does.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>def stat(self):\n    try:\n        meta = self.client._stat(self)\n    except FileNotFoundError:\n        raise NoStatError(\n            f\"No stats available for {self}; it may be a directory or not exist.\"\n        )\n    return meta\n</code></pre> <code>touch(self, exist_ok: bool = True)</code> \u00b6 <p>Create this file with the given access mode, if it doesn't exist.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>def touch(self, exist_ok: bool = True):\n    self.client._touch(self, exist_ok)\n</code></pre>"},{"location":"api-reference/local/#attributes","title":"Attributes","text":""},{"location":"api-reference/local/#classes","title":"Classes","text":"<p>Replacement for AzureBlobClient that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Source code in <code>cloudpathlib/local/implementations/azure.py</code> <pre><code>class LocalAzureBlobClient(LocalClient):\n    \"\"\"Replacement for AzureBlobClient that uses the local file system. Intended as a monkeypatch\n    substitute when writing tests.\n    \"\"\"\n\n    _cloud_meta = local_azure_blob_implementation\n\n    def __init__(self, *args, **kwargs):\n        cred_opts = [\n            kwargs.get(\"blob_service_client\", None),\n            kwargs.get(\"connection_string\", None),\n            kwargs.get(\"account_url\", None),\n            os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\", None),\n        ]\n        super().__init__(*args, **kwargs)\n\n        if all(opt is None for opt in cred_opts):\n            raise MissingCredentialsError(\n                \"AzureBlobClient does not support anonymous instantiation. \"\n                \"Credentials are required; see docs for options.\"\n            )\n</code></pre> <p>Replacement for AzureBlobPath that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> <p>Abstract client for accessing objects the local filesystem. Subclasses are as a monkeypatch substitutes for normal Client subclasses when writing tests.</p> Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>class LocalClient(Client):\n    \"\"\"Abstract client for accessing objects the local filesystem. Subclasses are as a monkeypatch\n    substitutes for normal Client subclasses when writing tests.\"\"\"\n\n    # Class-level variable to tracks the default storage directory for this client class\n    # that is used if a client is instantiated without a directory being explicitly provided\n    _default_storage_temp_dir: ClassVar[Optional[TemporaryDirectory]] = None\n\n    # Instance-level variable that tracks the local storage directory for this client\n    _local_storage_dir: Optional[Union[str, os.PathLike]]\n\n    def __init__(\n        self,\n        *args,\n        local_storage_dir: Optional[Union[str, os.PathLike]] = None,\n        file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n        local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n        content_type_method: Optional[Callable] = mimetypes.guess_type,\n        **kwargs,\n    ):\n        self._local_storage_dir = local_storage_dir\n\n        super().__init__(\n            local_cache_dir=local_cache_dir,\n            content_type_method=content_type_method,\n            file_cache_mode=file_cache_mode,\n        )\n\n    @classmethod\n    def get_default_storage_dir(cls) -&gt; Path:\n        \"\"\"Return the default storage directory for this client class. This is used if a client\n        is instantiated without a storage directory being explicitly provided. In this usage,\n        \"storage\" refers to the local storage that simulates the cloud.\n        \"\"\"\n        if cls._default_storage_temp_dir is None:\n            cls._default_storage_temp_dir = TemporaryDirectory()\n            _temp_dirs_to_clean.append(cls._default_storage_temp_dir)\n        return Path(cls._default_storage_temp_dir.name)\n\n    @classmethod\n    def reset_default_storage_dir(cls) -&gt; Path:\n        \"\"\"Reset the default storage directly. This tears down and recreates the directory used by\n        default for this client class when instantiating a client without explicitly providing\n        a storage directory. In this usage, \"storage\" refers to the local storage that simulates\n        the cloud.\n        \"\"\"\n        cls._default_storage_temp_dir = None\n        return cls.get_default_storage_dir()\n\n    @property\n    def local_storage_dir(self) -&gt; Path:\n        \"\"\"The local directory where files are stored for this client. This storage directory is\n        the one that simulates the cloud. If no storage directory was provided on instantiating the\n        client, the default storage directory for this client class is used.\n        \"\"\"\n        if self._local_storage_dir is None:\n            # No explicit local storage was provided on instantiating the client.\n            # Use the default storage directory for this class.\n            return self.get_default_storage_dir()\n        return Path(self._local_storage_dir)\n\n    def _cloud_path_to_local(self, cloud_path: \"LocalPath\") -&gt; Path:\n        return self.local_storage_dir / cloud_path._no_prefix\n\n    def _local_to_cloud_path(self, local_path: Union[str, os.PathLike]) -&gt; \"LocalPath\":\n        local_path = Path(local_path)\n        cloud_prefix = self._cloud_meta.path_class.cloud_prefix\n        return self.CloudPath(\n            f\"{cloud_prefix}{PurePosixPath(local_path.relative_to(self.local_storage_dir))}\"\n        )\n\n    def _download_file(self, cloud_path: \"LocalPath\", local_path: Union[str, os.PathLike]) -&gt; Path:\n        local_path = Path(local_path)\n        local_path.parent.mkdir(exist_ok=True, parents=True)\n\n        try:\n            shutil.copyfile(self._cloud_path_to_local(cloud_path), local_path)\n        except FileNotFoundError:\n            # erroneous FileNotFoundError appears in tests sometimes; patiently insist on the parent directory existing\n            sleep(1.0)\n            local_path.parent.mkdir(exist_ok=True, parents=True)\n            sleep(1.0)\n\n            shutil.copyfile(self._cloud_path_to_local(cloud_path), local_path)\n\n        return local_path\n\n    def _exists(self, cloud_path: \"LocalPath\") -&gt; bool:\n        return self._cloud_path_to_local(cloud_path).exists()\n\n    def _is_dir(self, cloud_path: \"LocalPath\") -&gt; bool:\n        return self._cloud_path_to_local(cloud_path).is_dir()\n\n    def _is_file(self, cloud_path: \"LocalPath\") -&gt; bool:\n        return self._cloud_path_to_local(cloud_path).is_file()\n\n    def _list_dir(\n        self, cloud_path: \"LocalPath\", recursive=False\n    ) -&gt; Iterable[Tuple[\"LocalPath\", bool]]:\n        pattern = \"**/*\" if recursive else \"*\"\n        for obj in self._cloud_path_to_local(cloud_path).glob(pattern):\n            yield (self._local_to_cloud_path(obj), obj.is_dir())\n\n    def _md5(self, cloud_path: \"LocalPath\") -&gt; str:\n        return md5(self._cloud_path_to_local(cloud_path).read_bytes()).hexdigest()\n\n    def _move_file(\n        self, src: \"LocalPath\", dst: \"LocalPath\", remove_src: bool = True\n    ) -&gt; \"LocalPath\":\n        self._cloud_path_to_local(dst).parent.mkdir(exist_ok=True, parents=True)\n\n        if remove_src:\n            self._cloud_path_to_local(src).replace(self._cloud_path_to_local(dst))\n        else:\n            shutil.copy(self._cloud_path_to_local(src), self._cloud_path_to_local(dst))\n        return dst\n\n    def _remove(self, cloud_path: \"LocalPath\", missing_ok: bool = True) -&gt; None:\n        local_storage_path = self._cloud_path_to_local(cloud_path)\n        if not missing_ok and not local_storage_path.exists():\n            raise FileNotFoundError(f\"File does not exist: {cloud_path}\")\n\n        if local_storage_path.is_file():\n            local_storage_path.unlink()\n        elif local_storage_path.is_dir():\n            shutil.rmtree(local_storage_path)\n\n    def _stat(self, cloud_path: \"LocalPath\") -&gt; os.stat_result:\n        stat_result = self._cloud_path_to_local(cloud_path).stat()\n\n        return os.stat_result(\n            (  # type: ignore\n                None,  # type: ignore # mode\n                None,  # ino\n                cloud_path.cloud_prefix,  # dev,\n                None,  # nlink,\n                None,  # uid,\n                None,  # gid,\n                stat_result.st_size,  # size,\n                None,  # atime,\n                stat_result.st_mtime,  # mtime,\n                None,  # ctime,\n            )\n        )\n\n    def _touch(self, cloud_path: \"LocalPath\", exist_ok: bool = True) -&gt; None:\n        local_storage_path = self._cloud_path_to_local(cloud_path)\n        if local_storage_path.exists() and not exist_ok:\n            raise FileExistsError(f\"File exists: {cloud_path}\")\n        local_storage_path.parent.mkdir(exist_ok=True, parents=True)\n        local_storage_path.touch()\n\n    def _upload_file(\n        self, local_path: Union[str, os.PathLike], cloud_path: \"LocalPath\"\n    ) -&gt; \"LocalPath\":\n        dst = self._cloud_path_to_local(cloud_path)\n        dst.parent.mkdir(exist_ok=True, parents=True)\n        shutil.copy(local_path, dst)\n        return cloud_path\n\n    def _get_metadata(self, cloud_path: \"LocalPath\") -&gt; Dict:\n        # content_type is the only metadata we test currently\n        if self.content_type_method is None:\n            content_type_method = lambda x: (None, None)\n        else:\n            content_type_method = self.content_type_method\n\n        return {\n            \"content_type\": content_type_method(str(self._cloud_path_to_local(cloud_path)))[0],\n        }\n\n    def _get_public_url(self, cloud_path: \"LocalPath\") -&gt; str:\n        return cloud_path.as_uri()\n\n    def _generate_presigned_url(\n        self, cloud_path: \"LocalPath\", expire_seconds: int = 60 * 60\n    ) -&gt; str:\n        raise NotImplementedError(\"Cannot generate a presigned URL for a local path.\")\n</code></pre> <p>Replacement for GSClient that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Source code in <code>cloudpathlib/local/implementations/gs.py</code> <pre><code>class LocalGSClient(LocalClient):\n    \"\"\"Replacement for GSClient that uses the local file system. Intended as a monkeypatch\n    substitute when writing tests.\n    \"\"\"\n\n    _cloud_meta = local_gs_implementation\n</code></pre> <p>Replacement for GSPath that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> <p>Abstract CloudPath for accessing objects the local filesystem. Subclasses are as a monkeypatch substitutes for normal CloudPath subclasses when writing tests.</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>class LocalPath(CloudPath):\n    \"\"\"Abstract CloudPath for accessing objects the local filesystem. Subclasses are as a\n    monkeypatch substitutes for normal CloudPath subclasses when writing tests.\"\"\"\n\n    client: \"LocalClient\"\n\n    def is_dir(self) -&gt; bool:\n        return self.client._is_dir(self)\n\n    def is_file(self) -&gt; bool:\n        return self.client._is_file(self)\n\n    def stat(self):\n        try:\n            meta = self.client._stat(self)\n        except FileNotFoundError:\n            raise NoStatError(\n                f\"No stats available for {self}; it may be a directory or not exist.\"\n            )\n        return meta\n\n    def touch(self, exist_ok: bool = True):\n        self.client._touch(self, exist_ok)\n</code></pre> <p>Replacement for S3Client that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p> Source code in <code>cloudpathlib/local/implementations/s3.py</code> <pre><code>class LocalS3Client(LocalClient):\n    \"\"\"Replacement for S3Client that uses the local file system. Intended as a monkeypatch\n    substitute when writing tests.\n    \"\"\"\n\n    _cloud_meta = local_s3_implementation\n</code></pre> <p>Replacement for S3Path that uses the local file system. Intended as a monkeypatch substitute when writing tests.</p>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobClient.AzureBlobPath","title":"<code>AzureBlobPath(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code>","text":"Source code in <code>cloudpathlib/local/implementations/azure.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobClient.__init__","title":"<code>__init__(self, *args, **kwargs)</code>  <code>special</code>","text":"Source code in <code>cloudpathlib/local/implementations/azure.py</code> <pre><code>def __init__(self, *args, **kwargs):\n    cred_opts = [\n        kwargs.get(\"blob_service_client\", None),\n        kwargs.get(\"connection_string\", None),\n        kwargs.get(\"account_url\", None),\n        os.getenv(\"AZURE_STORAGE_CONNECTION_STRING\", None),\n    ]\n    super().__init__(*args, **kwargs)\n\n    if all(opt is None for opt in cred_opts):\n        raise MissingCredentialsError(\n            \"AzureBlobClient does not support anonymous instantiation. \"\n            \"Credentials are required; see docs for options.\"\n        )\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath-attributes","title":"Attributes","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath.blob","title":"<code>blob: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath.cloud_prefix","title":"<code>cloud_prefix: str</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath.container","title":"<code>container: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath.drive","title":"<code>drive: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath.etag","title":"<code>etag</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath.md5","title":"<code>md5: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath-methods","title":"Methods","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.azure.LocalAzureBlobPath.mkdir","title":"<code>mkdir(self, parents = False, exist_ok = False)</code>","text":"<p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/implementations/azure.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    # not possible to make empty directory on blob storage\n    pass\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient-attributes","title":"Attributes","text":""},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient.local_storage_dir","title":"<code>local_storage_dir: Path</code>  <code>property</code> <code>readonly</code>","text":"<p>The local directory where files are stored for this client. This storage directory is the one that simulates the cloud. If no storage directory was provided on instantiating the client, the default storage directory for this client class is used.</p>"},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient-methods","title":"Methods","text":""},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient.__init__","title":"<code>__init__(self, *args, *, local_storage_dir: Union[str, os.PathLike] = None, file_cache_mode: Union[str, cloudpathlib.enums.FileCacheMode] = None, local_cache_dir: Union[str, os.PathLike] = None, content_type_method: Optional[Callable] = &lt;function guess_type at 0x7f08e5d20790&gt;, **kwargs)</code>  <code>special</code>","text":"Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>def __init__(\n    self,\n    *args,\n    local_storage_dir: Optional[Union[str, os.PathLike]] = None,\n    file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n    local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n    content_type_method: Optional[Callable] = mimetypes.guess_type,\n    **kwargs,\n):\n    self._local_storage_dir = local_storage_dir\n\n    super().__init__(\n        local_cache_dir=local_cache_dir,\n        content_type_method=content_type_method,\n        file_cache_mode=file_cache_mode,\n    )\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient.get_default_storage_dir","title":"<code>get_default_storage_dir() -&gt; Path</code>  <code>classmethod</code>","text":"<p>Return the default storage directory for this client class. This is used if a client is instantiated without a storage directory being explicitly provided. In this usage, \"storage\" refers to the local storage that simulates the cloud.</p> Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>@classmethod\ndef get_default_storage_dir(cls) -&gt; Path:\n    \"\"\"Return the default storage directory for this client class. This is used if a client\n    is instantiated without a storage directory being explicitly provided. In this usage,\n    \"storage\" refers to the local storage that simulates the cloud.\n    \"\"\"\n    if cls._default_storage_temp_dir is None:\n        cls._default_storage_temp_dir = TemporaryDirectory()\n        _temp_dirs_to_clean.append(cls._default_storage_temp_dir)\n    return Path(cls._default_storage_temp_dir.name)\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localclient.LocalClient.reset_default_storage_dir","title":"<code>reset_default_storage_dir() -&gt; Path</code>  <code>classmethod</code>","text":"<p>Reset the default storage directly. This tears down and recreates the directory used by default for this client class when instantiating a client without explicitly providing a storage directory. In this usage, \"storage\" refers to the local storage that simulates the cloud.</p> Source code in <code>cloudpathlib/local/localclient.py</code> <pre><code>@classmethod\ndef reset_default_storage_dir(cls) -&gt; Path:\n    \"\"\"Reset the default storage directly. This tears down and recreates the directory used by\n    default for this client class when instantiating a client without explicitly providing\n    a storage directory. In this usage, \"storage\" refers to the local storage that simulates\n    the cloud.\n    \"\"\"\n    cls._default_storage_temp_dir = None\n    return cls.get_default_storage_dir()\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSClient.GSPath","title":"<code>GSPath(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code>","text":"Source code in <code>cloudpathlib/local/implementations/gs.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSPath-attributes","title":"Attributes","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSPath.blob","title":"<code>blob: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSPath.bucket","title":"<code>bucket: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSPath.cloud_prefix","title":"<code>cloud_prefix: str</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSPath.drive","title":"<code>drive: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSPath.etag","title":"<code>etag</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSPath-methods","title":"Methods","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.gs.LocalGSPath.mkdir","title":"<code>mkdir(self, parents = False, exist_ok = False)</code>","text":"<p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/implementations/gs.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    # not possible to make empty directory on gs\n    pass\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localpath.LocalPath-methods","title":"Methods","text":""},{"location":"api-reference/local/#cloudpathlib.local.localpath.LocalPath.is_dir","title":"<code>is_dir(self) -&gt; bool</code>","text":"<p>Whether this path is a directory.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>def is_dir(self) -&gt; bool:\n    return self.client._is_dir(self)\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localpath.LocalPath.is_file","title":"<code>is_file(self) -&gt; bool</code>","text":"<p>Whether this path is a regular file (also True for symlinks pointing to regular files).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>def is_file(self) -&gt; bool:\n    return self.client._is_file(self)\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localpath.LocalPath.stat","title":"<code>stat(self)</code>","text":"<p>Return the result of the stat() system call on this path, like os.stat() does.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>def stat(self):\n    try:\n        meta = self.client._stat(self)\n    except FileNotFoundError:\n        raise NoStatError(\n            f\"No stats available for {self}; it may be a directory or not exist.\"\n        )\n    return meta\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.localpath.LocalPath.touch","title":"<code>touch(self, exist_ok: bool = True)</code>","text":"<p>Create this file with the given access mode, if it doesn't exist.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/localpath.py</code> <pre><code>def touch(self, exist_ok: bool = True):\n    self.client._touch(self, exist_ok)\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Client.S3Path","title":"<code>S3Path(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code>","text":"Source code in <code>cloudpathlib/local/implementations/s3.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Path-attributes","title":"Attributes","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Path.bucket","title":"<code>bucket: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Path.cloud_prefix","title":"<code>cloud_prefix: str</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Path.drive","title":"<code>drive: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Path.etag","title":"<code>etag</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Path.key","title":"<code>key: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Path-methods","title":"Methods","text":""},{"location":"api-reference/local/#cloudpathlib.local.implementations.s3.LocalS3Path.mkdir","title":"<code>mkdir(self, parents = False, exist_ok = False)</code>","text":"<p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/local/implementations/s3.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    # not possible to make empty directory on s3\n    pass\n</code></pre>"},{"location":"api-reference/s3client/","title":"cloudpathlib.S3Client","text":"<p>Client class for AWS S3 which handles authentication with AWS for <code>S3Path</code> instances. See documentation for the <code>__init__</code> method for detailed authentication options.</p> Source code in <code>cloudpathlib/s3/s3client.py</code> <pre><code>class S3Client(Client):\n    \"\"\"Client class for AWS S3 which handles authentication with AWS for [`S3Path`](../s3path/)\n    instances. See documentation for the [`__init__` method][cloudpathlib.s3.s3client.S3Client.__init__]\n    for detailed authentication options.\"\"\"\n\n    def __init__(\n        self,\n        aws_access_key_id: Optional[str] = None,\n        aws_secret_access_key: Optional[str] = None,\n        aws_session_token: Optional[str] = None,\n        no_sign_request: Optional[bool] = False,\n        botocore_session: Optional[\"botocore.session.Session\"] = None,\n        profile_name: Optional[str] = None,\n        boto3_session: Optional[\"Session\"] = None,\n        file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n        local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n        endpoint_url: Optional[str] = None,\n        boto3_transfer_config: Optional[\"TransferConfig\"] = None,\n        content_type_method: Optional[Callable] = mimetypes.guess_type,\n        extra_args: Optional[dict] = None,\n    ):\n        \"\"\"Class constructor. Sets up a boto3 [`Session`](\n        https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html).\n        Directly supports the same authentication interface, as well as the same environment\n        variables supported by boto3. See [boto3 Session documentation](\n        https://boto3.amazonaws.com/v1/documentation/api/latest/guide/session.html).\n\n        If no authentication arguments or environment variables are provided, then the client will\n        be instantiated as anonymous, which will only have access to public buckets.\n\n        Args:\n            aws_access_key_id (Optional[str]): AWS access key ID.\n            aws_secret_access_key (Optional[str]): AWS secret access key.\n            aws_session_token (Optional[str]): Session key for your AWS account. This is only\n                needed when you are using temporarycredentials.\n            no_sign_request (Optional[bool]): If `True`, credentials are not looked for and we use unsigned\n                requests to fetch resources. This will only allow access to public resources. This is equivalent\n                to `--no-sign-request` in the [AWS CLI](https://docs.aws.amazon.com/cli/latest/reference/).\n            botocore_session (Optional[botocore.session.Session]): An already instantiated botocore\n                Session.\n            profile_name (Optional[str]): Profile name of a profile in a shared credentials file.\n            boto3_session (Optional[Session]): An already instantiated boto3 Session.\n            file_cache_mode (Optional[Union[str, FileCacheMode]]): How often to clear the file cache; see\n                [the caching docs](https://cloudpathlib.drivendata.org/stable/caching/) for more information\n                about the options in cloudpathlib.eums.FileCacheMode.\n            local_cache_dir (Optional[Union[str, os.PathLike]]): Path to directory to use as cache\n                for downloaded files. If None, will use a temporary directory. Default can be set with\n                the `CLOUDPATHLIB_LOCAL_CACHE_DIR` environment variable.\n            endpoint_url (Optional[str]): S3 server endpoint URL to use for the constructed boto3 S3 resource and client.\n                Parameterize it to access a customly deployed S3-compatible object store such as MinIO, Ceph or any other.\n            boto3_transfer_config (Optional[dict]): Instantiated TransferConfig for managing\n                [s3 transfers](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.TransferConfig)\n            content_type_method (Optional[Callable]): Function to call to guess media type (mimetype) when\n                writing a file to the cloud. Defaults to `mimetypes.guess_type`. Must return a tuple (content type, content encoding).\n            extra_args (Optional[dict]): A dictionary of extra args passed to download, upload, and list functions as relevant. You\n                can include any keys supported by upload or download, and we will pass on only the relevant args. To see the extra\n                args that are supported look at the upload and download lists in the\n                [boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.S3Transfer).\n        \"\"\"\n        endpoint_url = endpoint_url or os.getenv(\"AWS_ENDPOINT_URL\")\n        if boto3_session is not None:\n            self.sess = boto3_session\n        else:\n            self.sess = Session(\n                aws_access_key_id=aws_access_key_id,\n                aws_secret_access_key=aws_secret_access_key,\n                aws_session_token=aws_session_token,\n                botocore_session=botocore_session,\n                profile_name=profile_name,\n            )\n\n        if no_sign_request:\n            self.s3 = self.sess.resource(\n                \"s3\",\n                endpoint_url=endpoint_url,\n                config=Config(signature_version=botocore.session.UNSIGNED),\n            )\n            self.client = self.sess.client(\n                \"s3\",\n                endpoint_url=endpoint_url,\n                config=Config(signature_version=botocore.session.UNSIGNED),\n            )\n        else:\n            self.s3 = self.sess.resource(\"s3\", endpoint_url=endpoint_url)\n            self.client = self.sess.client(\"s3\", endpoint_url=endpoint_url)\n\n        self.boto3_transfer_config = boto3_transfer_config\n\n        if extra_args is None:\n            extra_args = {}\n\n        self._extra_args = extra_args\n        self.boto3_dl_extra_args = {\n            k: v for k, v in extra_args.items() if k in S3Transfer.ALLOWED_DOWNLOAD_ARGS\n        }\n        self.boto3_ul_extra_args = {\n            k: v for k, v in extra_args.items() if k in S3Transfer.ALLOWED_UPLOAD_ARGS\n        }\n\n        # listing ops (list_objects_v2, filter, delete) only accept these extras:\n        # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html\n        self.boto3_list_extra_args = {\n            k: self._extra_args[k]\n            for k in [\"RequestPayer\", \"ExpectedBucketOwner\"]\n            if k in self._extra_args\n        }\n        self._endpoint_url = endpoint_url\n\n        super().__init__(\n            local_cache_dir=local_cache_dir,\n            content_type_method=content_type_method,\n            file_cache_mode=file_cache_mode,\n        )\n\n    def _get_metadata(self, cloud_path: S3Path) -&gt; Dict[str, Any]:\n        # get accepts all download extra args\n        data = self.s3.ObjectSummary(cloud_path.bucket, cloud_path.key).get(\n            **self.boto3_dl_extra_args\n        )\n\n        return {\n            \"last_modified\": data[\"LastModified\"],\n            \"size\": data[\"ContentLength\"],\n            \"etag\": data[\"ETag\"],\n            \"content_type\": data.get(\"ContentType\", None),\n            \"extra\": data[\"Metadata\"],\n        }\n\n    def _download_file(self, cloud_path: S3Path, local_path: Union[str, os.PathLike]) -&gt; Path:\n        local_path = Path(local_path)\n        obj = self.s3.Object(cloud_path.bucket, cloud_path.key)\n\n        obj.download_file(\n            str(local_path), Config=self.boto3_transfer_config, ExtraArgs=self.boto3_dl_extra_args\n        )\n        return local_path\n\n    def _is_file_or_dir(self, cloud_path: S3Path) -&gt; Optional[str]:\n        # short-circuit the root-level bucket\n        if not cloud_path.key:\n            return \"dir\"\n\n        # get first item by listing at least one key\n        return self._s3_file_query(cloud_path)\n\n    def _exists(self, cloud_path: S3Path) -&gt; bool:\n        # check if this is a bucket\n        if not cloud_path.key:\n            extra = {\n                k: self._extra_args[k] for k in [\"ExpectedBucketOwner\"] if k in self._extra_args\n            }\n\n            try:\n                self.client.head_bucket(Bucket=cloud_path.bucket, **extra)\n                return True\n            except ClientError:\n                return False\n\n        return self._s3_file_query(cloud_path) is not None\n\n    def _s3_file_query(self, cloud_path: S3Path):\n        \"\"\"Boto3 query used for quick checks of existence and if path is file/dir\"\"\"\n        # check if this is an object that we can access directly\n        try:\n            # head_object accepts all download extra args (note: Object.load does not accept extra args so we do not use it for this check)\n            self.client.head_object(\n                Bucket=cloud_path.bucket,\n                Key=cloud_path.key.rstrip(\"/\"),\n                **self.boto3_dl_extra_args,\n            )\n            return \"file\"\n\n        # else, confirm it is a dir by filtering to the first item under the prefix plus a \"/\"\n        except (ClientError, self.client.exceptions.NoSuchKey):\n            key = cloud_path.key.rstrip(\"/\") + \"/\"\n\n            return next(\n                (\n                    \"dir\"  # always a dir if we find anything with this query\n                    for obj in (\n                        self.s3.Bucket(cloud_path.bucket)\n                        .objects.filter(Prefix=key, **self.boto3_list_extra_args)\n                        .limit(1)\n                    )\n                ),\n                None,\n            )\n\n    def _list_dir(self, cloud_path: S3Path, recursive=False) -&gt; Iterable[Tuple[S3Path, bool]]:\n        # shortcut if listing all available buckets\n        if not cloud_path.bucket:\n            if recursive:\n                raise NotImplementedError(\n                    \"Cannot recursively list all buckets and contents; you can get all the buckets then recursively list each separately.\"\n                )\n\n            yield from (\n                (self.CloudPath(f\"s3://{b['Name']}\"), True)\n                for b in self.client.list_buckets().get(\"Buckets\", [])\n            )\n            return\n\n        prefix = cloud_path.key\n        if prefix and not prefix.endswith(\"/\"):\n            prefix += \"/\"\n\n        yielded_dirs = set()\n\n        paginator = self.client.get_paginator(\"list_objects_v2\")\n\n        for result in paginator.paginate(\n            Bucket=cloud_path.bucket,\n            Prefix=prefix,\n            Delimiter=(\"\" if recursive else \"/\"),\n            **self.boto3_list_extra_args,\n        ):\n            # yield everything in common prefixes as directories\n            for result_prefix in result.get(\"CommonPrefixes\", []):\n                canonical = result_prefix.get(\"Prefix\").rstrip(\"/\")  # keep a canonical form\n                if canonical not in yielded_dirs:\n                    yield (\n                        self.CloudPath(f\"s3://{cloud_path.bucket}/{canonical}\"),\n                        True,\n                    )\n                    yielded_dirs.add(canonical)\n\n            # check all the keys\n            for result_key in result.get(\"Contents\", []):\n                # yield all the parents of any key that have not been yielded already\n                o_relative_path = result_key.get(\"Key\")[len(prefix) :]\n                for parent in PurePosixPath(o_relative_path).parents:\n                    parent_canonical = prefix + str(parent).rstrip(\"/\")\n                    if parent_canonical not in yielded_dirs and str(parent) != \".\":\n                        yield (\n                            self.CloudPath(f\"s3://{cloud_path.bucket}/{parent_canonical}\"),\n                            True,\n                        )\n                        yielded_dirs.add(parent_canonical)\n\n                # if we already yielded this dir, go to next item in contents\n                canonical = result_key.get(\"Key\").rstrip(\"/\")\n                if canonical in yielded_dirs:\n                    continue\n\n                # s3 fake directories have 0 size and end with \"/\"\n                if result_key.get(\"Key\").endswith(\"/\") and result_key.get(\"Size\") == 0:\n                    yield (\n                        self.CloudPath(f\"s3://{cloud_path.bucket}/{canonical}\"),\n                        True,\n                    )\n                    yielded_dirs.add(canonical)\n\n                # yield object as file\n                else:\n                    yield (\n                        self.CloudPath(f\"s3://{cloud_path.bucket}/{result_key.get('Key')}\"),\n                        False,\n                    )\n\n    def _move_file(self, src: S3Path, dst: S3Path, remove_src: bool = True) -&gt; S3Path:\n        # just a touch, so \"REPLACE\" metadata\n        if src == dst:\n            o = self.s3.Object(src.bucket, src.key)\n            o.copy_from(\n                CopySource={\"Bucket\": src.bucket, \"Key\": src.key},\n                Metadata=self._get_metadata(src).get(\"extra\", {}),\n                MetadataDirective=\"REPLACE\",\n                **self.boto3_ul_extra_args,\n            )\n\n        else:\n            target = self.s3.Object(dst.bucket, dst.key)\n            target.copy(\n                {\"Bucket\": src.bucket, \"Key\": src.key},\n                ExtraArgs=self.boto3_dl_extra_args,\n                Config=self.boto3_transfer_config,\n            )\n\n            if remove_src:\n                self._remove(src)\n        return dst\n\n    def _remove(self, cloud_path: S3Path, missing_ok: bool = True) -&gt; None:\n        file_or_dir = self._is_file_or_dir(cloud_path=cloud_path)\n        if file_or_dir == \"file\":\n            resp = self.s3.Object(cloud_path.bucket, cloud_path.key).delete(\n                **self.boto3_list_extra_args\n            )\n            if resp.get(\"ResponseMetadata\").get(\"HTTPStatusCode\") not in (204, 200):\n                raise CloudPathException(\n                    f\"Delete operation failed for {cloud_path} with response: {resp}\"\n                )\n\n        elif file_or_dir == \"dir\":\n            # try to delete as a direcotry instead\n            bucket = self.s3.Bucket(cloud_path.bucket)\n\n            prefix = cloud_path.key\n            if prefix and not prefix.endswith(\"/\"):\n                prefix += \"/\"\n\n            resp = bucket.objects.filter(Prefix=prefix, **self.boto3_list_extra_args).delete(\n                **self.boto3_list_extra_args\n            )\n            if resp[0].get(\"ResponseMetadata\").get(\"HTTPStatusCode\") not in (204, 200):\n                raise CloudPathException(\n                    f\"Delete operation failed for {cloud_path} with response: {resp}\"\n                )\n\n        else:\n            if not missing_ok:\n                raise FileNotFoundError(\n                    f\"Cannot delete file that does not exist: {cloud_path} (consider passing missing_ok=True)\"\n                )\n\n    def _upload_file(self, local_path: Union[str, os.PathLike], cloud_path: S3Path) -&gt; S3Path:\n        obj = self.s3.Object(cloud_path.bucket, cloud_path.key)\n\n        extra_args = self.boto3_ul_extra_args.copy()\n\n        if self.content_type_method is not None:\n            content_type, content_encoding = self.content_type_method(str(local_path))\n            if content_type is not None:\n                extra_args[\"ContentType\"] = content_type\n            if content_encoding is not None:\n                extra_args[\"ContentEncoding\"] = content_encoding\n\n        obj.upload_file(str(local_path), Config=self.boto3_transfer_config, ExtraArgs=extra_args)\n        return cloud_path\n\n    def _get_public_url(self, cloud_path: S3Path) -&gt; str:\n        \"\"\"Apparently the best way to get the public URL is to generate a presigned URL\n        with the unsigned config set. This creates a temporary unsigned client to generate\n        the correct URL\n        See: https://stackoverflow.com/a/48197877\n        \"\"\"\n        unsigned_config = Config(signature_version=botocore.UNSIGNED)\n        unsigned_client = self.sess.client(\n            \"s3\", endpoint_url=self._endpoint_url, config=unsigned_config\n        )\n        url: str = unsigned_client.generate_presigned_url(\n            \"get_object\",\n            Params={\"Bucket\": cloud_path.bucket, \"Key\": cloud_path.key},\n            ExpiresIn=0,\n        )\n        return url\n\n    def _generate_presigned_url(self, cloud_path: S3Path, expire_seconds: int = 60 * 60) -&gt; str:\n        url: str = self.client.generate_presigned_url(\n            \"get_object\",\n            Params={\"Bucket\": cloud_path.bucket, \"Key\": cloud_path.key},\n            ExpiresIn=expire_seconds,\n        )\n        return url\n</code></pre>"},{"location":"api-reference/s3client/#cloudpathlib.s3.s3client.S3Client-methods","title":"Methods","text":""},{"location":"api-reference/s3client/#cloudpathlib.s3.s3client.S3Client.S3Path","title":"<code>S3Path(self, cloud_path: Union[str, ~BoundedCloudPath]) -&gt; ~BoundedCloudPath</code>","text":"Source code in <code>cloudpathlib/s3/s3client.py</code> <pre><code>def CloudPath(self, cloud_path: Union[str, BoundedCloudPath]) -&gt; BoundedCloudPath:\n    return self._cloud_meta.path_class(cloud_path=cloud_path, client=self)  # type: ignore\n</code></pre>"},{"location":"api-reference/s3client/#cloudpathlib.s3.s3client.S3Client.__init__","title":"<code>__init__(self, aws_access_key_id: Optional[str] = None, aws_secret_access_key: Optional[str] = None, aws_session_token: Optional[str] = None, no_sign_request: Optional[bool] = False, botocore_session: Optional[botocore.session.Session] = None, profile_name: Optional[str] = None, boto3_session: Optional[Session] = None, file_cache_mode: Union[str, cloudpathlib.enums.FileCacheMode] = None, local_cache_dir: Union[str, os.PathLike] = None, endpoint_url: Optional[str] = None, boto3_transfer_config: Optional[TransferConfig] = None, content_type_method: Optional[Callable] = &lt;function guess_type at 0x7f08e5d20790&gt;, extra_args: Optional[dict] = None)</code>  <code>special</code>","text":"<p>Class constructor. Sets up a boto3 <code>Session</code>. Directly supports the same authentication interface, as well as the same environment variables supported by boto3. See boto3 Session documentation.</p> <p>If no authentication arguments or environment variables are provided, then the client will be instantiated as anonymous, which will only have access to public buckets.</p> <p>Parameters:</p> Name Type Description Default <code>aws_access_key_id</code> <code>Optional[str]</code> <p>AWS access key ID.</p> <code>None</code> <code>aws_secret_access_key</code> <code>Optional[str]</code> <p>AWS secret access key.</p> <code>None</code> <code>aws_session_token</code> <code>Optional[str]</code> <p>Session key for your AWS account. This is only needed when you are using temporarycredentials.</p> <code>None</code> <code>no_sign_request</code> <code>Optional[bool]</code> <p>If <code>True</code>, credentials are not looked for and we use unsigned requests to fetch resources. This will only allow access to public resources. This is equivalent to <code>--no-sign-request</code> in the AWS CLI.</p> <code>False</code> <code>botocore_session</code> <code>Optional[botocore.session.Session]</code> <p>An already instantiated botocore Session.</p> <code>None</code> <code>profile_name</code> <code>Optional[str]</code> <p>Profile name of a profile in a shared credentials file.</p> <code>None</code> <code>boto3_session</code> <code>Optional[Session]</code> <p>An already instantiated boto3 Session.</p> <code>None</code> <code>file_cache_mode</code> <code>Optional[Union[str, FileCacheMode]]</code> <p>How often to clear the file cache; see the caching docs for more information about the options in cloudpathlib.eums.FileCacheMode.</p> <code>None</code> <code>local_cache_dir</code> <code>Optional[Union[str, os.PathLike]]</code> <p>Path to directory to use as cache for downloaded files. If None, will use a temporary directory. Default can be set with the <code>CLOUDPATHLIB_LOCAL_CACHE_DIR</code> environment variable.</p> <code>None</code> <code>endpoint_url</code> <code>Optional[str]</code> <p>S3 server endpoint URL to use for the constructed boto3 S3 resource and client. Parameterize it to access a customly deployed S3-compatible object store such as MinIO, Ceph or any other.</p> <code>None</code> <code>boto3_transfer_config</code> <code>Optional[dict]</code> <p>Instantiated TransferConfig for managing s3 transfers</p> <code>None</code> <code>content_type_method</code> <code>Optional[Callable]</code> <p>Function to call to guess media type (mimetype) when writing a file to the cloud. Defaults to <code>mimetypes.guess_type</code>. Must return a tuple (content type, content encoding).</p> <code>&lt;function guess_type at 0x7f08e5d20790&gt;</code> <code>extra_args</code> <code>Optional[dict]</code> <p>A dictionary of extra args passed to download, upload, and list functions as relevant. You can include any keys supported by upload or download, and we will pass on only the relevant args. To see the extra args that are supported look at the upload and download lists in the boto3 docs.</p> <code>None</code> Source code in <code>cloudpathlib/s3/s3client.py</code> <pre><code>def __init__(\n    self,\n    aws_access_key_id: Optional[str] = None,\n    aws_secret_access_key: Optional[str] = None,\n    aws_session_token: Optional[str] = None,\n    no_sign_request: Optional[bool] = False,\n    botocore_session: Optional[\"botocore.session.Session\"] = None,\n    profile_name: Optional[str] = None,\n    boto3_session: Optional[\"Session\"] = None,\n    file_cache_mode: Optional[Union[str, FileCacheMode]] = None,\n    local_cache_dir: Optional[Union[str, os.PathLike]] = None,\n    endpoint_url: Optional[str] = None,\n    boto3_transfer_config: Optional[\"TransferConfig\"] = None,\n    content_type_method: Optional[Callable] = mimetypes.guess_type,\n    extra_args: Optional[dict] = None,\n):\n    \"\"\"Class constructor. Sets up a boto3 [`Session`](\n    https://boto3.amazonaws.com/v1/documentation/api/latest/reference/core/session.html).\n    Directly supports the same authentication interface, as well as the same environment\n    variables supported by boto3. See [boto3 Session documentation](\n    https://boto3.amazonaws.com/v1/documentation/api/latest/guide/session.html).\n\n    If no authentication arguments or environment variables are provided, then the client will\n    be instantiated as anonymous, which will only have access to public buckets.\n\n    Args:\n        aws_access_key_id (Optional[str]): AWS access key ID.\n        aws_secret_access_key (Optional[str]): AWS secret access key.\n        aws_session_token (Optional[str]): Session key for your AWS account. This is only\n            needed when you are using temporarycredentials.\n        no_sign_request (Optional[bool]): If `True`, credentials are not looked for and we use unsigned\n            requests to fetch resources. This will only allow access to public resources. This is equivalent\n            to `--no-sign-request` in the [AWS CLI](https://docs.aws.amazon.com/cli/latest/reference/).\n        botocore_session (Optional[botocore.session.Session]): An already instantiated botocore\n            Session.\n        profile_name (Optional[str]): Profile name of a profile in a shared credentials file.\n        boto3_session (Optional[Session]): An already instantiated boto3 Session.\n        file_cache_mode (Optional[Union[str, FileCacheMode]]): How often to clear the file cache; see\n            [the caching docs](https://cloudpathlib.drivendata.org/stable/caching/) for more information\n            about the options in cloudpathlib.eums.FileCacheMode.\n        local_cache_dir (Optional[Union[str, os.PathLike]]): Path to directory to use as cache\n            for downloaded files. If None, will use a temporary directory. Default can be set with\n            the `CLOUDPATHLIB_LOCAL_CACHE_DIR` environment variable.\n        endpoint_url (Optional[str]): S3 server endpoint URL to use for the constructed boto3 S3 resource and client.\n            Parameterize it to access a customly deployed S3-compatible object store such as MinIO, Ceph or any other.\n        boto3_transfer_config (Optional[dict]): Instantiated TransferConfig for managing\n            [s3 transfers](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.TransferConfig)\n        content_type_method (Optional[Callable]): Function to call to guess media type (mimetype) when\n            writing a file to the cloud. Defaults to `mimetypes.guess_type`. Must return a tuple (content type, content encoding).\n        extra_args (Optional[dict]): A dictionary of extra args passed to download, upload, and list functions as relevant. You\n            can include any keys supported by upload or download, and we will pass on only the relevant args. To see the extra\n            args that are supported look at the upload and download lists in the\n            [boto3 docs](https://boto3.amazonaws.com/v1/documentation/api/latest/reference/customizations/s3.html#boto3.s3.transfer.S3Transfer).\n    \"\"\"\n    endpoint_url = endpoint_url or os.getenv(\"AWS_ENDPOINT_URL\")\n    if boto3_session is not None:\n        self.sess = boto3_session\n    else:\n        self.sess = Session(\n            aws_access_key_id=aws_access_key_id,\n            aws_secret_access_key=aws_secret_access_key,\n            aws_session_token=aws_session_token,\n            botocore_session=botocore_session,\n            profile_name=profile_name,\n        )\n\n    if no_sign_request:\n        self.s3 = self.sess.resource(\n            \"s3\",\n            endpoint_url=endpoint_url,\n            config=Config(signature_version=botocore.session.UNSIGNED),\n        )\n        self.client = self.sess.client(\n            \"s3\",\n            endpoint_url=endpoint_url,\n            config=Config(signature_version=botocore.session.UNSIGNED),\n        )\n    else:\n        self.s3 = self.sess.resource(\"s3\", endpoint_url=endpoint_url)\n        self.client = self.sess.client(\"s3\", endpoint_url=endpoint_url)\n\n    self.boto3_transfer_config = boto3_transfer_config\n\n    if extra_args is None:\n        extra_args = {}\n\n    self._extra_args = extra_args\n    self.boto3_dl_extra_args = {\n        k: v for k, v in extra_args.items() if k in S3Transfer.ALLOWED_DOWNLOAD_ARGS\n    }\n    self.boto3_ul_extra_args = {\n        k: v for k, v in extra_args.items() if k in S3Transfer.ALLOWED_UPLOAD_ARGS\n    }\n\n    # listing ops (list_objects_v2, filter, delete) only accept these extras:\n    # https://boto3.amazonaws.com/v1/documentation/api/latest/reference/services/s3.html\n    self.boto3_list_extra_args = {\n        k: self._extra_args[k]\n        for k in [\"RequestPayer\", \"ExpectedBucketOwner\"]\n        if k in self._extra_args\n    }\n    self._endpoint_url = endpoint_url\n\n    super().__init__(\n        local_cache_dir=local_cache_dir,\n        content_type_method=content_type_method,\n        file_cache_mode=file_cache_mode,\n    )\n</code></pre>"},{"location":"api-reference/s3path/","title":"cloudpathlib.S3Path","text":"<p>Class for representing and operating on AWS S3 URIs, in the style of the Python standard library's <code>pathlib</code> module. Instances represent a path in S3 with filesystem path semantics, and convenient methods allow for basic operations like joining, reading, writing, iterating over contents, etc. This class almost entirely mimics the <code>pathlib.Path</code> interface, so most familiar properties and methods should be available and behave in the expected way.</p> <p>The <code>S3Client</code> class handles authentication with AWS. If a client instance is not explicitly specified on <code>S3Path</code> instantiation, a default client is used. See <code>S3Client</code>'s documentation for more details.</p> Source code in <code>cloudpathlib/s3/s3path.py</code> <pre><code>class S3Path(CloudPath):\n    \"\"\"Class for representing and operating on AWS S3 URIs, in the style of the Python standard\n    library's [`pathlib` module](https://docs.python.org/3/library/pathlib.html). Instances\n    represent a path in S3 with filesystem path semantics, and convenient methods allow for basic\n    operations like joining, reading, writing, iterating over contents, etc. This class almost\n    entirely mimics the [`pathlib.Path`](https://docs.python.org/3/library/pathlib.html#pathlib.Path)\n    interface, so most familiar properties and methods should be available and behave in the\n    expected way.\n\n    The [`S3Client`](../s3client/) class handles authentication with AWS. If a client instance is\n    not explicitly specified on `S3Path` instantiation, a default client is used. See `S3Client`'s\n    documentation for more details.\n    \"\"\"\n\n    cloud_prefix: str = \"s3://\"\n    client: \"S3Client\"\n\n    @property\n    def drive(self) -&gt; str:\n        return self.bucket\n\n    def is_dir(self) -&gt; bool:\n        return self.client._is_file_or_dir(self) == \"dir\"\n\n    def is_file(self) -&gt; bool:\n        return self.client._is_file_or_dir(self) == \"file\"\n\n    def mkdir(self, parents=False, exist_ok=False):\n        # not possible to make empty directory on s3\n        pass\n\n    def touch(self, exist_ok: bool = True):\n        if self.exists():\n            if not exist_ok:\n                raise FileExistsError(f\"File exists: {self}\")\n            self.client._move_file(self, self)\n        else:\n            tf = TemporaryDirectory()\n            p = Path(tf.name) / \"empty\"\n            p.touch()\n\n            self.client._upload_file(p, self)\n\n            tf.cleanup()\n\n    def stat(self):\n        try:\n            meta = self.client._get_metadata(self)\n        except self.client.client.exceptions.NoSuchKey:\n            raise NoStatError(\n                f\"No stats available for {self}; it may be a directory or not exist.\"\n            )\n\n        return os.stat_result(\n            (\n                None,  # mode\n                None,  # ino\n                self.cloud_prefix,  # dev,\n                None,  # nlink,\n                None,  # uid,\n                None,  # gid,\n                meta.get(\"size\", 0),  # size,\n                None,  # atime,\n                meta.get(\"last_modified\", 0).timestamp(),  # mtime,\n                None,  # ctime,\n            )\n        )\n\n    @property\n    def bucket(self) -&gt; str:\n        return self._no_prefix.split(\"/\", 1)[0]\n\n    @property\n    def key(self) -&gt; str:\n        key = self._no_prefix_no_drive\n\n        # key should never have starting slash for\n        # use with boto, etc.\n        if key.startswith(\"/\"):\n            key = key[1:]\n\n        return key\n\n    @property\n    def etag(self):\n        return self.client._get_metadata(self).get(\"etag\")\n</code></pre>"},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path-attributes","title":"Attributes","text":""},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.bucket","title":"<code>bucket: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.cloud_prefix","title":"<code>cloud_prefix: str</code>","text":""},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.drive","title":"<code>drive: str</code>  <code>property</code> <code>readonly</code>","text":"<p>The drive prefix (letter or UNC path), if any. (Docstring copied from pathlib.Path)</p>"},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.etag","title":"<code>etag</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.key","title":"<code>key: str</code>  <code>property</code> <code>readonly</code>","text":""},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path-methods","title":"Methods","text":""},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.is_dir","title":"<code>is_dir(self) -&gt; bool</code>","text":"<p>Whether this path is a directory.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/s3/s3path.py</code> <pre><code>def is_dir(self) -&gt; bool:\n    return self.client._is_file_or_dir(self) == \"dir\"\n</code></pre>"},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.is_file","title":"<code>is_file(self) -&gt; bool</code>","text":"<p>Whether this path is a regular file (also True for symlinks pointing to regular files).  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/s3/s3path.py</code> <pre><code>def is_file(self) -&gt; bool:\n    return self.client._is_file_or_dir(self) == \"file\"\n</code></pre>"},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.mkdir","title":"<code>mkdir(self, parents = False, exist_ok = False)</code>","text":"<p>Create a new directory at this given path.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/s3/s3path.py</code> <pre><code>def mkdir(self, parents=False, exist_ok=False):\n    # not possible to make empty directory on s3\n    pass\n</code></pre>"},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.stat","title":"<code>stat(self)</code>","text":"<p>Return the result of the stat() system call on this path, like os.stat() does.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/s3/s3path.py</code> <pre><code>def stat(self):\n    try:\n        meta = self.client._get_metadata(self)\n    except self.client.client.exceptions.NoSuchKey:\n        raise NoStatError(\n            f\"No stats available for {self}; it may be a directory or not exist.\"\n        )\n\n    return os.stat_result(\n        (\n            None,  # mode\n            None,  # ino\n            self.cloud_prefix,  # dev,\n            None,  # nlink,\n            None,  # uid,\n            None,  # gid,\n            meta.get(\"size\", 0),  # size,\n            None,  # atime,\n            meta.get(\"last_modified\", 0).timestamp(),  # mtime,\n            None,  # ctime,\n        )\n    )\n</code></pre>"},{"location":"api-reference/s3path/#cloudpathlib.s3.s3path.S3Path.touch","title":"<code>touch(self, exist_ok: bool = True)</code>","text":"<p>Create this file with the given access mode, if it doesn't exist.  (Docstring copied from pathlib.Path)</p> Source code in <code>cloudpathlib/s3/s3path.py</code> <pre><code>def touch(self, exist_ok: bool = True):\n    if self.exists():\n        if not exist_ok:\n            raise FileExistsError(f\"File exists: {self}\")\n        self.client._move_file(self, self)\n    else:\n        tf = TemporaryDirectory()\n        p = Path(tf.name) / \"empty\"\n        p.touch()\n\n        self.client._upload_file(p, self)\n\n        tf.cleanup()\n</code></pre>"},{"location":"script/caching/","title":"Caching","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>from cloudpathlib import CloudPath\nfrom itertools import islice\n</pre> from cloudpathlib import CloudPath from itertools import islice In\u00a0[\u00a0]: Copied! <pre>ladi = CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349\")\n</pre> ladi = CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349\") In\u00a0[\u00a0]: Copied! <pre># list first 5 images for this incident\nfor p in islice(ladi.iterdir(), 5):\n    print(p)\n</pre> # list first 5 images for this incident for p in islice(ladi.iterdir(), 5):     print(p) <p>Just because we saw these images are available, it doesn't mean we have downloaded any of this data yet.</p> In\u00a0[\u00a0]: Copied! <pre># Nothing in the cache yet\nget_ipython().system('tree {ladi.fspath}')\n</pre> # Nothing in the cache yet get_ipython().system('tree {ladi.fspath}') <p>Now let's look at just the first image from this dataset, confirming that the file exists on S3.</p> In\u00a0[\u00a0]: Copied! <pre>flood_image = ladi / \"DSC_0001_5a63d42e-27c6-448a-84f1-bfc632125b8e.jpg\"\nflood_image.exists()\n</pre> flood_image = ladi / \"DSC_0001_5a63d42e-27c6-448a-84f1-bfc632125b8e.jpg\" flood_image.exists() In\u00a0[\u00a0]: Copied! <pre># Still nothing in the cache\nget_ipython().system('tree {ladi.fspath}')\n</pre> # Still nothing in the cache get_ipython().system('tree {ladi.fspath}') <p>Even though we refer to a specific file and make sure it exists in the cloud, we can still do all of that work without actually downloading the file.</p> <p>In order to read the file, we do have to download the data. Let's actually display the image:</p> In\u00a0[\u00a0]: Copied! <pre>get_ipython().run_cell_magic('time', '', '%matplotlib inline\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image\\n\\nwith flood_image.open(\"rb\") as f:\\n    i = Image.open(f)\\n    plt.imshow(i)\\n')\n</pre> get_ipython().run_cell_magic('time', '', '%matplotlib inline\\nimport matplotlib.pyplot as plt\\nfrom PIL import Image\\n\\nwith flood_image.open(\"rb\") as f:\\n    i = Image.open(f)\\n    plt.imshow(i)\\n') In\u00a0[\u00a0]: Copied! <pre># Downloaded image file in the cache\nget_ipython().system('tree {ladi.fspath}')\n</pre> # Downloaded image file in the cache get_ipython().system('tree {ladi.fspath}') <p>Just by using <code>open</code>, we've downloaded the file in the background to the cache. Now that it is local, we won't redownload that file unless it changes on the server. We can confirm that by checking if the file is faster to read a second time.</p> In\u00a0[\u00a0]: Copied! <pre>get_ipython().run_cell_magic('time', '', 'with flood_image.open(\"rb\") as f:\\n    i = Image.open(f)\\n    plt.imshow(i)\\n')\n</pre> get_ipython().run_cell_magic('time', '', 'with flood_image.open(\"rb\") as f:\\n    i = Image.open(f)\\n    plt.imshow(i)\\n') <p>Notice that the second display is much faster since we use the cached version!</p> In\u00a0[\u00a0]: Copied! <pre>from cloudpathlib import S3Client\n</pre> from cloudpathlib import S3Client In\u00a0[\u00a0]: Copied! <pre># explicitly instantiate a client that always uses the local cache\nclient = S3Client(local_cache_dir=\"data\")\n</pre> # explicitly instantiate a client that always uses the local cache client = S3Client(local_cache_dir=\"data\") In\u00a0[\u00a0]: Copied! <pre>ladi = client.CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349\")\n</pre> ladi = client.CloudPath(\"s3://ladi/Images/FEMA_CAP/2020/70349\") In\u00a0[\u00a0]: Copied! <pre># Again, nothing in the cache yet, but we the path is now in the \"data\" folder\nget_ipython().system('tree {ladi.fspath}')\n</pre> # Again, nothing in the cache yet, but we the path is now in the \"data\" folder get_ipython().system('tree {ladi.fspath}') <p>Now let's look at just the first image from this dataset. Note that paths created by using the <code>ladi</code> root (e.g., by using the <code>/</code> operator below or calls like <code>iterdir</code> and <code>glob</code>) will inherit the same <code>Client</code> instance, and therefore the same <code>local_cache_dir</code> without our having to do extra work.</p> In\u00a0[\u00a0]: Copied! <pre>flood_image = ladi / \"DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n</pre> flood_image = ladi / \"DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" In\u00a0[\u00a0]: Copied! <pre>with flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    plt.imshow(i)\n</pre> with flood_image.open(\"rb\") as f:     i = Image.open(f)     plt.imshow(i) In\u00a0[\u00a0]: Copied! <pre># Now just this one image file is in the cache\nget_ipython().system('tree {ladi.fspath}')\n</pre> # Now just this one image file is in the cache get_ipython().system('tree {ladi.fspath}') In\u00a0[\u00a0]: Copied! <pre># let's explicitly cleanup this directory, since it is not handled for us\nget_ipython().system('rm -rf data')\n</pre> # let's explicitly cleanup this directory, since it is not handled for us get_ipython().system('rm -rf data') In\u00a0[\u00a0]: Copied! <pre>from cloudpathlib.enums import FileCacheMode\n</pre> from cloudpathlib.enums import FileCacheMode In\u00a0[\u00a0]: Copied! <pre>print(\"\\n\".join(FileCacheMode))\n</pre> print(\"\\n\".join(FileCacheMode)) In\u00a0[\u00a0]: Copied! <pre># pass as string to client instantiation\nno_cache_client = S3Client(file_cache_mode=\"close_file\")\n</pre> # pass as string to client instantiation no_cache_client = S3Client(file_cache_mode=\"close_file\") In\u00a0[\u00a0]: Copied! <pre>flood_image = no_cache_client.CloudPath(\n    \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n)\n</pre> flood_image = no_cache_client.CloudPath(     \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" ) In\u00a0[\u00a0]: Copied! <pre>with flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    print(\"Image loaded...\")\n</pre> with flood_image.open(\"rb\") as f:     i = Image.open(f)     print(\"Image loaded...\") In\u00a0[\u00a0]: Copied! <pre># Illustrate that even though we read the file, the cache version does not exist\nprint(\"Cache file exists after finished reading: \", flood_image._local.exists())\n</pre> # Illustrate that even though we read the file, the cache version does not exist print(\"Cache file exists after finished reading: \", flood_image._local.exists()) In\u00a0[\u00a0]: Copied! <pre># pass enum to client instantiation\ncloud_path_client = S3Client(file_cache_mode=FileCacheMode.cloudpath_object)\n</pre> # pass enum to client instantiation cloud_path_client = S3Client(file_cache_mode=FileCacheMode.cloudpath_object) In\u00a0[\u00a0]: Copied! <pre>flood_image = cloud_path_client.CloudPath(\n    \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n)\n</pre> flood_image = cloud_path_client.CloudPath(     \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" ) In\u00a0[\u00a0]: Copied! <pre>with flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    print(\"Image loaded...\")\n</pre> with flood_image.open(\"rb\") as f:     i = Image.open(f)     print(\"Image loaded...\") In\u00a0[\u00a0]: Copied! <pre># cache exists while the CloudPath object persists\nlocal_cached_file = flood_image._local\nprint(\"Cache file exists after finished reading: \", local_cached_file.exists())\n</pre> # cache exists while the CloudPath object persists local_cached_file = flood_image._local print(\"Cache file exists after finished reading: \", local_cached_file.exists()) In\u00a0[\u00a0]: Copied! <pre># decrement reference count so garbage collector cleans up the file\ndel flood_image\n</pre> # decrement reference count so garbage collector cleans up the file del flood_image In\u00a0[\u00a0]: Copied! <pre># file is now cleaned up\nprint(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists())\n</pre> # file is now cleaned up print(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists()) In\u00a0[\u00a0]: Copied! <pre>tmp_dir_client = S3Client()\n</pre> tmp_dir_client = S3Client() In\u00a0[\u00a0]: Copied! <pre>flood_image = tmp_dir_client.CloudPath(\n    \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n)\n</pre> flood_image = tmp_dir_client.CloudPath(     \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" ) In\u00a0[\u00a0]: Copied! <pre>with flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    print(\"Image loaded...\")\n</pre> with flood_image.open(\"rb\") as f:     i = Image.open(f)     print(\"Image loaded...\") In\u00a0[\u00a0]: Copied! <pre># cache exists while the CloudPath object persists\nlocal_cached_file = flood_image._local\nprint(\"Cache file exists after finished reading: \", local_cached_file.exists())\n</pre> # cache exists while the CloudPath object persists local_cached_file = flood_image._local print(\"Cache file exists after finished reading: \", local_cached_file.exists()) In\u00a0[\u00a0]: Copied! <pre># decrement reference count so garbage collection runs\ndel flood_image\n</pre> # decrement reference count so garbage collection runs del flood_image In\u00a0[\u00a0]: Copied! <pre># file still exists\nprint(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists())\n</pre> # file still exists print(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists()) In\u00a0[\u00a0]: Copied! <pre># decrement reference count so garbage collector removes the client\ndel tmp_dir_client\n</pre> # decrement reference count so garbage collector removes the client del tmp_dir_client In\u00a0[\u00a0]: Copied! <pre># file still exists\nprint(\"Cache file exists after Client is no longer referenced: \", local_cached_file.exists())\n</pre> # file still exists print(\"Cache file exists after Client is no longer referenced: \", local_cached_file.exists()) In\u00a0[\u00a0]: Copied! <pre>persistent_client = S3Client(local_cache_dir=\"./cache\")\n</pre> persistent_client = S3Client(local_cache_dir=\"./cache\") In\u00a0[\u00a0]: Copied! <pre># cache mode set automatically to persistent if local_cache_dir and not explicit\nprint(\"Client cache mode set to: \", persistent_client.file_cache_mode)\n</pre> # cache mode set automatically to persistent if local_cache_dir and not explicit print(\"Client cache mode set to: \", persistent_client.file_cache_mode) In\u00a0[\u00a0]: Copied! <pre># Just uses default client\nflood_image = persistent_client.CloudPath(\n    \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\"\n)\n</pre> # Just uses default client flood_image = persistent_client.CloudPath(     \"s3://ladi/Images/FEMA_CAP/2020/70349/DSC_0002_a89f1b79-786f-4dac-9dcc-609fb1a977b1.jpg\" ) In\u00a0[\u00a0]: Copied! <pre>with flood_image.open(\"rb\") as f:\n    i = Image.open(f)\n    print(\"Image loaded...\")\n</pre> with flood_image.open(\"rb\") as f:     i = Image.open(f)     print(\"Image loaded...\") In\u00a0[\u00a0]: Copied! <pre># cache exists while the CloudPath object persists\nlocal_cached_file = flood_image._local\nprint(\"Cache file exists after finished reading: \", local_cached_file.exists())\n</pre> # cache exists while the CloudPath object persists local_cached_file = flood_image._local print(\"Cache file exists after finished reading: \", local_cached_file.exists()) In\u00a0[\u00a0]: Copied! <pre># decrement reference count so garbage collection runs\ndel flood_image\n</pre> # decrement reference count so garbage collection runs del flood_image In\u00a0[\u00a0]: Copied! <pre># file still exists\nprint(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists())\n</pre> # file still exists print(\"Cache file exists after CloudPath is no longer referenced: \", local_cached_file.exists()) In\u00a0[\u00a0]: Copied! <pre># decrement reference count so garbage collector removes the client\nclient_cache_dir = persistent_client._local_cache_dir\ndel persistent_client\n</pre> # decrement reference count so garbage collector removes the client client_cache_dir = persistent_client._local_cache_dir del persistent_client In\u00a0[\u00a0]: Copied! <pre># file still exists\nprint(\"Cache file exists after Client is no longer referenced: \", local_cached_file.exists())\n</pre> # file still exists print(\"Cache file exists after Client is no longer referenced: \", local_cached_file.exists()) In\u00a0[\u00a0]: Copied! <pre># explicitly remove persistent cache file\nimport shutil\n</pre> # explicitly remove persistent cache file import shutil In\u00a0[\u00a0]: Copied! <pre>shutil.rmtree(client_cache_dir)\n</pre> shutil.rmtree(client_cache_dir) <p>We show an example below of <code>InvalidConfigurationException</code> being raised with the mode being interpreted from the environment variable.</p> In\u00a0[\u00a0]: Copied! <pre>import os\n</pre> import os In\u00a0[\u00a0]: Copied! <pre># set the mode here so that it will be used when we instantiate the client\nos.environ[\"CLOUDPATHLIB_FILE_CACHE_MODE\"] = \"persistent\"\n</pre> # set the mode here so that it will be used when we instantiate the client os.environ[\"CLOUDPATHLIB_FILE_CACHE_MODE\"] = \"persistent\" In\u00a0[\u00a0]: Copied! <pre>tmp_dir_client = S3Client()\n</pre> tmp_dir_client = S3Client()"},{"location":"script/caching/#caching","title":"Caching\u00b6","text":"<p>Interacting with files on a cloud provider can mean a lot of waiting on files downloading and uploading. <code>cloudpathlib</code> provides seamless on-demand caching of cloud content that can be persistent across processes and sessions to make sure you only download or upload when you need to.</p>"},{"location":"script/caching/#are-we-synced","title":"Are we synced?\u00b6","text":"<p>Before <code>cloudpathlib</code>, we spent a lot of time syncing our remote and local files. There was no great solution. For example, I just need one file, but I only have a script that downloads the entire 800GB bucket (or worse, you can't remember exactly which files you need \ud83e\udd2e). Or even worse, you have all the files synced to your local machine, but you suspect that some are are up-to-date and some are stale. More often that I'd like to admit, the simplest answer was to blast the whole data directory and download all over again. Bandwidth doesn't grow on trees!</p>"},{"location":"script/caching/#cache-me-if-you-can","title":"Cache me if you can\u00b6","text":"<p>Part of what makes <code>cloudpathlib</code> so useful is that it takes care of all of that, leaving your precious mental resources free to do other things! It maintains a local cache and only downloads a file if the local version and remote versions are out of sync. Every time you read or write a file, <code>cloudpathlib</code> goes through these steps:</p> <ul> <li>Does the file exist in the cache already?</li> <li>If no, download it to the cache.</li> <li>If yes, does the cached version have the same modified time as the cloud version?</li> <li>If it is older, re-download the file and replace the old cached version with the updated version from the cloud.</li> <li>If the local one is newer, something is up! We don't want to overwrite your local changes with the version from the cloud. If we see this scenario, we'll raise an error and offer some options to resolve the versions.</li> </ul>"},{"location":"script/caching/#supporting-reading-and-writing","title":"Supporting reading and writing\u00b6","text":"<p>The cache logic also support writing to cloud files seamlessly in addition to reading. We do this by tracking when a <code>CloudPath</code> is opened and on the close of that file, we will upload the new version to the cloud if it has changed.</p> <p>Warning we don't upload files that weren't opened for write by <code>cloudpathlib</code>. For example, if you edit a file in the cache manually in a text edior, <code>cloudpathlib</code> won't know to update that file on the cloud. If you want to write to a file in the cloud, you should use the <code>open</code> or <code>write</code> methods, for example:</p> <pre>with my_cloud_path.open(\"w\") as f:\n    f.write(\"My new text!\")\n</pre> <p>This will download the file, write the text to the local version in the cache, and when that file is closed we know to upload the changed version to the cloud.</p> <p>As an example, let's look at using the Low Altitude Disaster Imagery open dataset on S3. We'll view one images available of a flooding incident available on S3.</p>"},{"location":"script/caching/#keeping-the-cache-around","title":"Keeping the cache around\u00b6","text":"<p>By default, the cache uses <code>tempfile</code> this means at some point either Python or your operating system will remove whatever files you have cached. This is helpful in that it means the downloaded files get cleaned up regularly and don't necessarily clutter up your local hard drive. If you want more control over how and when the cache is removed, see the Clearing the file cache section.</p> <p>However, sometimes I don't want to have to re-download files I know won't change. For example, in the LADI dataset, I may want to use the images in a Jupyter notebook and every time I restart the notebook I want to always have the downloaded files. I don't want to ever re-download since I know the LADI images won't be changing on S3. I want these to be there, even if I restart my whole machine.</p> <p>We can do this just by using a <code>Client</code> that does all the downloading/uploading to a specfic folder on our local machine. We set the cache folder by passing <code>local_cache_dir</code> to the <code>Client</code> when instantiating. You can also set a default for all clients by setting the <code>CLOUDPATHLIB_LOCAL_CACHE_DIR</code> to a path. (This is only recommended with (1) an absolute path, so you know where the cache is no matter where your code is running, and (2) if you only use the default client for one cloud provider and don't instantiate multiple. In this case, the clients will use the same cache dir and could overwrite each other's content. Setting <code>CLOUDPATHLIB_LOCAL_CACHE_DIR</code> to an empty string will be treated as it not being set.)</p>"},{"location":"script/caching/#accessing-the-cached-version-directly-read-only","title":"Accessing the cached version directly (read-only)\u00b6","text":"<p>Many Python libraries don't properly handle <code>PathLike</code> objects. These libraries often only expect a <code>str</code> to be passed when working with files or, even worse, they will call <code>str(p)</code> on a Path that is passed before using it.</p> <p>To use <code>cloudpathlib</code> with these libraries, you can pass <code>.fspath</code> which will provide the path to the cached version of the file as a string.</p> <p>Warning: Using the <code>.fspath</code> property will download the file from the cloud if it does not exist yet in the cache.</p> <p>Warning: Since we are no longer in control of opening/closing the file, we cannot upload any changes when the file is closed. Therefore, you should treat any code where you use <code>fspath</code> as read only. Writes directly to <code>fspath</code> will not be uploaded to the cloud.</p>"},{"location":"script/caching/#handling-conflicts","title":"Handling conflicts\u00b6","text":"<p>We try to be conservative in terms of not losing data\u2014especially data stored on the cloud, which is likely to be the canonical version. Given this, we will raise exceptions in two scenarios:</p> <p><code>OverwriteNewerLocalError</code> This exception is raised if we are asked to download a file, but our local version in the cache is newer. This likely means that the cached version has been updated, but not pushed to the cloud. To work around this you could remove the cache version explicitly if you know you don't need that data. If you did write changes you need, make sure your code uses the <code>cloudpathlib</code> versions of the <code>open</code>, <code>write_text</code>, or <code>write_bytes</code> methods, which will upload your changes to the cloud automatically.</p> <p>The <code>CloudPath.open</code> method supports a <code>force_overwrite_from_cloud</code> kwarg to force overwriting your local version.</p> <p><code>OverwriteNewerCloudError</code> This exception is raised if we are asked to upload a file, but the one on the cloud is newer than our local version. This likely means that a separate process has updated the cloud version, and we don't want to overwrite and lose that new data in the cloud.</p> <p>The <code>CloudPath.open</code> method supports a <code>force_overwrite_to_cloud</code> kwarg to force overwriting the cloud version.</p>"},{"location":"script/caching/#clearing-the-file-cache","title":"Clearing the file cache\u00b6","text":"<p>There's no perfect strategy for when to clear the file cache, and different applications will have different requirements and preferences. <code>cloudpathlib</code> provides fine-grained control over when cloud files are removed from the local disk. The cache can be emptied both manually and automatically. Because <code>cloudpathlib</code> uploads any changed files opened with <code>CloudPath.open</code> to the cloud as soon as they are closed, it is safe to delete the cached version of the file at any point as long as the file is not opened for writing at the time you are trying to remove it. If necessary, the file will be re-downloaded next time it is needed.</p> <p>We provide a number of ways to clear the file cache that can be useful if you want to reclaim disk space or if you don't need to keep cached files around.</p>"},{"location":"script/caching/#manually","title":"Manually\u00b6","text":"<p>It is recommended you pick an automatic strategy that works for your application. However, if you need to, you can clear the cache manually in three different ways: for individual CloudPath files, at the level of a <code>Client</code> instance, or at the operating system level.</p> <ul> <li><code>CloudPath.clear_cache()</code> - for an individual <code>CloudPath</code> remove the cached version of the file if it exists.</li> <li><code>*Client.clear_cache()</code> - All files downloaded by this specific client instance will be removed from the cache. If you didn't create a client instance yourself, you can get the one that is used by a cloudpath with <code>CloudPath.client</code> or get the default one for a particular provider with <code>get_default_client</code>, for example by calling <code>S3Client.get_default_client().clear_cache()</code>.</li> <li>By deleting the cached file itself or the containing directory using any normal method. To see where on a disk the cache is, you can use <code>CloudPath.fspath</code> for an individual file or use <code>*Client._local_cache_dir</code> for the client's cache. You can then use any method you like to delete these local files.</li> </ul> <p>However, for most cases, you shouldn't need to manage the file cache manually. By setting the automatic cache clearing beahvior to the most appropriate one for your use case below, you can have the cache automatically cleared.</p>"},{"location":"script/caching/#automatically","title":"Automatically\u00b6","text":"<p>We provide a number of different ways for the cache to get cleared automatically for you depending on your use case. These range from no cache clearing done by <code>cloudpathlib</code> (<code>\"persistent\"</code>), to the most aggressive (<code>\"close_file\"</code>), which deletes a file from the cache as soon as the file handle is closed and the file is uploaded to the cloud, if it was changed).</p> <p>The modes are defined in the <code>FileCacheMode</code> enum, which you can use directly or you can use the corresponding string value. Examples of both methods are included below.</p> <p>Note: There is not currently a cache mode that never writes a file to disk and only keeps it in memory.</p> <ul> <li><code>\"persistent\"</code> - <code>cloudpathlib</code> does not clear the cache at all. In this case, you must also pass a <code>local_cache_dir</code> when you instantiate the client.</li> <li><code>\"tmp_dir\"</code> (default) - Cached files are saved using Python's <code>TemporaryDirectory</code>. This provides three potential avenues for the cache to get cleared. First, cached files are removed by <code>cloudpathlib</code> when the <code>*Client</code> object is garbage collected. This happens on the next garbage collection run after the object leaves scope or <code>del</code> is called. Second, Python clears a temporary directory if all references to that directory leave scope. Finally since the folder is in an operating system temp directory, it will be cleared by the OS (which, depending on the OS, may not happen until system restart).</li> <li><code>\"cloudpath_object\"</code> - cached files are removed when the <code>CloudPath</code> object is garbage collected. This happens on the next garbage collection run after the object leaves scope or <code>del</code> is called.</li> <li><code>\"close_file\"</code> - since we only download a file to the cache on read/write, we can ensure the cache is empty by removing the cached file as soon as the read/write is finished. Reading/writing the same <code>CloudPath</code> multiple times will result in re-downloading the file from the cloud. Note: For this to work, <code>cloudpath</code> needs to be in control of the reading/writing of files. This means your code base should use the <code>CloudPath.write_*</code>, <code>CloudPath.read_*</code>, and <code>CloudPath.open</code> methods. Using <code>CloudPath.fspath</code> (or passing the <code>CloudPath</code> as a <code>PathLike</code> object to another library) will not clear the cache on file close since it was not opened by <code>cloudpathlib</code>.</li> </ul> <p>Note: Although we use it in the examples below, for <code>\"cloudpath_object\"</code> and <code>\"tmp_dir\"</code> you normally shouldn't need to explicitly call <code>del</code>. Letting Python garbage collection run on its own once all references to the object leave scope should be sufficient. See details in the Python docs).</p>"},{"location":"script/caching/#setting-the-cache-clearing-method","title":"Setting the cache clearing method\u00b6","text":"<p>You can set the cache clearing method either through the environment variable <code>CLOUDPATHLIB_FILE_CACHE_MODE</code> or by passing the mode to the <code>*Client</code> when you instantiate it. See below for an example.</p> <p>You can set <code>CLOUDPATHLIB_FILE_CACHE_MODE</code> to any of the supported values, which are printed below.</p>"},{"location":"script/caching/#file-cache-mode-close_file","title":"File cache mode: <code>\"close_file\"</code>\u00b6","text":"<p>Example instantiation by passing a string to the client.</p> <p>Local cache file is gone as soon as file is closed for reading.</p>"},{"location":"script/caching/#file-cache-mode-cloudpath_object","title":"File cache mode: <code>\"cloudpath_object\"</code>\u00b6","text":"<p>Example instantiation by passing enum member to the client.</p> <p>Local cache file exists after file is closed for reading.</p> <p>Local cache file is gone after <code>CloudPath</code> is no longer referenced (done explicitly with <code>del</code> here, but is usually called automatically by the garbage collector. See details in the Python docs).</p>"},{"location":"script/caching/#file-cache-mode-tmp_dir-default","title":"File cache mode: <code>\"tmp_dir\"</code> (default)\u00b6","text":"<p>Local cache file exists after file is closed for reading.</p> <p>Local cache file exists after <code>CloudPath</code> is no longer referenced.</p> <p>Local cache file is gone after the <code>Client</code> object is no longer referenced (done explicitly with <code>del</code> here, but is usually called automatically by the garbage collector. See details in the Python docs).</p>"},{"location":"script/caching/#file-cache-mode-persistent","title":"File cache mode: <code>\"persistent\"</code>\u00b6","text":"<p>If <code>local_cache_dir</code> is specificed, but <code>file_cache_mode</code> is not, then the mode is set to <code>\"persistent\"</code> automatically. Conversely, if you set the mode to <code>\"persistent\"</code> explicitly, you must also pass <code>local_cache_dir</code> or the <code>Client</code> will raise <code>InvalidConfigurationException</code>.</p> <p>Local cache file exists after file is closed for reading.</p> <p>Local cache file exists after <code>CloudPath</code> is no longer referenced.</p> <p>Local cache exists after the <code>Client</code> object is no longer referenced.</p>"},{"location":"script/caching/#caveats","title":"Caveats\u00b6","text":"<ul> <li><p>Automatic cache clearing works in most contexts, but there can be cases where execution of a program is halted before <code>cloudpathlib</code>'s cache clearing code is able to run. It is a good practice to monitor your cache folders and, if using temporary directories, trigger your operating system's temp directory clean up (which is OS-dependent, but restarting is usually sufficient).</p> </li> <li><p>Using <code>with CloudPath.open()</code> as a context manager to open files for read or write is the best way to ensure that automatic cache clearing happens consistently. The <code>\"close_file\"</code> cache clearing mode will not work as expected if you use another method to open files (e.g., calling the Python built-in <code>open</code>, using <code>CloudPath.fspath</code>, or where another library handles the opening/closing of the file).</p> </li> <li><p>The <code>download_to</code> and <code>upload_from</code> methods do not cache the file, since we assume if you are downloading or uploading you explicitly want the file to be in a particular location or know where it is already.</p> </li> </ul>"},{"location":"script/testing_mocked_cloudpathlib/","title":"Testing code that uses cloudpathlib","text":"In\u00a0[\u00a0]: Copied! <p>Testing code that interacts with external resources can be a pain. For automated unit tests, the best practice is to mock connections. We provide some tools in cloudpathlib to make mocking easier.</p> <p>In this section, we will show a few examples of how to mock cloudpathlib classes in the popular pytest framework using its monkeypatch feature. The general principles should work equivalently if you are using unittest.mock from the Python standard library. If you are new to mocking or having trouble applying it, we recommend you read and understand \"Where to patch\".</p> In\u00a0[\u00a0]: Copied! <pre>import ipytest\n</pre> import ipytest In\u00a0[\u00a0]: Copied! <pre>ipytest.autoconfig()\n</pre> ipytest.autoconfig() <p>In this example, we are testing a function <code>write</code> that directly instantiates a path using the <code>S3Path</code> constructor.</p> <p>Normally, calling <code>write</code> would either write to the real S3 bucket if able to authenticate, or it would fail with an error like <code>botocore.exceptions.NoCredentialsError</code>.</p> <p>We use <code>monkeypatch</code> to replace the reference to <code>S3Path</code> being used with <code>LocalS3Path</code>. Our write succeeds (despite not being authenticated), and we can double-check that the cloud path object returned is actually an instance of <code>LocalS3Path</code>.</p> <p>Note that if you are writing tests for a package, and you import <code>write</code> from another module, you should patch the reference to <code>S3Path</code> from that module instead.</p> In\u00a0[\u00a0]: Copied! <pre>get_ipython().run_cell_magic('run_pytest[clean]', '', '\\nimport cloudpathlib\\nfrom cloudpathlib.local import LocalS3Path\\n\\n\\ndef write(uri: str):\\n    \"\"\"Function that uses S3Path.\"\"\"\\n    cloud_path = cloudpathlib.S3Path(uri)\\n    cloud_path.write_text(\"cumulonimbus\")\\n    return cloud_path\\n\\n\\ndef test_write_monkeypatch(monkeypatch):\\n    \"\"\"Testing function using S3Path, patching with LocalS3Path.\"\"\"\\n\\n    monkeypatch.setattr(cloudpathlib, \"S3Path\", LocalS3Path)\\n\\n    cloud_path = write(\"s3://cloudpathlib-test-bucket/cumulonimbus.txt\")\\n    assert isinstance(cloud_path, LocalS3Path)\\n    assert cloud_path.read_text() == \"cumulonimbus\"\\n')\n</pre> get_ipython().run_cell_magic('run_pytest[clean]', '', '\\nimport cloudpathlib\\nfrom cloudpathlib.local import LocalS3Path\\n\\n\\ndef write(uri: str):\\n    \"\"\"Function that uses S3Path.\"\"\"\\n    cloud_path = cloudpathlib.S3Path(uri)\\n    cloud_path.write_text(\"cumulonimbus\")\\n    return cloud_path\\n\\n\\ndef test_write_monkeypatch(monkeypatch):\\n    \"\"\"Testing function using S3Path, patching with LocalS3Path.\"\"\"\\n\\n    monkeypatch.setattr(cloudpathlib, \"S3Path\", LocalS3Path)\\n\\n    cloud_path = write(\"s3://cloudpathlib-test-bucket/cumulonimbus.txt\")\\n    assert isinstance(cloud_path, LocalS3Path)\\n    assert cloud_path.read_text() == \"cumulonimbus\"\\n') In\u00a0[\u00a0]: Copied! <pre>from cloudpathlib import implementation_registry\n</pre> from cloudpathlib import implementation_registry In\u00a0[\u00a0]: Copied! <pre>implementation_registry\n</pre> implementation_registry <p>We use <code>monkeypatch</code> to replace the <code>CloudImplementation</code> object in the registry that is keyed to <code>\"s3\"</code> with the <code>local_s3_implementation</code> object that we import from the <code>cloudpathlib.local</code> module. Our write succeeds, and we can double-check that the created cloud path object is indeed a <code>LocalS3Path</code> instance.</p> In\u00a0[\u00a0]: Copied! <pre>get_ipython().run_cell_magic('run_pytest[clean]', '', '\\nfrom cloudpathlib import CloudPath, implementation_registry\\nfrom cloudpathlib.local import LocalS3Path, local_s3_implementation\\n\\n\\ndef write_with_dispatch(uri: str):\\n    \"\"\"Function that uses CloudPath to dispatch to S3Path.\"\"\"\\n    cloud_path = CloudPath(uri)\\n    cloud_path.write_text(\"cirrocumulus\")\\n    return cloud_path\\n\\n\\ndef test_write_with_dispatch_monkeypatch(monkeypatch):\\n    \"\"\"Testing function using CloudPath dispatch, patching registered implementation. Will pass.\"\"\"\\n\\n    monkeypatch.setitem(implementation_registry, \"s3\", local_s3_implementation)\\n\\n    cloud_path = write_with_dispatch(\"s3://cloudpathlib-test-bucket/cirrocumulus.txt\")\\n    assert isinstance(cloud_path, LocalS3Path)\\n    assert cloud_path.read_text() == \"cirrocumulus\"\\n')\n</pre> get_ipython().run_cell_magic('run_pytest[clean]', '', '\\nfrom cloudpathlib import CloudPath, implementation_registry\\nfrom cloudpathlib.local import LocalS3Path, local_s3_implementation\\n\\n\\ndef write_with_dispatch(uri: str):\\n    \"\"\"Function that uses CloudPath to dispatch to S3Path.\"\"\"\\n    cloud_path = CloudPath(uri)\\n    cloud_path.write_text(\"cirrocumulus\")\\n    return cloud_path\\n\\n\\ndef test_write_with_dispatch_monkeypatch(monkeypatch):\\n    \"\"\"Testing function using CloudPath dispatch, patching registered implementation. Will pass.\"\"\"\\n\\n    monkeypatch.setitem(implementation_registry, \"s3\", local_s3_implementation)\\n\\n    cloud_path = write_with_dispatch(\"s3://cloudpathlib-test-bucket/cirrocumulus.txt\")\\n    assert isinstance(cloud_path, LocalS3Path)\\n    assert cloud_path.read_text() == \"cirrocumulus\"\\n') <p>In this example, we set up test assets in a pytest fixture before running our tests. (We also do the monkeypatching in the fixture\u2014a code pattern for better reuse.)</p> <p>There are two options for interacting with the storage backend for the local path classes. The example fixture below shows both options in action.</p> In\u00a0[\u00a0]: Copied! <pre>get_ipython().run_cell_magic('run_pytest[clean]', '', '\\nimport pytest\\n\\nfrom cloudpathlib import CloudPath, implementation_registry\\nfrom cloudpathlib.local import LocalS3Client, LocalS3Path, local_s3_implementation\\n\\n\\n@pytest.fixture\\ndef cloud_asset_file(monkeypatch):\\n    \"\"\"Fixture that patches CloudPath dispatch and also sets up test assets in LocalS3Client\\'s\\n    local storage directory.\"\"\"\\n\\n    monkeypatch.setitem(implementation_registry, \"s3\", local_s3_implementation)\\n\\n    # Option 1: Use LocalS3Path to set up test assets directly\\n    local_cloud_path = LocalS3Path(\"s3://cloudpathlib-test-bucket/altostratus.txt\")\\n    local_cloud_path.write_text(\"altostratus\")\\n    \\n    # Option 2: Use the pathlib.Path object that points to the local storage directory\\n    local_pathlib_path: Path = (\\n        LocalS3Client.get_default_storage_dir() / \"cloudpathlib-test-bucket\" / \"nimbostratus.txt\"\\n    )\\n    local_pathlib_path.parent.mkdir(exist_ok=True, parents=True)\\n    local_pathlib_path.write_text(\"nimbostratus\")\\n\\n    yield\\n\\n    LocalS3Client.reset_default_storage_dir()  # clean up temp directory and replace with new one\\n\\n\\ndef test_with_assets(cloud_asset_file):\\n    \"\"\"Testing that a patched CloudPath finds the test asset created in the fixture.\"\"\"\\n\\n    cloud_path_1 = CloudPath(\"s3://cloudpathlib-test-bucket/altostratus.txt\")\\n    assert isinstance(cloud_path_1, LocalS3Path)\\n    assert cloud_path_1.exists()\\n    assert cloud_path_1.read_text() == \"altostratus\"\\n    \\n    cloud_path_2 = CloudPath(\"s3://cloudpathlib-test-bucket/nimbostratus.txt\")\\n    assert isinstance(cloud_path_2, LocalS3Path)\\n    assert cloud_path_2.exists()\\n    assert cloud_path_2.read_text() == \"nimbostratus\"\\n')\n</pre> get_ipython().run_cell_magic('run_pytest[clean]', '', '\\nimport pytest\\n\\nfrom cloudpathlib import CloudPath, implementation_registry\\nfrom cloudpathlib.local import LocalS3Client, LocalS3Path, local_s3_implementation\\n\\n\\n@pytest.fixture\\ndef cloud_asset_file(monkeypatch):\\n    \"\"\"Fixture that patches CloudPath dispatch and also sets up test assets in LocalS3Client\\'s\\n    local storage directory.\"\"\"\\n\\n    monkeypatch.setitem(implementation_registry, \"s3\", local_s3_implementation)\\n\\n    # Option 1: Use LocalS3Path to set up test assets directly\\n    local_cloud_path = LocalS3Path(\"s3://cloudpathlib-test-bucket/altostratus.txt\")\\n    local_cloud_path.write_text(\"altostratus\")\\n    \\n    # Option 2: Use the pathlib.Path object that points to the local storage directory\\n    local_pathlib_path: Path = (\\n        LocalS3Client.get_default_storage_dir() / \"cloudpathlib-test-bucket\" / \"nimbostratus.txt\"\\n    )\\n    local_pathlib_path.parent.mkdir(exist_ok=True, parents=True)\\n    local_pathlib_path.write_text(\"nimbostratus\")\\n\\n    yield\\n\\n    LocalS3Client.reset_default_storage_dir()  # clean up temp directory and replace with new one\\n\\n\\ndef test_with_assets(cloud_asset_file):\\n    \"\"\"Testing that a patched CloudPath finds the test asset created in the fixture.\"\"\"\\n\\n    cloud_path_1 = CloudPath(\"s3://cloudpathlib-test-bucket/altostratus.txt\")\\n    assert isinstance(cloud_path_1, LocalS3Path)\\n    assert cloud_path_1.exists()\\n    assert cloud_path_1.read_text() == \"altostratus\"\\n    \\n    cloud_path_2 = CloudPath(\"s3://cloudpathlib-test-bucket/nimbostratus.txt\")\\n    assert isinstance(cloud_path_2, LocalS3Path)\\n    assert cloud_path_2.exists()\\n    assert cloud_path_2.read_text() == \"nimbostratus\"\\n')"},{"location":"script/testing_mocked_cloudpathlib/#testing-code-that-uses-cloudpathlib","title":"Testing code that uses cloudpathlib\u00b6","text":""},{"location":"script/testing_mocked_cloudpathlib/#cloudpathliblocal-module","title":"cloudpathlib.local module\u00b6","text":"<p>In the <code>cloudpathlib.local</code> module, we provide \"Local\" classes that use the local filesystem in place of cloud storage. These classes are drop-in replacements for the normal cloud path classes, with the intent that you can use them as mock or monkeypatch substitutes in your tests.</p> <p>We also provide <code>CloudImplementation</code> objects which can be used to replace a registered implementation in the <code>cloudpathlib.implementation_registry</code> dictionary. Replacing the registered implementation will make <code>CloudPath</code>'s automatic dispatching use the replacement.</p> <p>See the examples below for how to use these replacements in your tests.</p> Cloud Provider Standard Classes Local Classes Local Implementation Object Azure Blob Storage <code>AzureBlobClient</code><code>AzureBlobPath</code> <code>LocalAzureBlobCient</code><code>LocalAzureBlobPath</code> <code>local_azure_blob_implementation</code> Google Cloud Storage <code>GSClient</code><code>GSPath</code> <code>LocalGSClient</code><code>LocalGSPath</code> <code>local_gs_implementation</code> Amazon S3 <code>S3Client</code><code>S3Path</code> <code>LocalS3Client</code><code>LocalS3Path</code> <code>local_s3_implementation</code>"},{"location":"script/testing_mocked_cloudpathlib/#examples-monkeypatching-in-pytest","title":"Examples: Monkeypatching in pytest\u00b6","text":""},{"location":"script/testing_mocked_cloudpathlib/#patching-direct-instantiation","title":"Patching direct instantiation\u00b6","text":""},{"location":"script/testing_mocked_cloudpathlib/#patching-cloudpath-dispatch","title":"Patching CloudPath dispatch\u00b6","text":"<p>In this example, we are testing a function <code>write_with_dispatch</code> that uses the <code>CloudPath</code> constructor which dispatches to <code>S3Path</code> based on the <code>\"s3://\"</code> URI scheme.</p> <p>In order to change the dispatch behavior, we need to patch the cloudpathlib <code>implementation_registry</code>. The registry object is a dictionary (actually <code>defaultdict</code>) that holds meta <code>CloudImplementation</code> objects for each cloud storage service.</p>"},{"location":"script/testing_mocked_cloudpathlib/#setting-up-test-assets","title":"Setting up test assets\u00b6","text":""},{"location":"script/testing_mocked_cloudpathlib/#1-use-local-path-class-methods","title":"1. Use local path class methods.\u00b6","text":"<p>This is the easiest and most direct approach. For example, <code>LocalS3Path</code> is fully functional and implements the same methods as <code>S3Path</code>.</p>"},{"location":"script/testing_mocked_cloudpathlib/#2-get-a-pathlibpath-object-that-points-to-the-local-storage-directory","title":"2. Get a <code>pathlib.Path</code> object that points to the local storage directory.\u00b6","text":"<p>Each <code>LocalClient</code> class has a <code>TemporaryDirectory</code> instance that serves as its default local storage location. This is stored as an attribute of the class so that it persists across client instances. (For real cloud clients, authenticating multiple times to the same storage location doesn't affect the contents.)</p> <p>You can use the <code>get_default_storage_dir</code> class method to get back a <code>pathlib.Path</code> object for that directory. Then you can use whatever <code>pathlib</code> or <code>shutil</code> functions to interact with it.</p> <p>Finally, the <code>reset_default_storage_dir</code> class method will clean up the current local storage temporary directory and set up a new one. We recommend you do this in the teardown of the test fixture.</p>"},{"location":"script/why_cloudpathlib/","title":"Why cloudpathlib?","text":"In\u00a0[\u00a0]: Copied! In\u00a0[\u00a0]: Copied! <pre>from pathlib import Path\n</pre> from pathlib import Path <p>For example, we can easily list all the files in a directory.</p> In\u00a0[\u00a0]: Copied! <pre>list(Path(\".\").glob(\"*\"))\n</pre> list(Path(\".\").glob(\"*\")) <p>There are methods to quickly learn everything there is to know about a filesystem path, and even do simple file manipulations.</p> In\u00a0[\u00a0]: Copied! <pre>notebook = Path(\"why_cloudpathlib.ipynb\").resolve()\n</pre> notebook = Path(\"why_cloudpathlib.ipynb\").resolve() In\u00a0[\u00a0]: Copied! <pre>print(f\"{'Path:':15}{notebook}\")\nprint(f\"{'Name:':15}{notebook.name}\")\nprint(f\"{'Stem:':15}{notebook.stem}\")\nprint(f\"{'Suffix:':15}{notebook.suffix}\")\nprint(f\"{'With suffix:':15}{notebook.with_suffix('.cpp')}\")\nprint(f\"{'Parent:':15}{notebook.parent}\")\nprint(f\"{'Read_text:'}\\n{notebook.read_text()[:200]}\\n\")\n</pre> print(f\"{'Path:':15}{notebook}\") print(f\"{'Name:':15}{notebook.name}\") print(f\"{'Stem:':15}{notebook.stem}\") print(f\"{'Suffix:':15}{notebook.suffix}\") print(f\"{'With suffix:':15}{notebook.with_suffix('.cpp')}\") print(f\"{'Parent:':15}{notebook.parent}\") print(f\"{'Read_text:'}\\n{notebook.read_text()[:200]}\\n\") <p>If you're new to pathlib, we highly recommend it over the older <code>os.path</code> module. We find that it has a much more intuitive and convenient interface. The official documentation is a helpful reference, and we also recommend this excellent cheat sheet by Chris Moffitt.</p> In\u00a0[\u00a0]: Copied! <pre># load environment variables from .env file;\n# not required, just where we keep our creds\nfrom dotenv import load_dotenv, find_dotenv\n</pre> # load environment variables from .env file; # not required, just where we keep our creds from dotenv import load_dotenv, find_dotenv In\u00a0[\u00a0]: Copied! <pre>load_dotenv(find_dotenv())\n</pre> load_dotenv(find_dotenv()) In\u00a0[\u00a0]: Copied! <pre>from cloudpathlib import S3Path\n</pre> from cloudpathlib import S3Path In\u00a0[\u00a0]: Copied! <pre>s3p = S3Path(\"s3://cloudpathlib-test-bucket/why_cloudpathlib/file.txt\")\ns3p.name\n</pre> s3p = S3Path(\"s3://cloudpathlib-test-bucket/why_cloudpathlib/file.txt\") s3p.name In\u00a0[\u00a0]: Copied! <pre># Nothing there yet...\ns3p.exists()\n</pre> # Nothing there yet... s3p.exists() In\u00a0[\u00a0]: Copied! <pre># Touch (just like with `pathlib.Path`)\ns3p.touch()\n</pre> # Touch (just like with `pathlib.Path`) s3p.touch() In\u00a0[\u00a0]: Copied! <pre># Bingo!\ns3p.exists()\n</pre> # Bingo! s3p.exists() In\u00a0[\u00a0]: Copied! <pre># list all the files in the directory\n[p for p in s3p.parent.iterdir()]\n</pre> # list all the files in the directory [p for p in s3p.parent.iterdir()] In\u00a0[\u00a0]: Copied! <pre>stat = s3p.stat()\nprint(f\"File size in bytes: {stat.st_size}\")\nstat\n</pre> stat = s3p.stat() print(f\"File size in bytes: {stat.st_size}\") stat In\u00a0[\u00a0]: Copied! <pre>s3p.write_text(\"Hello to all of my friends!\")\n</pre> s3p.write_text(\"Hello to all of my friends!\") In\u00a0[\u00a0]: Copied! <pre>stat = s3p.stat()\nprint(f\"File size in bytes: {stat.st_size}\")\nstat\n</pre> stat = s3p.stat() print(f\"File size in bytes: {stat.st_size}\") stat In\u00a0[\u00a0]: Copied! <pre># Delete (again just like with `pathlib.Path`)\ns3p.unlink()\n</pre> # Delete (again just like with `pathlib.Path`) s3p.unlink() In\u00a0[\u00a0]: Copied! <pre>s3p.exists()\n</pre> s3p.exists() In\u00a0[\u00a0]: Copied! <pre>from cloudpathlib import AzureBlobPath\n</pre> from cloudpathlib import AzureBlobPath In\u00a0[\u00a0]: Copied! <pre>azp = AzureBlobPath(\"az://cloudpathlib-test-container/file.txt\")\nazp.name\n</pre> azp = AzureBlobPath(\"az://cloudpathlib-test-container/file.txt\") azp.name In\u00a0[\u00a0]: Copied! <pre>azp.exists()\n</pre> azp.exists() In\u00a0[\u00a0]: Copied! <pre>azp.write_text(\"I'm on Azure, boss.\")\n</pre> azp.write_text(\"I'm on Azure, boss.\") In\u00a0[\u00a0]: Copied! <pre>azp.exists()\n</pre> azp.exists() In\u00a0[\u00a0]: Copied! <pre># list all the files in the directory\n[p for p in azp.parent.iterdir()]\n</pre> # list all the files in the directory [p for p in azp.parent.iterdir()] In\u00a0[\u00a0]: Copied! <pre>azp.exists()\n</pre> azp.exists() In\u00a0[\u00a0]: Copied! <pre>azp.unlink()\n</pre> azp.unlink() In\u00a0[\u00a0]: Copied! <pre>from cloudpathlib import CloudPath\n</pre> from cloudpathlib import CloudPath In\u00a0[\u00a0]: Copied! <pre>cloud_directory = CloudPath(\"s3://cloudpathlib-test-bucket/why_cloudpathlib/\")\n</pre> cloud_directory = CloudPath(\"s3://cloudpathlib-test-bucket/why_cloudpathlib/\") In\u00a0[\u00a0]: Copied! <pre>upload = cloud_directory / \"user_upload.txt\"\nupload.write_text(\"A user made this file!\")\n</pre> upload = cloud_directory / \"user_upload.txt\" upload.write_text(\"A user made this file!\") In\u00a0[\u00a0]: Copied! <pre>assert upload.exists()\nupload.unlink()\nassert not upload.exists()\n</pre> assert upload.exists() upload.unlink() assert not upload.exists() In\u00a0[\u00a0]: Copied! <pre>from cloudpathlib import CloudPath\n</pre> from cloudpathlib import CloudPath In\u00a0[\u00a0]: Copied! <pre># Changing this root path is the ONLY change!\ncloud_directory = CloudPath(\"az://cloudpathlib-test-container/why_cloudpathlib/\")\n</pre> # Changing this root path is the ONLY change! cloud_directory = CloudPath(\"az://cloudpathlib-test-container/why_cloudpathlib/\") In\u00a0[\u00a0]: Copied! <pre>upload = cloud_directory / \"user_upload.txt\"\nupload.write_text(\"A user made this file!\")\n</pre> upload = cloud_directory / \"user_upload.txt\" upload.write_text(\"A user made this file!\") In\u00a0[\u00a0]: Copied! <pre>assert upload.exists()\nupload.unlink()\nassert not upload.exists()\n</pre> assert upload.exists() upload.unlink() assert not upload.exists()"},{"location":"script/why_cloudpathlib/#why-cloudpathlib","title":"Why cloudpathlib?\u00b6","text":""},{"location":"script/why_cloudpathlib/#we-pathlib","title":"We \ud83d\ude0d pathlib\u00b6","text":"<p><code>pathlib</code> a wonderful tool for working with filesystem paths, available from the Python 3 standard library.</p>"},{"location":"script/why_cloudpathlib/#cross-platform-support","title":"Cross-platform support\u00b6","text":"<p>One great feature about using <code>pathlib</code> over regular strings is that it lets you write code with cross-platform file paths. It \"just works\" on Windows too. Write path manipulations that can run on anyone's machine!</p> <pre>path = Path.home()\npath\n&gt;&gt;&gt; C:\\Users\\DrivenData\\\n\ndocs = path / 'Documents'\ndocs\n&gt;&gt;&gt; C:\\Users\\DrivenData\\Documents\n</pre>"},{"location":"script/why_cloudpathlib/#we-also-cloud-storage","title":"We also \ud83d\ude0d cloud storage\u00b6","text":"<p>This is great, but I live in the future. Not every file I care about is on my machine. What do I do when I am working on S3? Do I have to explicitly download every file before I can do things with them?</p> <p>Of course not, if you use cloudpathlib!</p>"},{"location":"script/why_cloudpathlib/#cross-cloud-support","title":"Cross-cloud support\u00b6","text":"<p>That's cool, but I use Azure Blob Storage\u2015what can I do?</p>"},{"location":"script/why_cloudpathlib/#cloud-hopping","title":"Cloud hopping\u00b6","text":"<p>Moving between cloud storage providers should be a simple as moving between disks on your computer. Let's say that the Senior Vice President of Tomfoolery comes to me and says, \"We've got a mandate to migrate our application to Azure Blob Storage from S3!\"</p> <p>No problem, if I used <code>cloudpathlib</code>! The <code>CloudPath</code> class constructor automatically dispatches to the appropriate concrete class, the same way that <code>pathlib.Path</code> does for different operating systems.</p>"}]}